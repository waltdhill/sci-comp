{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9857bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # more basic functionality\n",
    "import scipy # advanced functionality, built on numpy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee0a08-d250-4e93-beea-c9a99ca81243",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- **Eigenvalues and Eigenvectors**\n",
    "    - Eigenvectors are the vectors that does not change its orientation when multiplied by the transition matrix, but it just scales by a factor of corresponding eigenvalues.\n",
    "- **Diagonalization & Eigendecomposition**\n",
    "    - A few applications of eigenvalues and eigenvectors that are very useful when handing the data in a matrix form because you could decompose them into matrices that are easy to manipulate.\n",
    "- **Underlying assumption behind the diagonalization and eigendecomposition**\n",
    "    - Make sure that the matrix you are trying to decompose is a square matrix and has linearly independent eigenvectors (different eigenvalues)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd36bfa",
   "metadata": {},
   "source": [
    "## Eigenvalue Problem and the Characteristic Polynomial\n",
    "\n",
    "A non-zero vector $v$ of dim. $\\mathbb{R}^n$ is an **eigenvector of square matrix $A : A\\in \\mathbb{R}^{n\\times n}$** \n",
    "- if it satisfies a linear equation of the form: $Av = \\lambda v$; for some scalar $\\lambda$ <mark>which we are solving for</mark>\n",
    "    - This is called the eigenvalue equation/problem\n",
    "    - **Geometric intuition**: The eigenvector(s) of $A$ are the vector(s) ($v$) which $A$ **only elongates/shrinks**, and never takes off it's span(s). \n",
    "        - The amount of this elongation/shrink is $\\lambda$, a scalar value\n",
    "- Rearranging the eigenvalue problem:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "Av&=\\lambda v\\\\ \n",
    "Av&=(\\lambda I) v = \\begin{bsmallmatrix} \n",
    "                    \\lambda & 0 & \\dots & 0 \\\\ \n",
    "                    0 & \\lambda & \\dots & 0 \\\\ \n",
    "                    \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "                    0 & 0 & \\dots & \\lambda \\\\ \n",
    "                    \\end{bsmallmatrix} \\begin{bsmallmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bsmallmatrix}\\\\\n",
    "(A-\\lambda I) v &= 0\\\\\n",
    "\\end{align*} $$\n",
    "\n",
    "$\\text{Since }v \\ne 0\\text{, solve for:}$\n",
    "\n",
    "$$ \\begin{align*}\n",
    "p(\\lambda) = \\text{det}(A-\\lambda I)&=0\\\\\n",
    "\\end{align*} $$\n",
    "\n",
    "\n",
    "\n",
    "- <mark>The **only way for $(A-\\lambda I) v = 0$ to be possible** (given non-zero $v$) is for $\\text{det}(A-\\lambda I)=0$</mark>\n",
    "    - i.e. The matrix $(A-\\lambda I$) represents a *linear transform. of the vector space which \"reduces\" its dimensionality* (at least 1 dim is lost)\n",
    "    - A matrix **cannot squish non-zero vectors into the $\\vec{0}$ vector, except when their determinant is 0**\n",
    "- By computing the determinant, we get the eigenvalues $\\lambda_1, \\lambda_2(, ..., \\lambda_n)$ (1 for each dimension of the square matrix).\n",
    "    - Computing $\\text{det}(A-\\lambda I)$ requires solving a **characteristic polynomial** whose roots are the $\\lambda$(s)\n",
    " \n",
    "#### Why the $\\text{det}(A-\\lambda I) = 0$ observation matters\n",
    "- The observation that $\\text{det}(A-\\lambda I)\\equiv 0$ is only useful because solving it yields the eigenvalues $\\lambda$s.\n",
    "    - Those helps us solve for the eigenvectors $v$s (i.e. those vectors that this diagonally altered matrix $(A-\\lambda I)$ \"shrinks\" to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e466f403-1866-4cb2-8772-d8a8f70e40e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[4 2 2]\n",
      " [2 4 2]\n",
      " [2 2 4]]\n",
      "determinant(A): 32.0\n",
      "\n",
      "Eigenvalues - shape: (3,) values: [2. 8. 2.]\n",
      "\n",
      "Eigenvectors - shape: (3, 3) values:\n",
      " [[-0.82  0.41  0.41]\n",
      " [ 0.58  0.58  0.58]\n",
      " [ 0.51 -0.81  0.3 ]]\n",
      "Check against the Eigenvalue Problem Av = λv\n",
      "\n",
      "Av:\n",
      " [[-1.63299316  4.61880215  1.01339709]\n",
      " [ 0.81649658  4.61880215 -1.61564839]\n",
      " [ 0.81649658  4.61880215  0.6022513 ]]\n",
      "\n",
      "λv:\n",
      " [[-1.63299316  4.61880215  1.01339709]\n",
      " [ 0.81649658  4.61880215 -1.61564839]\n",
      " [ 0.81649658  4.61880215  0.6022513 ]]\n"
     ]
    }
   ],
   "source": [
    "# A = np.random.randn(4,4)\n",
    "# A = np.array([[3,1],\n",
    "#              [0,2]])\n",
    "\n",
    "import scipy\n",
    "A = np.array([[4,2,2],\n",
    "             [2,4,2],\n",
    "             [2,2,4]])\n",
    "# A = np.diag((1,2,3))\n",
    "detA = np.linalg.det(A)\n",
    "\n",
    "print('A:\\n', A)\n",
    "print('determinant(A):', detA)\n",
    "\n",
    "eig_vals,eig_vecs = np.linalg.eig(A)\n",
    "# eig_vals,eig_vecs = np.linalg.eigh(A)\n",
    "# eig_vals,eig_vecs = scipy.linalg.eig(A)\n",
    "\n",
    "print('\\nEigenvalues - shape:', eig_vals.shape, 'values:', np.round(eig_vals,0))\n",
    "print('\\nEigenvectors - shape:', eig_vecs.shape, 'values:\\n', np.round(eig_vecs.T,2)) # not sure why this is so different to Wolfram and others \n",
    "\n",
    "# Link 1: https://www.wolframalpha.com/input?i=eigenvectors+of+%7B%7B4%2C2%2C2%7D%2C%7B2%2C4%2C2%7D%2C%7B2%2C2%2C4%7D%7D\n",
    "# Link 2: https://matrixcalc.org/vectors.html#eigenvectors%28%7B%7B4,2,2%7D,%7B2,4,2%7D,%7B2,2,4%7D%7D%29\n",
    "\n",
    "# But let's test it against the eigenvalue problem: Av = λv (\n",
    "print('Check against the Eigenvalue Problem Av = λv')\n",
    "print('\\nAv:\\n', A@eig_vecs)\n",
    "print('\\nλv:\\n', eig_vals*eig_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6aef64-d20e-492c-a79b-37f839e9edad",
   "metadata": {},
   "source": [
    "## Eigenbasis, Diagonalisation, and Eigen-decomposition\n",
    "\n",
    "## Eigenbasis\n",
    "- If our <mark>**basis vectors** ($\\hat{i} = \\begin{bsmallmatrix} 1 \\\\ 0 \\end{bsmallmatrix}\\\\, \\hat{j} = \\begin{bsmallmatrix} 0 \\\\ 1 \\end{bsmallmatrix}\\\\, \\dots$) are themselves **eigenvectors**</mark>. It is called an eigenbasis then if we inspect $A$, the transformation matrix, it will have the form known as a <mark>**Diagonal Matrix**</mark>:\n",
    "\n",
    "$$A_{\\text{diag}} = \\begin{bsmallmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{bsmallmatrix}$$\n",
    "\n",
    "- Its form is <mark>(diagonal) because recall $A$ **only scales** (stretch/shrink) its eigenvectors</mark>, which in this case are the basis vectors\n",
    "    - It is very easy to compute large powers of diagonal matrices (they simply scale vectors by the eigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56503247-ab66-4a84-aa98-6328156776a1",
   "metadata": {},
   "source": [
    "## <mark>Diagonalisation</mark>: Using the Eigenbasis to Diagonalise any non-diagonal $A$\n",
    "1. Find the eigenvectors of $A$\n",
    "2. Make a **change of basis** matrix $S$ whose columns are the eigenvectors of $A$. We'll use this to switch the coordinate system of $A$\n",
    "3. Diagonalise $A$ to get $\\Lambda$ by doing this:\n",
    "$$\\Lambda = S^{-1}AS$$\n",
    "4. The new matrix $\\Lambda$ is guaranteed to be diagonal, with its **eigenvalues on the main diagonal**\n",
    "\n",
    "#### Derivation: Why is diagonalisation $\\Lambda = S^{-1}AS$ possible in the first place?\n",
    "\n",
    "Show that $AS=S\\Lambda$\n",
    "- Suppose we have $m$ linearly independent eigenvectors of $A$;\n",
    "    - then $S$ is a matrix, where each column is an eigenvector of $A$, $v_{1\\dots m}$\n",
    "    - $AS = A \\begin{bsmallmatrix} v_1 & v_2 & \\dots & v_m \\end{bsmallmatrix} = \\begin{bsmallmatrix} A v_1 & A v_2 & \\dots & A v_m \\end{bsmallmatrix} = \\begin{bsmallmatrix} \\lambda_1 v_1 & \\lambda_2 v_2 & \\dots & \\lambda_m v_m \\end{bsmallmatrix} $ (final step because recall $Av=\\lambda $)\n",
    "- And so $\\begin{bsmallmatrix} \\lambda_1 v_1 & \\lambda_2 v_2 & \\dots & \\lambda_m v_m \\end{bsmallmatrix}$ = $\\begin{bsmallmatrix} \\ v_1 & \\ v_2 & \\dots & \\ v_m \\end{bsmallmatrix} \\begin{bsmallmatrix} \n",
    "\\ \\lambda_1 & 0 & \\dots & 0 \\\\\n",
    "\\ 0 & \\lambda_2 & \\dots & 0 \\\\\n",
    "\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\ 0 & 0 & \\dots & \\lambda_m \\\\\n",
    "\\end{bsmallmatrix} $ which is $S\\Lambda$\n",
    "\n",
    "#### Assumptions:\n",
    "\n",
    "The matrix $A$ you are trying to decompose must:\n",
    "- be a square matrix\n",
    "- have linearly independent eigenvectors (different eigenvalues, 1 for each row of the matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2065feec-1471-449f-ad8f-9a0e5fd6dae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall A:\n",
      " [[4 2 2]\n",
      " [2 4 2]\n",
      " [2 2 4]]\n",
      "\n",
      "And as calculated earlier for A:\n",
      "\n",
      "Eigenvalues: [2. 8. 2.]\n",
      "\n",
      "Eigenvectors:\n",
      " [[-0.82  0.41  0.41]\n",
      " [ 0.58  0.58  0.58]\n",
      " [ 0.51 -0.81  0.3 ]]\n",
      "\n",
      "------\n",
      "\n",
      "Lambda = S^-1 @ A @ S:\n",
      " [[ 2.  0. -0.]\n",
      " [ 0.  8. -0.]\n",
      " [-0. -0.  2.]]\n"
     ]
    }
   ],
   "source": [
    "print('Recall A:\\n',A)\n",
    "print('\\nAnd as calculated earlier for A:')\n",
    "print('\\nEigenvalues:', np.round(eig_vals,0))\n",
    "print('\\nEigenvectors:\\n', np.round(eig_vecs.T,2))\n",
    "print('\\n------\\n')\n",
    "\n",
    "# Define a change of basis matrix S, and then diagonalise A using Lambda = S^(-1) A S\n",
    "S = eig_vecs \n",
    "Lambda = np.linalg.inv(S)@A@S\n",
    "\n",
    "print('Lambda = S^-1 @ A @ S:\\n', np.round(Lambda,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e9a039-5489-462f-a102-4f4ffd066ef1",
   "metadata": {},
   "source": [
    "## Now that we know $AS=S\\Lambda$, we can do:\n",
    "\n",
    "### Diagonalisation:\n",
    "\n",
    "- <mark>Takes $A$ matrix to produce $\\Lambda$, a diagonal matrix with **eigenvalues on the main diagonal**</mark>\n",
    "- Multiply both sides by $S^{-1}$ **from the left**. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "S^{-1}\\times AS &= S^{-1}\\times S\\Lambda \\\\\n",
    "S^{-1}AS &= S^{-1}S\\Lambda \\\\\n",
    "S^{-1}AS &= \\Lambda \\\\ \\\\\n",
    "\\Lambda &= S^{-1}AS\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Eigendecomposition\n",
    "\n",
    "- After diagonalising $A$ to get $\\Lambda$, <mark>we can use eigendecomposition to do quick matrix multiplications of A</mark>\n",
    "- Multiply both sides by $S^{-1}$ **from the right**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "AS \\times S^{-1} &= S\\Lambda \\times S^{-1} \\\\\n",
    "ASS^{-1} &= S\\Lambda S^{-1} \\\\\n",
    "A &= S\\Lambda S^{-1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\text{Now note, if we raise $A$ to arbitrary powers}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A^2 &= (S\\Lambda S^{-1})(S\\Lambda S^{-1}) \\\\\n",
    "&= S\\Lambda (S^{-1}S)\\Lambda S^{-1} = S\\Lambda^2 S^{-1}\\\\\\\\\n",
    "\\text{In general } A^k &= S\\Lambda^k S^{-1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420599d0-33a9-4b8b-923b-b0844aff59b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_eigendecomposed = S @ Lambda @ S^-1:\n",
      " [[4. 2. 2.]\n",
      " [2. 4. 2.]\n",
      " [2. 2. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# Perform eigendecomposition to recover A from its eigenvectors and eigenvalues\n",
    "A_eig_decomp = S@Lambda@np.linalg.inv(S)\n",
    "print('A_eigendecomposed = S @ Lambda @ S^-1:\\n', A_eig_decomp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f1a9d",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "- When exactly do we use decompositions?\n",
    "- What is the intuition behind an eigenvalue and eigenvector?\n",
    "- Some interesting edge cases (what are the eigenvalues and eigenvectors for):\n",
    "    - A rotation-only matrix like $\\begin{bsmallmatrix} 0 & -1 \\\\ 1 & 0 \\end{bsmallmatrix}$ has only imaginary $\\lambda=i$. No eigenvectors, as each vector is rotated\n",
    "    - A shear matrix like $\\begin{bsmallmatrix} 1 & 1 \\\\ 0 & 1 \\end{bsmallmatrix}$: only 1 eigenvalue ($\\lambda = 1$), so only 1 eigenvector (all vectors on the x-axis are eigenvectors)\n",
    "    - A scaling-only matrix like $\\begin{bsmallmatrix} 2 & 0 \\\\ 0 & 2 \\end{bsmallmatrix}$: only 1 eigenvalue ($\\lambda = 2$), but **all** vectors in the plane are eigenvectors, and scaled by 2x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
