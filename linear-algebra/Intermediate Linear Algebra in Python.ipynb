{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9857bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # more basic functionality\n",
    "import scipy # advanced functionality, built on numpy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee0a08-d250-4e93-beea-c9a99ca81243",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- Eigenvalues and Eigenvectors\n",
    "    - Eigenvectors are the vectors that does not change its orientation when multiplied by the transition matrix, but it just scales by a factor of corresponding eigenvalues.\n",
    "- Diagonalization & Eigendecomposition\n",
    "    - A few applications of eigenvalues and eigenvectors that are very useful when handing the data in a matrix form because you could decompose them into matrices that are easy to manipulate.\n",
    "- Underlying assumption behind the diagonalization and eigendecomposition\n",
    "    - Make sure that the matrix you are trying to decompose is a square matrix and has linearly independent eigenvectors (different eigenvalues)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97349eed",
   "metadata": {},
   "source": [
    "## More terminology:\n",
    "\n",
    "### Spans and spaces\n",
    "\n",
    "- **Span**: The span of a set of vectors is all linear combinations of those vectors\n",
    "- **Vector space** is denoted as $\\mathbb{R}^n$. \n",
    "    - Every element in a vector space can be written as a **linear combination** of the elements in the **basis** (unit) vectors\n",
    "        - **Basis (unit) vectors** (example): For a 2D vector space, the basis vectors are $\\hat{i} = \\begin{bsmallmatrix} 1 \\\\ 0 \\end{bsmallmatrix}$ and $\\hat{j} = \\begin{bsmallmatrix} 0 \\\\ 1 \\end{bsmallmatrix}$\n",
    "        - A matrix $A$ applies a linear transformation to a vector space (i.e. all vectors in the space)\n",
    "        - <mark>The columns of $A$ represent the **landing points** for the basis (unit) vectors **after the transformation**</mark>\n",
    "        - By extension, $A$ moves **every input vector** (more precisely, the **point where every vector's tip is**) **linearly** to a new location.\n",
    "        - We only need to know how $A$ transforms the bases $\\hat{i}$ and $\\hat{j}$, since\n",
    "            - any vector $v$ is <mark>just a **linear combination** of $\\hat{i}$ and $\\hat{j}$ **both before and after being transformed by $A$**</mark>\n",
    "- **Subspace**: a subset of a larger vector space\n",
    "    - **Column space**: Span of all column vectors of of matrix $A$\n",
    "    - **Row space**: Span of all row vectors of matrix $A$\n",
    "    - **Null space**: If $A \\cdot x = 0$, the span of all solutions $x$ constitutes the **null space** of $A$\n",
    "    - **Left-Null space**: If $A^T \\cdot x = 0$, the span of all solutions $x$ constitutes the **left-null space** of $A$\n",
    "    \n",
    "### More properties\n",
    "\n",
    "- **Rank**: The number of **linearly independent** columns (or rows) in $A$ is its rank\n",
    "- **Orthonormal vectors**: Two unit length vectors whose inner (i.e. dot) products are 0\n",
    "- **Real value matrices**:\n",
    "    - **Orthogonal matrices**: If $A$'s rows and cols are orthonormal vectors, $A$ is an orthogonal matrix. It satisfies:\n",
    "        - $A^TA = AA^T = I$\n",
    "    - **Symmetric matrix**: where $A^T = A$ (square matrices only) \n",
    "- **Complex value matrices**\n",
    "    - Hermitian matrix: Complex matrices' analog to orthogonal matrix\n",
    "    - Unitary matrix: Complex matrices' analog to symmetric matrix\n",
    "- **Determinant**: This can be computed for any square matrix $A$\n",
    "    - Matrices are only invertible if $det(A) \\ne 0$. Such matrices are **non-singular**; and satisfy $AA^{-1}=I$\n",
    "    - Matrices where $det(A) = 0$ are not invertible. They are **singular** matrices\n",
    "  \n",
    "#### Examples of the above (where relevant):\n",
    "\n",
    "- The **span** of vectors (0,1) and (1,0) is the whole x-y plane\n",
    "- A vector, $v$, with 3 elements is said to exist in **vector space** $\\mathbb{R}^3$\n",
    "- The $\\mathbb{R}^3$ vector, $v$ is composed of **basis vectors** (1,0,0), (0,1,0), and (0,0,1).\n",
    "- The subspace of a 3D vector (in $\\mathbb{R}^3$) is the span of vectors (1,0,0) and (0,1,0) \n",
    "    - in this case the 2D x-y plane is a subspace (subset) of the 3D x,y,z vector space\n",
    "- Vectors (1,2,3) and (10,20,30) are linearly dependent since one is a multiple of the other.\n",
    "    - A matrix with those two vectors would be rank 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d0f0a1",
   "metadata": {},
   "source": [
    "### Recap: A few ways to multiply vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ae22b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of vector multiplication\n",
      "inner: 122\n",
      "outer: [[28 32 36]\n",
      " [35 40 45]\n",
      " [42 48 54]]\n",
      "hadamard: [28 40 54]\n"
     ]
    }
   ],
   "source": [
    "# Find the inner and outer products of two 1D arrays (not exactly vectors, no double [[]])\n",
    "a = np.array([4,5,6])\n",
    "b = np.array([7,8,9])\n",
    "\n",
    "print('Types of vector multiplication')\n",
    "print('inner:', np.inner(a,b)) # dot prod; dims are: [1x3][3x1]=[1x1] <-- output dim\n",
    "print('outer:', np.outer(a,b)) # dims are [3x1][1x3]=[3x3] <-- output dim\n",
    "print('hadamard:', a * b) # elementwise (or hadamard) product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265a5d1",
   "metadata": {},
   "source": [
    "### Gram-Schmidt Process\n",
    "\n",
    "Use this to orthonormalise anything (vector or matrix (orthogonalise))\n",
    "\n",
    "- Orthonormalise a set of vectors $\\{\\bar{v_1}, \\bar{v_2}, \\bar{v_3}, ..., \\bar{v_n}\\}$ \n",
    "    - to $\\{\\bar{u_1}, \\bar{u_2}, \\bar{u_3}, ..., \\bar{u_n}\\}$, where each $u$ vector is in the same $\\mathbb{R}^n$ vector space, \n",
    "        - but each vector is unit length, and \n",
    "        - is mutually orthogonal with other vectors\n",
    "\n",
    "I.e. Transform a set of vectors into a set of orthonormal vectors in the same vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b7651",
   "metadata": {},
   "source": [
    "## Matrix decompositions\n",
    "\n",
    "### Gaussian Elimination (or Decomposition?)\n",
    "\n",
    "- **Purpose**: We use Gaussian Elimination to simplify a system of linear equations, $Ax=b$ into *row echelon form* (or *reduced row echelon form*; which allows solving $Ax=b$ by simple inspection)\n",
    "- Application: \n",
    "    - Solving linear system $Ax=b$, \n",
    "    - Computing inverse matrices\n",
    "    - Computing rank\n",
    "    - Computing determinant\n",
    "    - **Elementary row operations**: Methods by which the above are done\n",
    "        - Swapping rows\n",
    "        - Scaling rows\n",
    "        - Adding rows to each other (i.e. creating linear combinations)\n",
    "        \n",
    "- **Row echelon form**: The first *non-zero* element from the left in each row (aka leading coefficient, pivot) is **always to the right of** the first *non-zero* element in the row above\n",
    "- **Reduced row echelon form**: Row echelon form whose pivots are $1$ and column containing pivots are $0$ elsewhere\n",
    "\n",
    "- Elementary row operation\n",
    "\n",
    "### LU Decomposition\n",
    "\n",
    "Like Gaussian Decomposition, but more computationally efficient\n",
    "\n",
    "Decompose any matrix $A$ (square or not) into:\n",
    "- A lower triangular matrix $L$\n",
    "- An upper triangular matrix $U$\n",
    "- Sometimes needing to reorder $A$ using a $P$ matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029f0ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-0.55038208 -1.64274921  0.74248386  1.05567946]\n",
      " [-0.37105223  1.64296612 -0.9953272   1.42100597]\n",
      " [-0.88663242  1.07443251  1.56042151  0.51827791]]\n",
      "\n",
      "P:\n",
      " [[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "L:\n",
      " [[ 1.          0.          0.        ]\n",
      " [ 0.62075565  1.          0.        ]\n",
      " [ 0.41849612 -0.51665389  1.        ]]\n",
      "\n",
      "U:\n",
      " [[-0.88663242  1.07443251  1.56042151  0.51827791]\n",
      " [ 0.         -2.30970927 -0.22615661  0.73395552]\n",
      " [ 0.          0.         -1.76520225  1.58330965]]\n",
      "\n",
      "----\n",
      "\n",
      "Recomposition: PLU = A:\n",
      " [[-0.55038208 -1.64274921  0.74248386  1.05567946]\n",
      " [-0.37105223  1.64296612 -0.9953272   1.42100597]\n",
      " [-0.88663242  1.07443251  1.56042151  0.51827791]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(3,4)\n",
    "print('A:\\n', a)\n",
    "\n",
    "p,l,u = scipy.linalg.lu(a)\n",
    "print('\\nP:\\n', p)\n",
    "print('\\nL:\\n', l)\n",
    "print('\\nU:\\n', u)\n",
    "print('\\n----\\n\\nRecomposition: PLU = A:\\n', p@l@u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d62a29",
   "metadata": {},
   "source": [
    "### QR Decomposition\n",
    "\n",
    "Decompose a matrix $A$ into:\n",
    "- an orthogonal matrix $Q$\n",
    "- an upper triangular matrix $R$\n",
    "\n",
    "It's used in QR algorithms to solve the linear least square problem. \n",
    "\n",
    "Also, the $Q$ matrix is sometimes what we desire after the **Gram-Schmidt process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dda371d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-1.33656849  0.47121422  0.84715639 -1.63111615]\n",
      " [ 1.0110986   0.73512738 -0.23842762  0.01694656]\n",
      " [-0.4905946  -0.94730809  0.71554459  0.96648081]]\n",
      "\n",
      "Q:\n",
      " [[-0.76538983 -0.58201376  0.27466052]\n",
      " [ 0.57900855 -0.43644523  0.68866876]\n",
      " [-0.28094042  0.68613086  0.67104167]]\n",
      "\n",
      "R:\n",
      " [[ 1.74625851  0.33111959 -0.98748191  0.98672838]\n",
      " [ 0.         -1.2450733   0.10196114  1.6050681 ]\n",
      " [ 0.          0.          0.548643    0.21221625]]\n",
      "\n",
      "----\n",
      "\n",
      "Recomposition: QR = A:\n",
      " [[-1.33656849  0.47121422  0.84715639 -1.63111615]\n",
      " [ 1.0110986   0.73512738 -0.23842762  0.01694656]\n",
      " [-0.4905946  -0.94730809  0.71554459  0.96648081]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(3,4)\n",
    "print('A:\\n', a)\n",
    "\n",
    "q,r = np.linalg.qr(a)\n",
    "print('\\nQ:\\n', q)\n",
    "print('\\nR:\\n', r)\n",
    "print('\\n----\\n\\nRecomposition: QR = A:\\n', q@r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ccbc55",
   "metadata": {},
   "source": [
    "### Cholesky Decomposition\n",
    "\n",
    "Decompose a symmetric (or Hermitian) positive-definite matrix into:\n",
    "\n",
    "- a lower triangular matrix $L$\n",
    "- and its transpose (or conjugate transpose) $L.H.$\n",
    "\n",
    "Used in algorithms for numerical convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20a0a2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " [[1 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 4]]\n",
      "\n",
      "L:\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.         1.41421356 0.         0.        ]\n",
      " [0.         0.         1.73205081 0.        ]\n",
      " [0.         0.         0.         2.        ]]\n",
      "\n",
      "----\n",
      "\n",
      "Recomposition: LL^T:\n",
      " [[1. 0. 0. 0.]\n",
      " [0. 2. 0. 0.]\n",
      " [0. 0. 3. 0.]\n",
      " [0. 0. 0. 4.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.diagflat([[1,2], [3,4]])\n",
    "print('x:\\n', x)\n",
    "\n",
    "L = np.linalg.cholesky(x)\n",
    "print('\\nL:\\n', L)\n",
    "\n",
    "print('\\n----\\n\\nRecomposition: LL^T:\\n', L @ L.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd36bfa",
   "metadata": {},
   "source": [
    "## Eigenvalue Problem and the Characteristic Polynomial\n",
    "\n",
    "A non-zero vector $v$ of dim. $\\mathbb{R}^n$ is an **eigenvector of square matrix $A : A\\in \\mathbb{R}^{n\\times n}$** \n",
    "- if it satisfies a linear equation of the form: $Av = \\lambda v$; for some scalar $\\lambda$ <mark>which we are solving for</mark>\n",
    "    - This is called the eigenvalue equation/problem\n",
    "    - **Geometric intuition**: The eigenvector(s) of $A$ are the vector(s) ($v$) which $A$ **only elongates/shrinks**, and never takes off it's span(s). \n",
    "        - The amount of this elongation/shrink is $\\lambda$, a scalar value\n",
    "- Rearranging the eigenvalue problem:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "Av&=\\lambda v\\\\ \n",
    "Av&=(\\lambda I) v = \\begin{bsmallmatrix} \n",
    "                    \\lambda & 0 & \\dots & 0 \\\\ \n",
    "                    0 & \\lambda & \\dots & 0 \\\\ \n",
    "                    \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "                    0 & 0 & \\dots & \\lambda \\\\ \n",
    "                    \\end{bsmallmatrix} \\begin{bsmallmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bsmallmatrix}\\\\\n",
    "(A-\\lambda I) v &= 0\\\\\n",
    "\\end{align*} $$\n",
    "\n",
    "$\\text{Since }v \\ne 0\\text{, solve for:}$\n",
    "\n",
    "$$ \\begin{align*}\n",
    "p(\\lambda) = \\text{det}(A-\\lambda I)&=0\\\\\n",
    "\\end{align*} $$\n",
    "\n",
    "\n",
    "\n",
    "- <mark>The **only way for $(A-\\lambda I) v = 0$ to be possible** (given non-zero $v$) is for $\\text{det}(A-\\lambda I)=0$</mark>\n",
    "    - i.e. The matrix $(A-\\lambda I$) represents a *linear transform. of the vector space which \"reduces\" its dimensionality* (at least 1 dim is lost)\n",
    "    - A matrix **cannot squish non-zero vectors into the $\\vec{0}$ vector, except when their determinant is 0**\n",
    "- By computing the determinant, we get the eigenvalues $\\lambda_1, \\lambda_2(, ..., \\lambda_n)$ (1 for each dimension of the square matrix).\n",
    "    - Computing $\\text{det}(A-\\lambda I)$ requires solving a **characteristic polynomial** whose roots are the $\\lambda$(s)\n",
    " \n",
    "#### Why the $\\text{det}(A-\\lambda I) = 0$ observation matters\n",
    "- The observation that $\\text{det}(A-\\lambda I)\\equiv 0$ is only useful because solving it yields the eigenvalues $\\lambda$s.\n",
    "    - Those helps us solve for the eigenvectors $v$s (i.e. those vectors that this diagonally altered matrix $(A-\\lambda I)$ \"shrinks\" to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e466f403-1866-4cb2-8772-d8a8f70e40e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[3 1]\n",
      " [0 2]]\n",
      "Eigenvalues):\n",
      " [3. 2.]\n",
      "Eigenvectors:\n",
      " [[ 1.         -0.70710678]\n",
      " [ 0.          0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "# A = np.random.randn(4,4)\n",
    "A = np.array([[3,1],\n",
    "             [0,2]])\n",
    "print('A:\\n', A)\n",
    "lambdas,v = np.linalg.eig(A)\n",
    "\n",
    "print('Eigenvalues):\\n', lambdas)\n",
    "print('Eigenvectors:\\n', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6aef64-d20e-492c-a79b-37f839e9edad",
   "metadata": {},
   "source": [
    "## Eigenbasis, Diagonalisation, and Eigen-decomposition\n",
    "\n",
    "## Eigenbasis\n",
    "- If our <mark>**basis vectors** ($\\hat{i} = \\begin{bsmallmatrix} 1 \\\\ 0 \\end{bsmallmatrix}\\\\, \\hat{j} = \\begin{bsmallmatrix} 0 \\\\ 1 \\end{bsmallmatrix}\\\\, \\dots$) are themselves **eigenvectors**</mark>. It is called an eigenbasis then if we inspect $A$, the transformation matrix, it will have the form known as a <mark>**Diagonal Matrix**</mark>:\n",
    "$$A = \\begin{bsmallmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{bsmallmatrix}$$\n",
    "- Its form is <mark>(diagonal) because recall $A$ **only scales** (stretch/shrink) its eigenvectors</mark>, which in this case are the basis vectors\n",
    "    - It is very easy to compute large powers of diagonal matrices (they simply scale vectors by the eigenvalues)\n",
    " \n",
    "## <mark>Diagonalisation</mark>: Using the Eigenbasis to Diagonalise any non-diagonal $A$\n",
    "1. Find the eigenvectors of $A$\n",
    "2. Make a **change of basis** matrix $S$ whose columns are the eigenvectors of $A$. We'll use this to switch the coordinate system of $A$\n",
    "3. Diagonalise $A$ to get $\\Lambda$ by doing this:\n",
    "$$\\Lambda = S^{-1}AS$$\n",
    "4. The new matrix $\\Lambda$ is guaranteed to be diagonal, with its **eigenvalues on the main diagonal**\n",
    "\n",
    "#### Why is diagonalisation $\\Lambda = S^{-1}AS$ possible in the first place?\n",
    "\n",
    "Show that $AS=S\\Lambda$\n",
    "- Suppose we have $m$ linearly independent eigenvectors of $A$;\n",
    "    - then $S$ is a matrix, where each column is an eigenvector of $A$, $v_{1\\dots m}$\n",
    "    - $AS = A \\begin{bsmallmatrix} v_1 & v_2 & \\dots & v_m \\end{bsmallmatrix} = \\begin{bsmallmatrix} A v_1 & A v_2 & \\dots & A v_m \\end{bsmallmatrix} = \\begin{bsmallmatrix} \\lambda_1 v_1 & \\lambda_2 v_2 & \\dots & \\lambda_m v_m \\end{bsmallmatrix} $ (final step because recall $Av=\\lambda $)\n",
    "- And so $\\begin{bsmallmatrix} \\lambda_1 v_1 & \\lambda_2 v_2 & \\dots & \\lambda_m v_m \\end{bsmallmatrix}$ = $\\begin{bsmallmatrix} \\ v_1 & \\ v_2 & \\dots & \\ v_m \\end{bsmallmatrix} \\begin{bsmallmatrix} \n",
    "\\ \\lambda_1 & 0 & \\dots & 0 \\\\\n",
    "\\ 0 & \\lambda_2 & \\dots & 0 \\\\\n",
    "\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\ 0 & 0 & \\dots & \\lambda_m \\\\\n",
    "\\end{bsmallmatrix} $ which is $S\\Lambda$\n",
    "\n",
    "#### Assumptions:\n",
    "\n",
    "The matrix $A$ you are trying to decompose must:\n",
    "- be a square matrix\n",
    "- have linearly independent eigenvectors (different eigenvalues, 1 for each row of the matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e9a039-5489-462f-a102-4f4ffd066ef1",
   "metadata": {},
   "source": [
    "## Now that we know $AS=S\\Lambda$, we can do:\n",
    "\n",
    "### Diagonalisation:\n",
    "\n",
    "- <mark>Takes $A$ matrix to produce $\\Lambda$, a diagonal matrix with **eigenvalues on the main diagonal**</mark>\n",
    "- Multiply both sides by $S^{-1}$ **from the left**. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "S^{-1}\\times AS &= S^{-1}\\times S\\Lambda \\\\\n",
    "S^{-1}AS &= S^{-1}S\\Lambda \\\\\n",
    "S^{-1}AS &= \\Lambda \\\\ \\\\\n",
    "\\Lambda &= S^{-1}AS\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Eigendecomposition\n",
    "\n",
    "- After diagonalising $A$ to get $\\Lambda$, <mark>we can use eigendecomposition to do quick matrix multiplications of A</mark>\n",
    "- Multiply both sides by $S^{-1}$ **from the right**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "AS \\times S^{-1} &= S\\Lambda \\times S^{-1} \\\\\n",
    "ASS^{-1} &= S\\Lambda S^{-1} \\\\\n",
    "A &= S\\Lambda S^{-1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\text{Now note, if we raise $A$ to arbitrary powers}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A^2 &= (S\\Lambda S^{-1})(S\\Lambda S^{-1}) \\\\\n",
    "&= S\\Lambda (S^{-1}S)\\Lambda S^{-1} = S\\Lambda^2 S^{-1}\\\\\\\\\n",
    "\\text{In general } A^k &= S\\Lambda^k S^{-1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f1a9d",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "- When exactly do we use decompositions?\n",
    "- What is the intuition behind an eigenvalue and eigenvector?\n",
    "- Some interesting edge cases (what are the eigenvalues and eigenvectors for):\n",
    "    - A rotation-only matrix like $\\begin{bsmallmatrix} 0 & -1 \\\\ 1 & 0 \\end{bsmallmatrix}$ has only imaginary $\\lambda=i$. No eigenvectors, as each vector is rotated\n",
    "    - A shear matrix like $\\begin{bsmallmatrix} 1 & 1 \\\\ 0 & 1 \\end{bsmallmatrix}$: only 1 eigenvalue ($\\lambda = 1$), so only 1 eigenvector (all vectors on the x-axis are eigenvectors)\n",
    "    - A scaling-only matrix like $\\begin{bsmallmatrix} 2 & 0 \\\\ 0 & 2 \\end{bsmallmatrix}$: only 1 eigenvalue ($\\lambda = 2$), but **all** vectors in the plane are eigenvectors, and scaled by 2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1db70e-f55e-480b-ac43-47185fcdfe45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
