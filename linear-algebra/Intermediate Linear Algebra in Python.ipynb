{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763ec085",
   "metadata": {},
   "source": [
    "# Terminology we will cover\n",
    "\n",
    "- Vector space,\n",
    "- subspace, \n",
    "- span, \n",
    "- column space, \n",
    "- row space,\n",
    "- null space,\n",
    "- left-null space,\n",
    "- rank, \n",
    "- basis, \n",
    "- orthogonal matrix, \n",
    "- symmetric matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9857bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # more basic functionality\n",
    "import scipy # advanced functionality, built on numpy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97349eed",
   "metadata": {},
   "source": [
    "## More terminology:\n",
    "\n",
    "### Spans and spaces\n",
    "\n",
    "- **Span**: The span of a set of vectors is all linear combinations of those vectors\n",
    "- **Vector space** is denoted as $\\mathbb{R}^n$. \n",
    "    - Every element in a vector space can be written as a **linear combination** of the elements in the **basis** vectors\n",
    "- **Subspace**: a subset of a larger vector space\n",
    "    - **Column space**: Span of all column vectors of of matrix $A$\n",
    "    - **Row space**: Span of all row vectors of matrix $A$\n",
    "    - **Null space**: If $A \\cdot x = 0$, the span of all solutions $x$ constitutes the **null space** of $A$\n",
    "    - **Left-Null space**: If $A^T \\cdot x = 0$, the span of all solutions $x$ constitutes the **left-null space** of $A$\n",
    "    \n",
    "### More properties\n",
    "\n",
    "- **Rank**: The number of **linearly independent** columns (or rows) in $A$ is its rank\n",
    "- **Orthonormal vectors**: Two unit length vectors whose inner (i.e. dot) products are 0\n",
    "- **Real value matrices**:\n",
    "    - **Orthogonal matrices**: If $A$'s rows and cols are orthonormal vectors, $A$ is an orthogonal matrix. It satisfies:\n",
    "        - $A^TA = AA^T = I$\n",
    "    - **Symmetric matrix**: where $A^T = A$ (square matrices only) \n",
    "- **Complex value matrices**\n",
    "    - Hermitian matrix: Complex matrices' analog to orthogonal matrix\n",
    "    - Unitary matrix: Complex matrices' analog to symmetric matrix\n",
    "- **Determinant**: This can be computed for any square matrix $A$\n",
    "    - Matrices are only invertible if $det(A) \\ne 0$. Such matrices are **non-singular**; and satisfy $AA^{-1}=I$\n",
    "    - Matrices where $det(A) = 0$ are not invertible. They are **singular** matrices\n",
    "  \n",
    "#### Examples of the above (where relevant):\n",
    "\n",
    "- The **span** of vectors (0,1) and (1,0) is the whole x-y plane\n",
    "- A vector, $v$, with 3 elements is said to exist in **vector space** $\\mathbb{R}^3$\n",
    "- The $\\mathbb{R}^3$ vector, $v$ is composed of **basis vectors** (1,0,0), (0,1,0), and (0,0,1).\n",
    "- The subspace of a 3D vector (in $\\mathbb{R}^3$) is the span of vectors (1,0,0) and (0,1,0) \n",
    "    - in this case the 2D x-y plane is a subspace (subset) of the 3D x,y,z vector space\n",
    "- Vectors (1,2,3) and (10,20,30) are linearly dependent since one is a multiple of the other.\n",
    "    - A matrix with those two vectors would be rank 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d0f0a1",
   "metadata": {},
   "source": [
    "### Recap: A few ways to multiply vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66ae22b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of vector multiplication\n",
      "inner: 122\n",
      "outer: [[28 32 36]\n",
      " [35 40 45]\n",
      " [42 48 54]]\n",
      "hadamard: [28 40 54]\n"
     ]
    }
   ],
   "source": [
    "# Find the inner and outer products of two 1D arrays (not exactly vectors, no double [[]])\n",
    "a = np.array([4,5,6])\n",
    "b = np.array([7,8,9])\n",
    "\n",
    "print('Types of vector multiplication')\n",
    "print('inner:', np.inner(a,b)) # dot prod; dims are: [1x3][3x1]=[1x1] <-- output dim\n",
    "print('outer:', np.outer(a,b)) # dims are [3x1][1x3]=[3x3] <-- output dim\n",
    "print('hadamard:', a * b) # elementwise (or hadamard) product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265a5d1",
   "metadata": {},
   "source": [
    "### Gram-Schmidt Process\n",
    "\n",
    "Use this to orthonormalise anything (vector or matrix (orthogonalise))\n",
    "\n",
    "- Orthonormalise a set of vectors $\\{\\bar{v_1}, \\bar{v_2}, \\bar{v_3}, ..., \\bar{v_n}\\}$ \n",
    "    - to $\\{\\bar{u_1}, \\bar{u_2}, \\bar{u_3}, ..., \\bar{u_n}\\}$, where each $u$ vector is in the same $\\mathbb{R}^n$ vector space, \n",
    "        - but each vector is unit length, and \n",
    "        - is mutually orthogonal with other vectors\n",
    "\n",
    "I.e. Transform a set of vectors into a set of orthonormal vectors in the same vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b7651",
   "metadata": {},
   "source": [
    "## Matrix decompositions\n",
    "\n",
    "### Gaussian Elimination (or Decomposition?)\n",
    "\n",
    "- **Purpose**: We use Gaussian Elimination to simplify a system of linear equations, $Ax=b$ into *row echelon form* (or *reduced row echelon form*; which allows solving $Ax=b$ by simple inspection)\n",
    "- Application: \n",
    "    - Solving linear system $Ax=b$, \n",
    "    - Computing inverse matrices\n",
    "    - Computing rank\n",
    "    - Computing determinant\n",
    "    - **Elementary row operations**: Methods by which the above are done\n",
    "        - Swapping rows\n",
    "        - Scaling rows\n",
    "        - Adding rows to each other (i.e. creating linear combinations)\n",
    "        \n",
    "- **Row echelon form**: The first *non-zero* element from the left in each row (aka leading coefficient, pivot) is **always to the right of** the first *non-zero* element in the row above\n",
    "- **Reduced row echelon form**: Row echelon form whose pivots are $1$ and column containing pivots are $0$ elsewhere\n",
    "\n",
    "- Elementary row operation\n",
    "\n",
    "### LU Decomposition\n",
    "\n",
    "Like Gaussian Decomposition, but more computationally efficient\n",
    "\n",
    "Decompose any matrix $A$ (square or not) into:\n",
    "- A lower triangular matrix $L$\n",
    "- An upper triangular matrix $U$\n",
    "- Sometimes needing to reorder $A$ using a $P$ matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "029f0ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-0.49204441 -0.07917771  1.10139403  0.19962414]\n",
      " [-0.42790238 -0.98536345  0.3305584   1.15725279]\n",
      " [-1.84798501 -1.56122698 -2.02944482 -0.40656445]]\n",
      "\n",
      "P:\n",
      " [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "L:\n",
      " [[ 1.          0.          0.        ]\n",
      " [ 0.23155079  1.          0.        ]\n",
      " [ 0.26625996 -0.53940701  1.        ]]\n",
      "\n",
      "U:\n",
      " [[-1.84798501 -1.56122698 -2.02944482 -0.40656445]\n",
      " [ 0.         -0.62386012  0.80047795  1.25139311]\n",
      " [ 0.          0.          2.07353735  0.98288619]]\n",
      "\n",
      "----\n",
      "\n",
      "Recomposition: PLU = A:\n",
      " [[-0.49204441 -0.07917771  1.10139403  0.19962414]\n",
      " [-0.42790238 -0.98536345  0.3305584   1.15725279]\n",
      " [-1.84798501 -1.56122698 -2.02944482 -0.40656445]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(3,4)\n",
    "print('A:\\n', a)\n",
    "\n",
    "p,l,u = scipy.linalg.lu(a)\n",
    "print('\\nP:\\n', p)\n",
    "print('\\nL:\\n', l)\n",
    "print('\\nU:\\n', u)\n",
    "print('\\n----\\n\\nRecomposition: PLU = A:\\n', p@l@u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d62a29",
   "metadata": {},
   "source": [
    "### QR Decomposition\n",
    "\n",
    "Decompose a matrix $A$ into:\n",
    "- an orthogonal matrix $Q$\n",
    "- an upper triangular matrix $R$\n",
    "\n",
    "It's used in QR algorithms to solve the linear least square problem. \n",
    "\n",
    "Also, the $Q$ matrix is sometimes what we desire after the **Gram-Schmidt process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dda371d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ 0.37217767  0.3206812   0.65309184 -1.0867384 ]\n",
      " [-0.83743001  0.99418817  0.34573079  0.32837543]\n",
      " [-1.81760742 -0.94519082  0.75863833  1.79588125]]\n",
      "\n",
      "Q:\n",
      " [[-0.18283803 -0.17463965  0.96750775]\n",
      " [ 0.41140043 -0.90738447 -0.08604133]\n",
      " [ 0.89292773  0.38230148  0.23775122]]\n",
      "\n",
      "R:\n",
      " [[-2.03555937 -0.49361037  0.70023297  1.93738308]\n",
      " [ 0.         -1.31946241 -0.13773792  0.57839291]\n",
      " [ 0.          0.          0.78249147 -0.65270874]]\n",
      "\n",
      "----\n",
      "\n",
      "Recomposition: QR = A:\n",
      " [[ 0.37217767  0.3206812   0.65309184 -1.0867384 ]\n",
      " [-0.83743001  0.99418817  0.34573079  0.32837543]\n",
      " [-1.81760742 -0.94519082  0.75863833  1.79588125]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(3,4)\n",
    "print('A:\\n', a)\n",
    "\n",
    "q,r = np.linalg.qr(a)\n",
    "print('\\nQ:\\n', q)\n",
    "print('\\nR:\\n', r)\n",
    "print('\\n----\\n\\nRecomposition: QR = A:\\n', q@r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ccbc55",
   "metadata": {},
   "source": [
    "### Cholesky Decomposition\n",
    "\n",
    "Decompose a symmetric (or Hermitian) positive-definite matrix into:\n",
    "\n",
    "- a lower triangular matrix $L$\n",
    "- and its transpose (or conjugate transpose) $L.H.$\n",
    "\n",
    "Used in algorithms for numerical convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20a0a2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " [[1 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 4]]\n",
      "\n",
      "L:\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.         1.41421356 0.         0.        ]\n",
      " [0.         0.         1.73205081 0.        ]\n",
      " [0.         0.         0.         2.        ]]\n",
      "\n",
      "----\n",
      "\n",
      "Recomposition: LL^T:\n",
      " [[1. 0. 0. 0.]\n",
      " [0. 2. 0. 0.]\n",
      " [0. 0. 3. 0.]\n",
      " [0. 0. 0. 4.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.diagflat([[1,2], [3,4]])\n",
    "print('x:\\n', x)\n",
    "\n",
    "L = np.linalg.cholesky(x)\n",
    "print('\\nL:\\n', L)\n",
    "\n",
    "print('\\n----\\n\\nRecomposition: LL^T:\\n', L @ L.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd36bfa",
   "metadata": {},
   "source": [
    "## Eigen-decomposition, Diagonalisation, and the Characteristic Polynomial\n",
    "\n",
    "A non-zero vector $v$ of dim. $\\mathbb{R}^n$ is an **eigenvector of square matrix $A : A\\in \\mathbb{R}^{n\\times n}$** \n",
    "- if it satisfies a linear equation of the form: $Av = \\lambda v$; for some scalar $\\lambda$\n",
    "    - This is called the eigenvalue equation/problem\n",
    "    - **Geometric intuition**: The eigenvectors of $A$ are the vectors which $A$ only elongates or shrinks. \n",
    "        - The amount of this elongation/shrink is $\\lambda$\n",
    "- Rearranging the eigenvalue problem:\n",
    "\n",
    "$$ \\begin{align*} \n",
    "Av&=\\lambda v\\\\ \n",
    "(A-\\lambda I) v &= 0\\\\ \\\\\n",
    "\\text{Since } v &\\ne 0 \\text{, solve for } \\\\\n",
    "p(\\lambda) = \\text{det}(A-\\lambda I)&=0\\\\\n",
    "\\end{align*} $$\n",
    "\n",
    "Solving for $\\lambda$ will be in a polynomial form (because of how determinants are calculated). $p(\\lambda) is the **characteristic polynomial**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f1a9d",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "- When exactly do we use decompositions?\n",
    "- What is the intuition behind an eigenvalue and eigenvector?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
