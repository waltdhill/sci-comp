{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d0f0a1",
   "metadata": {},
   "source": [
    "## 1. Recap: A few ways to multiply vectors and matrices\n",
    "\n",
    "### 1.1. Vector multiplication operations (4 approaches)\n",
    "\n",
    "Given we have 2 vectors, $\\textbf{a}$ and $\\textbf{b}$, of same length (i.e. $\\textbf{a}, \\textbf{b}\\in\\mathbb{R}^{n}$,), we can \"multiply\" them in the following ways:\n",
    "\n",
    "1. Vector dot (inner) product: $\\textbf{a} \\cdot \\textbf{b} = \\textbf{a}^T\\textbf{b} = \\Sigma_{i=1}^n a_i b_i = \\Vert\\textbf{a}\\Vert\\:\\Vert\\textbf{b}\\Vert \\cos \\theta$\n",
    "2. Vector outer product: $\\textbf{a} \\otimes \\textbf{b} = \\textbf{a} \\textbf{b}^T$. The resultant matrix is of size $n \\times n$ and its elements are given by: $ (\\textbf{a} \\otimes \\textbf{b})_{ij} = a_i b_j$\n",
    "3. Vector Hadamard (aka element-wise) product: $\\mathbf{a}\\odot \\mathbf{b}$. Elements of the resultant vector are given by: $ (\\mathbf{a}\\odot \\mathbf{b})_{i} = (\\mathbf{a})_{i}(\\mathbf{b})_{i} $\n",
    "4. Vector cross product: $\\mathbf{a} \\times \\mathbf{b} = \\Vert \\mathbf{a} \\Vert \\: \\Vert \\mathbf{b} \\Vert\\sin{(\\theta)} \\, \\mathbf{n}$\n",
    "\n",
    "### 1.2. Matrix multiplication operations (4 approaches)\n",
    "\n",
    "Given we have a matrix, $\\textbf{A}:\\textbf{A}\\in\\mathbb{R}^{m \\times n}$, following are a few multiplication operations involving $\\textbf{A}$. <mark>**NB: inner dimensions must match!**</mark>\n",
    "1. Matrix $\\textbf{A}$ and a vector:\n",
    "   1. Matrix times vector: $\\textbf{Av}$, where vector $\\textbf{v}\\in\\mathbb{R}^{n \\times 1}$. Hence, resultant column vector $\\textbf{Av}\\in\\mathbb{R}^{m \\times 1}$\n",
    "   2. Vector times matrix: $\\textbf{w}^\\textrm{T}\\textbf{A}$, where vector $\\textbf{w}\\in\\mathbb{R}^{1 \\times m}$. Hence, resultant row vector $\\textbf{w}^\\textrm{T}\\textbf{A}\\in\\mathbb{R}^{1 \\times n}$\n",
    "2. Matrix $\\textbf{A}$ and another matrix \n",
    "   1. Matrix Hadamard (aka element-wise) product: $\\mathbf{A}\\odot \\mathbf{B}$, where $\\textbf{A},\\textbf{B}\\in\\mathbb{R}^{m \\times n}$. Elements of the resultant matrix are given by: $ (A\\odot B)_{ij} = (A)_{ij}(B)_{ij} $\n",
    "   2. Matrix multiplication: $\\textbf{AB}$, where $\\textbf{A}\\in\\mathbb{R}^{m \\times p}$ and $\\textbf{B}\\in\\mathbb{R}^{p \\times n}$. Hence, inner dimensions match, and resultant matrix $\\textbf{AB}\\in\\mathbb{R}^{m \\times n}$\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ae22b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given vectors a: [4 5 6] and b: [7 8 9]\n",
      "\n",
      "4 types of vector multiplication\n",
      "- Inner (aka dot) product: a•b = (a^T)b = 122\n",
      "- Hadamard (elementwise) product: a⊙b [28 40 54]\n",
      "- Cross product, a⨉b: [-3  6 -3]\n",
      "- Outer product, a[3⨉1] ⨂ b[1⨉3]:\n",
      " [[28 32 36]\n",
      " [35 40 45]\n",
      " [42 48 54]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # more basic functionality\n",
    "import scipy  # advanced functionality, built on numpy\n",
    "\n",
    "# Find the inner and outer products of two 1D arrays (not exactly vectors, no double [[]])\n",
    "a = np.array([4, 5, 6])\n",
    "b = np.array([7, 8, 9])\n",
    "\n",
    "print(\"Given vectors a:\", a, \"and b:\", b)\n",
    "\n",
    "print(\"\\n4 types of vector multiplication\")\n",
    "print(\n",
    "    \"- Inner (aka dot) product: a•b = (a^T)b =\", np.inner(a, b)\n",
    ")  # dot prod; dims are: [1x3][3x1]=[1x1] <-- output dim, scalar\n",
    "print(\"- Hadamard (elementwise) product: a⊙b\", a * b)  # elementwise (or hadamard) product\n",
    "print(\"- Cross product, a⨉b:\", np.cross(a, b))\n",
    "print(\"- Outer product, a[3⨉1] ⨂ b[1⨉3]:\\n\", np.outer(a, b))  # dims are [3x1][1x3]=[3x3] <-- output dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97349eed",
   "metadata": {},
   "source": [
    "## 2. More terminology:\n",
    "\n",
    "### 2.1. Spans and spaces\n",
    "\n",
    "- **Span**: The span of a set of vectors is **all linear combinations of those vectors**\n",
    "- **Vector space** is denoted as $\\mathbb{R}^n$. \n",
    "    - Every element in a vector space can be written as a **linear combination** of the elements in the **basis** (unit) vectors\n",
    "        - **Basis (unit) vectors** (example): For a 2D vector space, the basis vectors are $\\hat{i} = \\left[\\begin{smallmatrix} 1 \\\\ 0 \\end{smallmatrix}\\right]$ and $\\hat{j} = \\left[\\begin{smallmatrix} 0 \\\\ 1 \\end{smallmatrix}\\right]$\n",
    "        - A matrix $\\textbf{A}$ applies a linear transformation to a vector space (i.e. all vectors in the space)\n",
    "        - <mark>The columns of</mark> $\\textbf{A}$ <mark>represent the **landing points** for the basis (unit) vectors **after the transformation**</mark>\n",
    "            - By extension, $\\textbf{A}$ moves **every input vector** (more precisely, the **point where every vector's tip is**) **linearly** to a new location.\n",
    "        - We only need to know how $\\textbf{A}$ transforms the bases $\\hat{i}$ and $\\hat{j}$, since\n",
    "            - any other vector $\\textbf{v}$ is <mark>just a **linear combination** of $\\hat{i}$ and $\\hat{j}$ **both before and after being transformed by $\\textbf{A}$**</mark>\n",
    "- **Subspace**: a subset of a larger vector space\n",
    "    - **Column space** (aka *range*, or *image*): Span of all column vectors of of matrix $\\textbf{A}$\n",
    "    - **Row space**: Span of all row vectors of matrix $\\textbf{A}$\n",
    "    - **Null space**: If $\\textbf{A} \\cdot \\textbf{x} = 0$, the span of all solutions $\\textbf{x}$ constitutes the **null space** of $\\textbf{A}$\n",
    "    - **Left-Null space**: If $\\textbf{A}^\\textrm{T} \\cdot \\textbf{x} = 0$, the span of all solutions $\\textbf{x}$ constitutes the **left-null space** of $\\textbf{A}$\n",
    "    \n",
    "### 2.2. More properties\n",
    "\n",
    "- **Rank**: The number of **linearly independent** columns (or rows) in $\\textbf{A}$ is its rank\n",
    "- **Orthonormal vectors**: Two unit length vectors whose inner (i.e. dot) products are 0 (e.g. $\\hat{i}$ and $\\hat{j}$)\n",
    "- **Real value matrices**:\n",
    "    - **Orthogonal matrices**: If $\\textbf{A}$'s rows and cols are orthonormal vectors, $\\textbf{A}$ is an orthogonal matrix. It satisfies:\n",
    "        - $\\textbf{A}^\\textrm{T}\\textbf{A} = \\textbf{AA}^\\textrm{T} = \\textbf{I}$\n",
    "    - **Symmetric matrix**: where $\\textbf{A}^\\textrm{T} = \\textbf{A}$ (square matrices only) \n",
    "- **Complex value matrices**\n",
    "    - Hermitian matrix: Complex matrices' analog to orthogonal matrix\n",
    "    - Unitary matrix: Complex matrices' analog to symmetric matrix\n",
    "- **Determinant**: This can be computed for any square matrix $\\textbf{A}$\n",
    "    - Matrices are only invertible if $det(\\textbf{A}) \\ne 0$. Such matrices are **non-singular**; and satisfy $\\textbf{AA}^{-1}=\\textbf{I}$\n",
    "    - Matrices where $det(\\textbf{A}) = 0$ are not invertible. They are **singular** matrices\n",
    "  \n",
    "### 2.3. Examples of the above (where relevant):\n",
    "\n",
    "- The **span** of vectors $\\left[\\begin{smallmatrix} 1 \\\\ 0 \\end{smallmatrix}\\right]$ and $\\left[\\begin{smallmatrix} 0 \\\\ 1 \\end{smallmatrix}\\right]$ is the whole $x$-$y$ plane.\n",
    "- A vector, $\\textbf{v}$, with 3 elements is said to exist in **vector space** $\\mathbb{R}^3$\n",
    "- The $\\mathbb{R}^3$ vector, $\\textbf{v}$ is composed of (i.e. a linear combination of) the **basis vectors** $\\hat{i} = \\left[\\begin{smallmatrix} 1 \\\\ 0 \\\\ 0 \\end{smallmatrix}\\right]$, $\\hat{j} = \\left[\\begin{smallmatrix} 0 \\\\ 1 \\\\ 0 \\end{smallmatrix}\\right]$ and $\\hat{k} = \\left[\\begin{smallmatrix} 0 \\\\ 0 \\\\ 1 \\end{smallmatrix}\\right]$.\n",
    "- The subspace of a 3D vector (in $\\mathbb{R}^3$) is the span of vectors $\\left[\\begin{smallmatrix} 1 \\\\ 0 \\\\ 0 \\end{smallmatrix}\\right]$, $\\left[\\begin{smallmatrix} 0 \\\\ 1 \\\\ 0 \\end{smallmatrix}\\right]$.\n",
    "    - in this case the 2D $x$-$y$ plane is a subspace (subset) of the 3D $x, y, z$ vector space\n",
    "- Vectors $\\left[\\begin{smallmatrix} 1 \\\\ 2 \\\\ 3 \\end{smallmatrix}\\right]$ and $\\left[\\begin{smallmatrix} 10 \\\\ 20 \\\\ 30 \\end{smallmatrix}\\right]$ are linearly dependent since one is a multiple of the other.\n",
    "    - A matrix with those two vectors would be rank 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02005fb4",
   "metadata": {},
   "source": [
    "# TODO: Kroenecker product, Tensors, Tensor products, \n",
    "\n",
    "https://en.wikipedia.org/wiki/Tensor\n",
    "https://en.wikipedia.org/wiki/Tensor_product\n",
    "https://en.wikipedia.org/wiki/Kronecker_product\n",
    "https://en.wikipedia.org/wiki/Block_matrix\n",
    "https://math.stackexchange.com/questions/973559/outer-product-of-two-matrices\n",
    "https://stackoverflow.com/questions/24839481/python-matrix-outer-product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265a5d1",
   "metadata": {},
   "source": [
    "## 3. Gram-Schmidt Process\n",
    "\n",
    "Use this to orthonormalise anything (vector or matrix (orthogonalise))\n",
    "\n",
    "- Orthonormalise a set of vectors $\\{\\textbf{v}_1, \\textbf{v}_2, \\textbf{v}_3, ..., \\textbf{v}_n\\}$ \n",
    "    - to $\\{\\textbf{u}_1, \\textbf{u}_2, \\textbf{u}_3, ..., \\textbf{u}_n\\}$, where each $\\textbf{u}_i$ vector is in the same $\\mathbb{R}^n$ vector space, \n",
    "        - but each $\\textbf{u}_i$ vector is unit length, and \n",
    "        - is mutually orthogonal with other vectors\n",
    "\n",
    "I.e. Transform a set of vectors into a set of orthonormal vectors in the same vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b7651",
   "metadata": {},
   "source": [
    "## 4. Matrix decompositions\n",
    "\n",
    "### 4.1. Gaussian Elimination (or Decomposition?)\n",
    "\n",
    "- **Purpose**: We use Gaussian Elimination to simplify a system of linear equations, $Ax=b$ into *row echelon form* (or *reduced row echelon form*; which allows solving $Ax=b$ by simple inspection)\n",
    "- Application: \n",
    "    - Solving linear system $Ax=b$, \n",
    "    - Computing inverse matrices\n",
    "    - Computing rank\n",
    "    - Computing determinant\n",
    "    - **Elementary row operations**: Methods by which the above are done\n",
    "        - Swapping rows\n",
    "        - Scaling rows\n",
    "        - Adding rows to each other (i.e. creating linear combinations)\n",
    "        \n",
    "- **Row echelon form**: The first *non-zero* element from the left in each row (aka leading coefficient, pivot) is **always to the right of** the first *non-zero* element in the row above\n",
    "- **Reduced row echelon form**: Row echelon form whose pivots are $1$ and column containing pivots are $0$ elsewhere\n",
    "\n",
    "- Elementary row operation\n",
    "\n",
    "### 4.2. LU Decomposition\n",
    "\n",
    "Like Gaussian Decomposition, but more computationally efficient\n",
    "\n",
    "Decompose any matrix $A$ (square or not) into:\n",
    "- A lower triangular matrix $L$\n",
    "- An upper triangular matrix $U$\n",
    "- Sometimes needing to reorder $A$ using a $P$ matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029f0ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(3, 4)\n",
    "print(\"A:\\n\", a)\n",
    "\n",
    "p, l, u = scipy.linalg.lu(a)\n",
    "print(\"\\nP:\\n\", p)\n",
    "print(\"\\nL:\\n\", l)\n",
    "print(\"\\nU:\\n\", u)\n",
    "print(\"\\n----\\n\\nRecomposition: PLU = A:\\n\", p @ l @ u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d62a29",
   "metadata": {},
   "source": [
    "### 4.3. QR Decomposition\n",
    "\n",
    "Decompose a matrix $A$ into:\n",
    "- an orthogonal matrix $Q$\n",
    "- an upper triangular matrix $R$\n",
    "\n",
    "It's used in QR algorithms to solve the linear least square problem. \n",
    "\n",
    "Also, the $Q$ matrix is sometimes what we desire after the **Gram-Schmidt process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda371d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(3, 4)\n",
    "print(\"A:\\n\", a)\n",
    "\n",
    "q, r = np.linalg.qr(a)\n",
    "print(\"\\nQ:\\n\", q)\n",
    "print(\"\\nR:\\n\", r)\n",
    "print(\"\\n----\\n\\nRecomposition: QR = A:\\n\", q @ r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ccbc55",
   "metadata": {},
   "source": [
    "### 4.4. Cholesky Decomposition\n",
    "\n",
    "Decompose a symmetric (or Hermitian) positive-definite matrix into:\n",
    "\n",
    "- a lower triangular matrix $L$\n",
    "- and its transpose (or conjugate transpose) $L.H.$\n",
    "\n",
    "Used in algorithms for numerical convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.diagflat([[1, 2], [3, 4]])\n",
    "print(\"x:\\n\", x)\n",
    "\n",
    "L = np.linalg.cholesky(x)\n",
    "print(\"\\nL:\\n\", L)\n",
    "\n",
    "print(\"\\n----\\n\\nRecomposition: LL^T:\\n\", L @ L.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f1a9d",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "- When exactly do we use decompositions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
