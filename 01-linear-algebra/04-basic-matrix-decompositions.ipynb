{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d0f0a1",
   "metadata": {},
   "source": [
    "## 1. Recap: A few ways to multiply vectors and matrices\n",
    "\n",
    "### 1.1. Vector multiplication operations (4 approaches)\n",
    "\n",
    "Given we have 2 vectors, $\\textbf{a}$ and $\\textbf{b}$, of same length (i.e. $\\textbf{a}, \\textbf{b}\\in\\mathbb{R}^{n}$,), we can \"multiply\" them in the following ways:\n",
    "\n",
    "1. Vector dot (inner) product: $\\textbf{a} \\cdot \\textbf{b} = \\textbf{a}^T\\textbf{b} = \\Sigma_{i=1}^n a_i b_i = \\Vert\\textbf{a}\\Vert\\:\\Vert\\textbf{b}\\Vert \\cos \\theta$\n",
    "2. Vector outer product: $\\textbf{a} \\otimes \\textbf{b} = \\textbf{a} \\textbf{b}^T$. The resultant matrix is of size $n \\times n$ and its elements are given by: $ (\\textbf{a} \\otimes \\textbf{b})_{ij} = a_i b_j$\n",
    "3. Vector Hadamard (aka element-wise) product: $\\textbf{a}\\odot \\textbf{b}$. Elements of the resultant vector are given by: $ (\\textbf{a}\\odot \\textbf{b})_{i} = (\\textbf{a})_{i}(\\textbf{b})_{i} $\n",
    "4. Vector cross product: $\\textbf{a} \\times \\textbf{b} = \\Vert \\textbf{a} \\Vert \\: \\Vert \\textbf{b} \\Vert\\sin{(\\theta)} \\, \\textbf{n}$\n",
    "\n",
    "### 1.2. Matrix multiplication operations (4 approaches)\n",
    "\n",
    "Given we have a matrix, $\\textbf{A}:\\textbf{A}\\in\\mathbb{R}^{m \\times n}$, following are a few multiplication operations involving $\\textbf{A}$. <mark>**NB: inner dimensions must match!**</mark>\n",
    "1. Matrix $\\textbf{A}$ and some vector (examples: column vector $\\textbf{v}$, and row vector $\\textbf{w}^\\textrm{T}$):\n",
    "   1. Matrix times vector: $\\textbf{Av}$, where vector $\\textbf{v}\\in\\mathbb{R}^{n \\times 1}$. Hence, resultant column vector $\\textbf{Av}\\in\\mathbb{R}^{m \\times 1}$\n",
    "   2. Vector times matrix: $\\textbf{w}^\\textrm{T}\\textbf{A}$, where vector $\\textbf{w}^\\textrm{T}\\in\\mathbb{R}^{1 \\times m}$. Hence, resultant row vector $\\textbf{w}^\\textrm{T}\\textbf{A}\\in\\mathbb{R}^{1 \\times n}$\n",
    "2. Matrix $\\textbf{A}$ and some matrix (examples: $\\textbf{A}$ and $\\textbf{B}$ matrices are of same size, but $\\textbf{C}$ matrix has different size):\n",
    "   1. Matrix Hadamard (aka element-wise) product: $\\textbf{A}\\odot \\textbf{B}$, where $\\textbf{A},\\textbf{B}\\in\\mathbb{R}^{m \\times n}$. Elements of the resultant matrix are given by: $ (A\\odot B)_{ij} = (A)_{ij}(B)_{ij} $\n",
    "   2. Matrix multiplication: $\\textbf{AC}$, where $\\textbf{A}\\in\\mathbb{R}^{m \\times p}$ and $\\textbf{C}\\in\\mathbb{R}^{p \\times n}$. Hence, inner dimensions match, and resultant matrix $\\textbf{AC}\\in\\mathbb{R}^{m \\times n}$\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66ae22b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given vectors a: [4 5 6] and b: [7 8 9]\n",
      "\n",
      "4 types of vector multiplication\n",
      "- Inner (aka dot) product: a•b = (a^T)b = 122\n",
      "- Hadamard (elementwise) product: a⊙b [28 40 54]\n",
      "- Cross product, a⨉b: [-3  6 -3]\n",
      "- Outer product, a[3⨉1] ⨂ b[1⨉3]:\n",
      " [[28 32 36]\n",
      " [35 40 45]\n",
      " [42 48 54]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # more basic functionality\n",
    "import scipy  # advanced functionality, built on numpy\n",
    "\n",
    "# Find the inner and outer products of two 1D arrays (not exactly vectors, no double [[]])\n",
    "a = np.array([4, 5, 6])\n",
    "b = np.array([7, 8, 9])\n",
    "\n",
    "print(\"Given vectors a:\", a, \"and b:\", b)\n",
    "\n",
    "print(\"\\n4 types of vector multiplication\")\n",
    "print(\n",
    "    \"- Inner (aka dot) product: a•b = (a^T)b =\", np.inner(a, b)\n",
    ")  # dot prod; dims are: [1x3][3x1]=[1x1] <-- output dim, scalar\n",
    "print(\"- Hadamard (elementwise) product: a⊙b\", a * b)  # elementwise (or hadamard) product\n",
    "print(\"- Cross product, a⨉b:\", np.cross(a, b))\n",
    "print(\"- Outer product, a[3⨉1] ⨂ b[1⨉3]:\\n\", np.outer(a, b))  # dims are [3x1][1x3]=[3x3] <-- output dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265a5d1",
   "metadata": {},
   "source": [
    "## 3. Gram-Schmidt Process\n",
    "\n",
    "Use this to orthonormalise anything (vector or matrix (orthogonalise))\n",
    "\n",
    "- Orthonormalise a set of vectors $\\{\\textbf{v}_1, \\textbf{v}_2, \\textbf{v}_3, ..., \\textbf{v}_n\\}$ \n",
    "    - to $\\{\\textbf{u}_1, \\textbf{u}_2, \\textbf{u}_3, ..., \\textbf{u}_n\\}$, where each $\\textbf{u}_i$ vector is in the same $\\mathbb{R}^n$ vector space, \n",
    "        - but each $\\textbf{u}_i$ vector is unit length, and \n",
    "        - is mutually orthogonal with other vectors\n",
    "\n",
    "I.e. Transform a set of vectors into a set of orthonormal vectors in the same vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b7651",
   "metadata": {},
   "source": [
    "## 4. Matrix decompositions\n",
    "\n",
    "### 4.1. Gaussian Elimination (or Decomposition?)\n",
    "\n",
    "- **Purpose**: We use Gaussian Elimination to simplify a system of linear equations, $\\textbf{Ax=b}$ into *row echelon form* (or *reduced row echelon form*; which allows solving $\\textbf{Ax=b}$ by simple inspection)\n",
    "- Application: \n",
    "    - Solving linear system $\\textbf{Ax=b}$, \n",
    "    - Computing inverse matrices\n",
    "    - Computing rank\n",
    "    - Computing determinant\n",
    "    - **Elementary row operations**: Methods by which the above are done\n",
    "        - Swapping rows\n",
    "        - Scaling rows\n",
    "        - Adding rows to each other (i.e. creating linear combinations)\n",
    "        \n",
    "- **Row echelon form**: The first *non-zero* element from the left in each row (aka leading coefficient, pivot) is **always to the right of** the first *non-zero* element in the row above\n",
    "- **Reduced row echelon form**: Row echelon form whose pivots are $1$ and column containing pivots are $0$ elsewhere\n",
    "\n",
    "- Elementary row operation\n",
    "\n",
    "### 4.2. LU Decomposition\n",
    "\n",
    "Like Gaussian Decomposition, but more computationally efficient\n",
    "\n",
    "Decompose any matrix $\\textbf{A}$ (square or not) into:\n",
    "- A lower triangular matrix $\\textbf{L}$\n",
    "- An upper triangular matrix $\\textbf{U}$\n",
    "- Sometimes needing to reorder $\\textbf{A}$ using a $\\textbf{P}$ matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "029f0ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[-1.42490189 -0.91221588 -1.93062968 -0.36094733]\n",
      " [-0.53549468 -0.13589574  0.47988497  2.88155355]\n",
      " [-0.00873341  1.07556415  1.3429852  -0.12939715]]\n",
      "\n",
      "P:\n",
      " [[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "\n",
      "L:\n",
      " [[1.         0.         0.        ]\n",
      " [0.00612913 1.         0.        ]\n",
      " [0.37581162 0.19139304 1.        ]]\n",
      "\n",
      "U:\n",
      " [[-1.42490189 -0.91221588 -1.93062968 -0.36094733]\n",
      " [ 0.          1.08115524  1.35481827 -0.12718486]\n",
      " [ 0.          0.          0.94613525  3.04154405]]\n",
      "\n",
      "----\n",
      "\n",
      "Recomposition: PLU = A:\n",
      " [[-1.42490189 -0.91221588 -1.93062968 -0.36094733]\n",
      " [-0.53549468 -0.13589574  0.47988497  2.88155355]\n",
      " [-0.00873341  1.07556415  1.3429852  -0.12939715]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(3, 4)\n",
    "print(\"A:\\n\", a)\n",
    "\n",
    "p, l, u = scipy.linalg.lu(a)\n",
    "print(\"\\nP:\\n\", p)\n",
    "print(\"\\nL:\\n\", l)\n",
    "print(\"\\nU:\\n\", u)\n",
    "print(\"\\n----\\n\\nRecomposition: PLU = A:\\n\", p @ l @ u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d62a29",
   "metadata": {},
   "source": [
    "### 4.3. QR Decomposition\n",
    "\n",
    "Decompose a matrix $\\textbf{A}$ into:\n",
    "- an orthogonal matrix $\\textbf{Q}$\n",
    "- an upper triangular matrix $\\textbf{R}$\n",
    "\n",
    "It's used in QR algorithms to solve the linear least square problem. \n",
    "\n",
    "Also, the $\\textbf{Q}$ matrix is sometimes what we desire after the **Gram-Schmidt process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda371d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ 0.1896593   0.18586154 -1.09719411  1.42682447]\n",
      " [-0.76706589  1.29836654  0.61490277  0.26874477]\n",
      " [-0.82562671 -0.88687333 -0.79760309  0.90900003]]\n",
      "\n",
      "Q:\n",
      " [[-0.16595838 -0.13945844  0.97622188]\n",
      " [ 0.67120893 -0.74122266  0.0082184 ]\n",
      " [ 0.72245166  0.65661275  0.21661787]]\n",
      "\n",
      "R:\n",
      " [[-1.1428124   0.19990683  0.01858711  0.60029899]\n",
      " [ 0.         -1.57063101 -0.82648325  0.19867857]\n",
      " [ 0.          0.         -1.23882645  1.59201156]]\n",
      "\n",
      "----\n",
      "\n",
      "Recomposition: QR = A:\n",
      " [[ 0.1896593   0.18586154 -1.09719411  1.42682447]\n",
      " [-0.76706589  1.29836654  0.61490277  0.26874477]\n",
      " [-0.82562671 -0.88687333 -0.79760309  0.90900003]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(3, 4)\n",
    "print(\"A:\\n\", a)\n",
    "\n",
    "q, r = np.linalg.qr(a)\n",
    "print(\"\\nQ:\\n\", q)\n",
    "print(\"\\nR:\\n\", r)\n",
    "print(\"\\n----\\n\\nRecomposition: QR = A:\\n\", q @ r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ccbc55",
   "metadata": {},
   "source": [
    "### 4.4. Cholesky Decomposition\n",
    "\n",
    "Decompose a symmetric (or Hermitian) positive-definite matrix into:\n",
    "\n",
    "- a lower triangular matrix $\\textbf{L}$\n",
    "- and its transpose (or conjugate transpose) $\\textbf{L.H.}$\n",
    "\n",
    "Used in algorithms for numerical convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20a0a2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " [[1 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 4]]\n",
      "\n",
      "L:\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.         1.41421356 0.         0.        ]\n",
      " [0.         0.         1.73205081 0.        ]\n",
      " [0.         0.         0.         2.        ]]\n",
      "\n",
      "----\n",
      "\n",
      "Recomposition: LL^T:\n",
      " [[1. 0. 0. 0.]\n",
      " [0. 2. 0. 0.]\n",
      " [0. 0. 3. 0.]\n",
      " [0. 0. 0. 4.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.diagflat([[1, 2], [3, 4]])\n",
    "print(\"x:\\n\", x)\n",
    "\n",
    "L = np.linalg.cholesky(x)\n",
    "print(\"\\nL:\\n\", L)\n",
    "\n",
    "print(\"\\n----\\n\\nRecomposition: LL^T:\\n\", L @ L.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f1a9d",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "- When exactly do we use decompositions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
