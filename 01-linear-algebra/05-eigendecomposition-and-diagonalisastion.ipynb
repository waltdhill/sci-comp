{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eee0a08-d250-4e93-beea-c9a99ca81243",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- **Eigenvalues and Eigenvectors**\n",
    "    - Eigenvectors are the vectors that does not change its orientation when multiplied by the transition matrix, but it just scales by a factor of corresponding eigenvalues.\n",
    "- **Diagonalization & Eigendecomposition**\n",
    "    - A few applications of eigenvalues and eigenvectors that are very useful when handing the data in a matrix form because you could decompose them into matrices that are easy to manipulate.\n",
    "- **Underlying assumption behind the diagonalization and eigendecomposition**\n",
    "    - Make sure that the matrix you are trying to decompose is a square matrix and has linearly independent eigenvectors (different eigenvalues)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd36bfa",
   "metadata": {},
   "source": [
    "## Eigenvalue Problem and the Characteristic Polynomial\n",
    "\n",
    "A non-zero vector $\\textbf{v}$ of dim. $\\mathbb{R}^n$ is an **eigenvector of square matrix $\\textbf{A} : \\textbf{A}\\in \\mathbb{R}^{n\\times n}$** \n",
    "- if it satisfies a linear equation of the form: $\\textbf{Av} = \\lambda \\textbf{v}$; for some scalar $\\lambda$ <mark>which we are solving for</mark>\n",
    "    - This is called the eigenvalue equation/problem\n",
    "    - **Geometric intuition**: The eigenvector(s) of $\\textbf{A}$ are the vector(s) ($\\textbf{v}$) which $\\textbf{A}$ **only elongates/shrinks**, and never takes off it's span(s). \n",
    "        - The amount of this elongation/shrink is $\\lambda$, a scalar value\n",
    "- Rearranging the eigenvalue problem:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "\\textbf{Av}&=\\lambda \\textbf{v}\\\\ \n",
    "\\textbf{Av}&=(\\lambda \\textbf{I}) \\textbf{v} = \\left[\\begin{smallmatrix} \n",
    "                    \\lambda & 0 & \\dots & 0 \\\\ \n",
    "                    0 & \\lambda & \\dots & 0 \\\\ \n",
    "                    \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "                    0 & 0 & \\dots & \\lambda \\\\ \n",
    "                    \\end{smallmatrix}\\right] \\left[\\begin{smallmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{smallmatrix} \\right] \\\\\n",
    "(\\textbf{A}-\\lambda \\textbf{I}) \\textbf{v} &= \\textbf{0}\\\\\n",
    "\\end{align*} $$\n",
    "\n",
    "$\\text{Since }\\textbf{v} \\ne \\textbf{0}\\text{, solve for:}$\n",
    "\n",
    "$$ \\begin{align*}\n",
    "p(\\lambda) = \\text{det}(\\textbf{A}-\\lambda \\textbf{I})&=\\textbf{0}\\\\\n",
    "\\end{align*} $$\n",
    "\n",
    "- <mark>The **only way for $(\\textbf{A}-\\lambda \\textbf{I}) \\textbf{v} = \\textbf{0}$ to be possible** (given non-zero $\\textbf{v}$) is if $\\text{det}(\\textbf{A}-\\lambda \\textbf{I})=\\textbf{0}$</mark>\n",
    "    - i.e. The matrix $(\\textbf{A}-\\lambda \\textbf{I}$) represents a *linear transformation of the vector space which \"reduces\" its dimensionality* (at least 1 dim is lost)\n",
    "    - A matrix **cannot squish non-zero vectors into the $\\vec{0}$ vector, except when their determinant is 0**\n",
    "- By computing the determinant, we get the eigenvalues $\\lambda_1, \\lambda_2(, ..., \\lambda_n)$ (1 for each dimension of the square matrix).\n",
    "    - Computing $\\text{det}(\\textbf{A}-\\lambda \\textbf{I})$ requires solving a **characteristic polynomial** whose roots are the $\\lambda$(s)\n",
    " \n",
    "#### Why the $\\text{det}(\\textbf{A}-\\lambda \\textbf{I}) = \\textbf{0}$ observation matters\n",
    "- The observation that $\\text{det}(\\textbf{A}-\\lambda \\textbf{I})\\equiv \\textbf{0}$ is only useful because solving it yields the eigenvalues $\\lambda$s.\n",
    "    - Those helps us solve for the eigenvectors $\\textbf{v}$'s (i.e. those vectors that this diagonally altered matrix $(\\textbf{A}-\\lambda \\textbf{I})$ \"shrinks\" to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e466f403-1866-4cb2-8772-d8a8f70e40e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[4 2 2]\n",
      " [2 4 2]\n",
      " [2 2 4]]\n",
      "determinant(A): 32.0\n",
      "\n",
      "Eigenvalues - shape: (3,) values: [2. 8. 2.]\n",
      "\n",
      "Eigenvectors - shape: (3, 3) values:\n",
      " [[-0.82  0.41  0.41]\n",
      " [ 0.58  0.58  0.58]\n",
      " [ 0.51 -0.81  0.3 ]]\n",
      "Check against the Eigenvalue Problem Av = λv\n",
      "\n",
      "Av:\n",
      " [[-1.63299316  4.61880215  1.01339709]\n",
      " [ 0.81649658  4.61880215 -1.61564839]\n",
      " [ 0.81649658  4.61880215  0.6022513 ]]\n",
      "\n",
      "λv:\n",
      " [[-1.63299316  4.61880215  1.01339709]\n",
      " [ 0.81649658  4.61880215 -1.61564839]\n",
      " [ 0.81649658  4.61880215  0.6022513 ]]\n"
     ]
    }
   ],
   "source": [
    "# A = np.random.randn(4,4)\n",
    "# A = np.array([[3,1],\n",
    "#              [0,2]])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[4, 2, 2], [2, 4, 2], [2, 2, 4]])\n",
    "# A = np.diag((1,2,3))\n",
    "detA = np.linalg.det(A)\n",
    "\n",
    "print(\"A:\\n\", A)\n",
    "print(\"determinant(A):\", detA)\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(A)\n",
    "# eig_vals,eig_vecs = np.linalg.eigh(A)\n",
    "# eig_vals,eig_vecs = scipy.linalg.eig(A)\n",
    "\n",
    "print(\"\\nEigenvalues - shape:\", eig_vals.shape, \"values:\", np.round(eig_vals, 0))\n",
    "print(\n",
    "    \"\\nEigenvectors - shape:\", eig_vecs.shape, \"values:\\n\", np.round(eig_vecs.T, 2)\n",
    ")  # not sure why this is so different to Wolfram and others\n",
    "\n",
    "# Link 1: https://www.wolframalpha.com/input?i=eigenvectors+of+%7B%7B4%2C2%2C2%7D%2C%7B2%2C4%2C2%7D%2C%7B2%2C2%2C4%7D%7D\n",
    "# Link 2: https://matrixcalc.org/vectors.html#eigenvectors%28%7B%7B4,2,2%7D,%7B2,4,2%7D,%7B2,2,4%7D%7D%29\n",
    "\n",
    "# But let's test it against the eigenvalue problem: Av = λv (\n",
    "print(\"Check against the Eigenvalue Problem Av = λv\")\n",
    "print(\"\\nAv:\\n\", A @ eig_vecs)\n",
    "print(\"\\nλv:\\n\", eig_vals * eig_vecs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6aef64-d20e-492c-a79b-37f839e9edad",
   "metadata": {},
   "source": [
    "## Eigenbasis, Diagonalisation, and Eigen-decomposition\n",
    "\n",
    "## Eigenbasis\n",
    "- If our <mark>**basis vectors**</mark> ($\\hat{i} = \\left[\\begin{smallmatrix} 1 \\\\ 0 \\end{smallmatrix}\\right], \\hat{j} = \\left[\\begin{smallmatrix} 0 \\\\ 1 \\end{smallmatrix}\\right], \\dots$) <mark>are themselves **eigenvectors**</mark>, it is called an eigenbasis. \n",
    "- Then if we inspect $\\textbf{A}$, the transformation matrix, it will have the form known as a <mark>**Diagonal Matrix**</mark>:\n",
    "\n",
    "$$\\textbf{A}_{\\text{diag}} = \\left[\\begin{smallmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{smallmatrix}\\right]$$\n",
    "\n",
    "- Its form is <mark>(diagonal) because recall</mark> $\\textbf{A}$ <mark>**only scales** (stretch/shrink) its eigenvectors</mark>, which in this case are the basis vectors\n",
    "    - It is very easy to compute large powers of diagonal matrices (they simply scale vectors by the eigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56503247-ab66-4a84-aa98-6328156776a1",
   "metadata": {},
   "source": [
    "## <mark>Diagonalisation</mark>: Using the Eigenbasis to Diagonalise any non-diagonal $\\textbf{A}$\n",
    "1. Find the eigenvectors of $\\textbf{A}$\n",
    "2. Make a **change of basis** matrix $\\textbf{S}$ whose columns are the eigenvectors of $\\textbf{A}$. We'll use this to switch the coordinate system of $\\textbf{A}$\n",
    "3. Diagonalise $\\textbf{A}$ to get $\\Lambda$ by doing this:\n",
    "$$\\Lambda = \\textbf{S}^{-1}\\textbf{AS}$$\n",
    "4. The new matrix $\\Lambda$ is guaranteed to be diagonal, with its **eigenvalues on the main diagonal**\n",
    "\n",
    "#### Derivation: Why is diagonalisation $\\Lambda = \\textbf{S}^{-1}\\textbf{AS}$ possible in the first place?\n",
    "\n",
    "Show that $\\textbf{AS=S}\\Lambda$\n",
    "- Suppose we have $m$ linearly independent eigenvectors of $\\textbf{A}$;\n",
    "    - then $\\textbf{S}$ is a matrix, where each column is an eigenvector of $\\textbf{A}$, $\\textbf{v}_{1\\dots m}$\n",
    "    - $\\textbf{AS = A} \\left[\\begin{smallmatrix} v_1 & v_2 & \\dots & v_m \\end{smallmatrix}\\right] = \\left[\\begin{smallmatrix} \\textbf{A} v_1 & \\textbf{A} v_2 & \\dots & \\textbf{A} v_m \\end{smallmatrix}\\right] = \\left[\\begin{smallmatrix} \\lambda_1 v_1 & \\lambda_2 v_2 & \\dots & \\lambda_m v_m \\end{smallmatrix}\\right] $ (final step because recall $\\textbf{Av}=\\lambda $)\n",
    "- And so $\\left[\\begin{smallmatrix} \\lambda_1 v_1 & \\lambda_2 v_2 & \\dots & \\lambda_m v_m \\end{smallmatrix}\\right]$ = $\\left[\\begin{smallmatrix} \\ v_1 & \\ v_2 & \\dots & \\ v_m \\end{smallmatrix}\\right] \\left[\\begin{smallmatrix} \n",
    "\\ \\lambda_1 & 0 & \\dots & 0 \\\\\n",
    "\\ 0 & \\lambda_2 & \\dots & 0 \\\\\n",
    "\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\ 0 & 0 & \\dots & \\lambda_m \\\\\n",
    "\\end{smallmatrix}\\right] $ which is $\\textbf{S}\\Lambda$\n",
    "\n",
    "#### Assumptions:\n",
    "\n",
    "The matrix $\\textbf{A}$ you are trying to decompose must:\n",
    "- be a square matrix\n",
    "- have linearly independent eigenvectors (different eigenvalues, 1 for each row of the matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2065feec-1471-449f-ad8f-9a0e5fd6dae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall A:\n",
      " [[4 2 2]\n",
      " [2 4 2]\n",
      " [2 2 4]]\n",
      "\n",
      "And as calculated earlier for A:\n",
      "\n",
      "Eigenvalues: [2. 8. 2.]\n",
      "\n",
      "Eigenvectors:\n",
      " [[-0.82  0.41  0.41]\n",
      " [ 0.58  0.58  0.58]\n",
      " [ 0.51 -0.81  0.3 ]]\n",
      "\n",
      "------\n",
      "\n",
      "Lambda = S^-1 @ A @ S:\n",
      " [[ 2.  0. -0.]\n",
      " [ 0.  8. -0.]\n",
      " [-0. -0.  2.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall A:\\n\", A)\n",
    "print(\"\\nAnd as calculated earlier for A:\")\n",
    "print(\"\\nEigenvalues:\", np.round(eig_vals, 0))\n",
    "print(\"\\nEigenvectors:\\n\", np.round(eig_vecs.T, 2))\n",
    "print(\"\\n------\\n\")\n",
    "\n",
    "# Define a change of basis matrix S, and then diagonalise A using Lambda = S^(-1) A S\n",
    "S = eig_vecs\n",
    "Lambda = np.linalg.inv(S) @ A @ S\n",
    "\n",
    "print(\"Lambda = S^-1 @ A @ S:\\n\", np.round(Lambda, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e9a039-5489-462f-a102-4f4ffd066ef1",
   "metadata": {},
   "source": [
    "## Now that we know $\\textbf{AS=S}\\Lambda$, we can do:\n",
    "\n",
    "### Diagonalisation:\n",
    "\n",
    "- <mark>Takes $\\textbf{A}$ matrix to produce $\\Lambda$, a diagonal matrix with **eigenvalues on the main diagonal**</mark>\n",
    "- Multiply both sides by $\\textbf{S}^{-1}$ **from the left**. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\textbf{S}^{-1}\\times \\textbf{AS} &= \\textbf{S}^{-1}\\times S\\Lambda \\\\\n",
    "\\textbf{S}^{-1}\\textbf{AS} &= \\textbf{S}^{-1}\\textbf{S}\\Lambda \\\\\n",
    "\\textbf{S}^{-1}\\textbf{AS} &= \\Lambda \\\\ \\\\\n",
    "\\Lambda &= \\textbf{S}^{-1}\\textbf{AS}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Eigendecomposition\n",
    "\n",
    "- After diagonalising $\\textbf{A}$ to get $\\Lambda$, <mark>we can use eigendecomposition to do quick matrix multiplications of $\\textbf{A}$</mark>\n",
    "- Multiply both sides by $\\textbf{S}^{-1}$ **from the right**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\textbf{AS} \\times \\textbf{S}^{-1} &= \\textbf{S}\\Lambda \\times \\textbf{S}^{-1} \\\\\n",
    "\\textbf{ASS}^{-1} &= \\textbf{S}\\Lambda \\textbf{S}^{-1} \\\\\n",
    "\\textbf{A} &= \\textbf{S}\\Lambda \\textbf{S}^{-1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\text{Now note, if we raise \\textbf{A} to arbitrary powers}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\textbf{A}^2 &= (\\textbf{S}\\Lambda \\textbf{S}^{-1})(\\textbf{S}\\Lambda \\textbf{S}^{-1}) \\\\\n",
    "&= \\textbf{S}\\Lambda (\\textbf{S}^{-1}\\textbf{S})\\Lambda \\textbf{S}^{-1} = \\textbf{S}\\Lambda^2 \\textbf{S}^{-1}\\\\\\\\\n",
    "\\text{In general } \\textbf{A}^k &= \\textbf{S}\\Lambda^k \\textbf{S}^{-1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420599d0-33a9-4b8b-923b-b0844aff59b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_eigendecomposed = S @ Lambda @ S^-1:\n",
      " [[4. 2. 2.]\n",
      " [2. 4. 2.]\n",
      " [2. 2. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# Perform eigendecomposition to recover A from its eigenvectors and eigenvalues\n",
    "A_eig_decomp = S @ Lambda @ np.linalg.inv(S)\n",
    "print(\"A_eigendecomposed = S @ Lambda @ S^-1:\\n\", A_eig_decomp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f1a9d",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "- When exactly do we use decompositions?\n",
    "- What is the intuition behind an eigenvalue and eigenvector?\n",
    "- Some interesting edge cases (what are the eigenvalues and eigenvectors for):\n",
    "    - A rotation-only matrix like $\\left[\\begin{smallmatrix} 0 & -1 \\\\ 1 & 0 \\end{smallmatrix}\\right]$ has only imaginary $\\lambda=i$. No eigenvectors, as each vector is rotated\n",
    "    - A shear matrix like $\\left[\\begin{smallmatrix} 1 & 1 \\\\ 0 & 1 \\end{smallmatrix}\\right]$: only 1 eigenvalue ($\\lambda = 1$), so only 1 eigenvector (all vectors on the $x$-axis are eigenvectors)\n",
    "    - A scaling-only matrix like $\\left[\\begin{smallmatrix} 2 & 0 \\\\ 0 & 2 \\end{smallmatrix}\\right]$: only 1 eigenvalue ($\\lambda = 2$), but **all** vectors in the plane are eigenvectors, and scaled by a factor of $2$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
