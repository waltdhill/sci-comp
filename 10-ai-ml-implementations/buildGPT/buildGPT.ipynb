{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bea7ea7-e63e-4639-af82-6a20297cc0be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download tiny shakespeare training dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64cfbf0f-57f6-4c6c-b635-8d868dd04f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in and inspect the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93dbe375-cc9f-4b3b-b254-8e89a7e801b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7615eedc-f572-4039-970f-5ed557011c16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examine the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a79011-1f2d-4e58-964e-f9f441bf178b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb34ed8a-2b2b-44d9-b641-828663cc1e30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers \n",
    "# aka \"tokenise\" the input text - convert chars (in strings) into integer elements\n",
    "s_to_i = { ch:i for i,ch in enumerate(chars) } # create lookup table from character to integer\n",
    "i_to_s = { i:ch for i,ch in enumerate(chars) } # create lookup table from integer to character\n",
    "\n",
    "# print(s_to_i.keys(), '\\n')\n",
    "# print(s_to_i.values())\n",
    "# print('\\n')\n",
    "# print(i_to_s.keys(), '\\n')\n",
    "# print(i_to_s.values())\n",
    "\n",
    "# An encoder and decoder together are referred to as a \"tokeniser\"\n",
    "encode = lambda s: [s_to_i[c] for c in s] # encoder: take a string, output a list of integers representing that string\n",
    "decode = lambda l: ''.join([i_to_s[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81f505-1b3c-4053-bc2b-34db0223cd1e",
   "metadata": {},
   "source": [
    "## Tokenisation schema\n",
    "\n",
    "There are many other tokenisation schema (usually with much larger vocabularies, the unit is a sub-word, not just individual letters)\n",
    "* Google uses SentencePiece - a sub-word schema\n",
    "* OpenAI uses tiktoken - a fast Byte Pair Encoding (BPE) tokeniser (used by GPT-2) \n",
    "    * [e.g. using tiktoken gives you 3-integer sequence, given it has a much larger vocabulary of size 50,257 tokens]\n",
    "\n",
    "**It's a trade off**: \n",
    "* *larger vocabularies* (i.e. larger code books of tokens) get you *shorter integer sequences*\n",
    "* vice versa: *smaller vocabularies* (i.e. smaller code books of tokens) get you *longer integer sequences*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "782208c4-dd5f-40fb-9ee6-ce9ee7ed9925",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# encode (tokenise) the entire text dataset, and store it as a torch.Tensor\n",
    "import torch # using PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 chars from earlier will, to the GPT, look like this\n",
    "# If torch not installed error, run the following terminal command in active jupyter env: \n",
    "# conda install pytorch::pytorch torchvision torchaudio -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65adedc4-d8c0-4d89-bd92-ee03d9028de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the data into train and validation sets\n",
    "n = int(0.9*len(data)) # the first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:] # to assess how much the model has overfit the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed0ee9-6a2e-42f1-93d9-1aeeadb13323",
   "metadata": {},
   "source": [
    "## Training the transformer architecture on the text dataset\n",
    "\n",
    "* Let's input the integer sequence into the transformer so it can \"learn\" the training data.\n",
    "* This needs to be done in blocks/chunks (sampled randomly from the training set), not the entire integer sequence at once as this is infeasible\n",
    "* Each block/chunk has a maximum length of `block_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c218572-5e4b-4ec2-99d8-c7a5d1a081b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 # max length of a chunk (aka. context_length)\n",
    "train_data[:block_size+1] # let's sample 9 chars from the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089eeab-32e9-4b68-bb93-c7adfd75ee05",
   "metadata": {},
   "source": [
    "### Sample a block from the dataset\n",
    "* The 9 characters (tokens) sampled above actually **simultaneously contain 8 examples** of \"how english works\".\n",
    "* This simultaneously trains the transformer to make a prediction at each of the 8 positions in the block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0ba50bc-b4ab-40b4-a5b0-c17628805a82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the 8 examples hidden in the 9 character chunk sampled from the training set\n",
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] # inputs to the transformer (the first `block_size` chars)\n",
    "y = train_data[1:block_size+1] # the targets to each position in the input transformer (i.e. offset by 1)\n",
    "\n",
    "print('These are the 8 examples hidden in the 9 character chunk sampled from the training set')\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] # context: all chars in x, upto and including t\n",
    "    target = y[t] # target the t'th char in y (aka the t+1'th char in x)\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8acf80c-fcea-427d-a5ea-6a0e396ff131",
   "metadata": {},
   "source": [
    "### Why train on all 8 examples (context size of 1 up to context size of `block_size`?)\n",
    "* So the transformer is used to seeing contexts of all lengths (all the way from 1 to `block_size`), \n",
    "* This way we can start the sampling generation from as little as 1 character (token) of context, up to `block_size` characters, and transformer knows how to predict the next character (token)\n",
    "* More than `block_size` characters (tokens), and we need to truncate, bc the transformer is never expected to see more than `block_size` inputs. \n",
    "    * This is why GPT-3/4 are so good, with `block_size` of 8k-32k tokens, allowing for very large inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a668a05-ad3c-4206-8816-e7bbeb0fecb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have randomly sampled four 9-char (9-token) chunks from the full text to form two new 4x8 tensors (input and target)\n",
      "We now have a batch containing a total of 32 independent examples to train the transformer\n",
      "\n",
      "inputs (for transformer to process):\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "\n",
      "targets (for transformer to look up for\n",
      " each corresp. input sequence):\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n",
      "----\n",
      "\n",
      "Here are the 32 independent examples spelled out, for the transformer to be trained on\n",
      "Another way to read the following is \"in the context of [input integer sequence], [target] comes next\"\n",
      "\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # independent sequences processed in parallel by the transformer on each forward/backward pass (independently, due to GPUs being highly efficient!)\n",
    "block_size = 8 # maximum context length for predicitons\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generate 4 random numbers; positions in the total Shakespeare text to sample 4 blocks from\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # inputs to the transformer (first `block_size` chars)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # targets for each position in inputs (i.e. offset by 1)\n",
    "    return x, y\n",
    "\n",
    "print('We have randomly sampled four 9-char (9-token) chunks from the full text to form two new 4x8 tensors (input and target)')\n",
    "print('We now have a batch containing a total of 32 independent examples to train the transformer')\n",
    "print()\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs (for transformer to process):')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print()\n",
    "print('targets (for transformer to look up for\\n each corresp. input sequence):')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('\\n----\\n')\n",
    "\n",
    "print('Here are the 32 independent examples spelled out, for the transformer to be trained on')\n",
    "print('Another way to read the following is \"in the context of [input integer sequence], [target] comes next\"')\n",
    "print()\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46e3969f-9b87-44a1-a115-158d236e181a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de512e-5208-4eab-a7d5-fd0a252d9fb1",
   "metadata": {},
   "source": [
    "## Bigram language model \n",
    "A simple first baseline neural network for language modelling\n",
    "# TBC: See makemore series for indepth explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "891dc0cf-efc1-489f-a1ce-9a9fe2e6ccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape # because PyTorch F.cross_entropy() docs wants a (B,C,T) matrix instead OR a (B*T, C) 2D array (a little confusing)\n",
    "            logits = logits.view(B*T, C) # stretching out the array into 1D vector (B*T = 32-elem long), and preserving Channel (C) as 2nd dim (reshape logits for PyTorch compatibility)\n",
    "            targets = targets.view(B*T) # same as for logits. match dim., stretching targets to a 32-elem 1D array (note, targets.view(-1) also works, PyTorch guesses to match to logits.)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss) # We have 65 possible vocabulary elements, so (guess) we expect -ln(1/65) = ~4.17, but loss ie actually ~4.87. Means initial predictions are not diffuse (flat). They have some entropy (bumpy) so we're guessing wrong for the next token!\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # 1x1 tensor (B=1, T=1), with 0 (newline character) to start the generation.\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e51e6-0946-4941-b7cf-c8ce44631f83",
   "metadata": {},
   "source": [
    "### Note\n",
    "The output generated is random because the model is untrained so generates garbage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1df654f-ab62-4a13-a9d9-38c2e8a794b1",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8284e300-4f02-4cfc-8d49-d4346962d8e6",
   "metadata": {},
   "source": [
    "### Create the optimiser\n",
    "\n",
    "In makemore series, only stochastic gradient descent (SGD) was used for its simplicity.\n",
    "\n",
    "Here, the `AdamW` optimiser is used. It is a much more advanced optimiser\n",
    "* For usual large networks, a learning rate of `3e-4` or similar order is suggested\n",
    "* Since our network is so small, the learning rate of `1e-3` works! Larger would probably work\n",
    "\n",
    "#### How it works\n",
    "\n",
    "An optimiser object:\n",
    "\n",
    "0. Resets all gradients to 0\n",
    "1. Gets the gradients for all parameters\n",
    "2. and updates the parameters using those gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1084b921-5604-458a-970e-90098a4bd3d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4225 parameters\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "print(sum(p.numel() for p in m.parameters()), 'parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76fbbd81-b574-475a-be4c-78134b18e328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.382369041442871\n",
      "4225 parameters\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # larger!\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data, 2 [B x T] tensors\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb) \n",
    "    optimizer.zero_grad(set_to_none=True) # zero out all gradients from previous step\n",
    "    loss.backward() # get gradients for all parameters\n",
    "    optimizer.step() # user gradients to update parameters\n",
    "\n",
    "print(loss.item())\n",
    "print(sum(p.numel() for p in m.parameters()), 'parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11629faf-c183-478a-ab2c-5fc91029fa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "BEY:! Indy; by s afreanoo adicererupa anse tecorro llaus a!\n",
      "OLeneerithesinthengove fal amas trr\n",
      "TI ar I t, mes, n IUSt my w, fredeeyove\n",
      "THek' merer, dd\n",
      "We ntem lud engitheso; cer ize helorowaginte the?\n",
      "Thak orblyoruldvicee chot, p,\n",
      "Bealivolde Th li\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99d0e4-bd51-435d-b059-d24971bed1ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Massive improvement!\n",
    "\n",
    "Note though, the tokens are not yet talking to each other (i.e. the model is not seeing the full context, only the most recent step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79703d36-66d9-4858-9c35-9b4e9bb0fa30",
   "metadata": {},
   "source": [
    "### [FOLD] Full Code so far in script form for iteration/analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdfd705f-d2f9-4f68-8587-f6463162f5d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004225 M parameters\n",
      "step 0: train loss 4.7303, val loss 4.7214\n",
      "step 300: train loss 3.2538, val loss 3.2504\n",
      "step 600: train loss 2.7325, val loss 2.7583\n",
      "step 900: train loss 2.6179, val loss 2.6369\n",
      "step 1200: train loss 2.5534, val loss 2.5522\n",
      "step 1500: train loss 2.5072, val loss 2.5444\n",
      "step 1800: train loss 2.4819, val loss 2.5093\n",
      "step 2100: train loss 2.4867, val loss 2.5008\n",
      "step 2400: train loss 2.5053, val loss 2.5000\n",
      "step 2700: train loss 2.4817, val loss 2.4973\n",
      "step 2999: train loss 2.4918, val loss 2.5306\n",
      "\n",
      "Fr! kyoredung he: w,\n",
      "STh t-usu, cuche-f hy:\n",
      "\n",
      "\n",
      "HE g cof Cve Op a me\n",
      "\n",
      "HJagar\n",
      "IIAs Th e hralsu!\n",
      "It!\n",
      "CELI spre pr saith ld acewineriker\n",
      "ANERospr'se thiethiker hofur thinglon ithind asce ng:\n",
      "jero h hererrd rdiglerusomm d fethal meanst ppl tthane toug ls th un,\n",
      "AD ner tidoethe ne or d wiminkyowempaveele pofotes dend touranerVOupe\n",
      "S:\n",
      "\n",
      "' thar prendaig, ar wg whiofang dstoum Thee CH:\n",
      "Y:\n",
      "\n",
      "DI fu wO my me he n:\n",
      "BEYet,ULORUCARE g tor, mik's hos.\n",
      "bl linditherandisins my t-l ol andoroul meartof an ay,\n",
      "Witheral igr\n",
      "\n",
      "CELLO:\n",
      "SThe sin'ss HOUpenalaf tryowive.\n",
      "He d hast t.\n",
      "USOLAn hindlalesed wr't tho theck, or beathaf whoul dico.\n",
      "HES:\n",
      "\n",
      "FLO, rd Ponline MOf thEN Fin:USathaite, n:wo o--tsatofthind te.\n",
      "lo inchty tred makin cors as ayol, by f ho grpove s, ctht\n",
      "ourmbtig b?\n",
      "Th y, tos ardas.\n",
      "\n",
      "\n",
      "\n",
      "A:\n",
      "Ald bumoCEENVWhspese ts\n",
      "Th.Vogodchrelyos IIOR:\n",
      "Plthecad th m;\n",
      "MNCldlirthtYouleber s ing ceere Imesil mbh'sh t.\n",
      "AR:\n",
      "AD:\n",
      "\n",
      "Bue s whone INAnd te;\n",
      "Mat, the y l,\n",
      "Benong\n",
      "ANe--yonde t;\n",
      "\n",
      "\n",
      "NVLO tchofarecomond f me akisth\n",
      "OLAsoun teaurselay str\n",
      "A Thord lde sthire\n",
      "pHalene ais ng,\n",
      "Th by nd r mprl t lalalorear RDe winatllorer l\n",
      "Ne\n",
      "Ana dive mbaninem e tat f:\n",
      "ALEdet tha mithoul an? wen bes seou is wchumed th ps nd s, d moind se is thatty tayouacllfllo thor?\n",
      "What qu ledec3xaby ivelerkerothe I y\n",
      "NARICAr y me fr'd nd t OWh pth t y lige s tencoulf ad ded,\n",
      "ARD ea ad hean, tagrst,\n",
      "RIUREno ge at ver;\n",
      "Andwn;\n",
      "\n",
      "CJac,\n",
      "Wan:\n",
      "CO:\n",
      "\n",
      "Leirn igit d.\n",
      "HNGENave sed s-garesammy sand t s-VINom'sp n:\n",
      "Bothswhifosayoomy amy howieare sise burapbe w ls he thout t; is RD ig w an\n",
      "Ay tl ane kee eve,\n",
      "Artcke m as h weney, e.\n",
      "CHowes f ulll a't GLAchowhe lld.\n",
      "\n",
      "\n",
      "Ge pas\n",
      "Hathingo,\n",
      "Dilvewound\n",
      "\n",
      "Whir laroulouritherto be igCHaven'sle,\n",
      "Whent:\n",
      "Wht Bour hikeYOn\n",
      "TomernoBELALI nderid w'dy; tly ady; n,\n",
      "HAnd yous mowere ifour is has y,\n",
      "I:\n",
      "O: tere ewrery, shastaigsout s curne tre sous, be to e tor tthot'Whtit keal tr k,\n",
      "Thaine e d'd tr'sest.\n",
      "Y heeat me tht, illen din'st, wilpr l that!qulifast d:\n",
      "To brithind au ns, t; t:qur's-amy; vel wXIO:\n",
      "CHevewhalloler p IS:\n",
      "BD:\n",
      "AM\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 0. hyperparameters\n",
    "batch_size = 4 # actually 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # run on GPU if available\n",
    "eval_iters = 200\n",
    "# ------------\n",
    "\n",
    "# 1. Reproducibility \n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# 2. Read the data\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 3. Get the tokeniser (encoder and decoder functions)\n",
    "# Here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# 4. Create train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# 5. Use the data loader to get a batch of random inputs and targets (integer seq's from full text dataset)\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # ensure loaded data is moved to device (CUDA GPU)\n",
    "    return x, y\n",
    "\n",
    "# 6. Averages the loss value over multiple batches\n",
    "# @torch.no_grad() is a context manager. \n",
    "#     It tells PyTorch everything in estimate_loss() function doesn't undergo back propogation (i.e. tell PyTorch we don't intend to call .backward() method)\n",
    "#     PyTorch thus doesn't store intermediate variables (gradients??), so can be much more mem efficiency\n",
    "@torch.no_grad() # context manager\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # NB: model is set to eval phase because some layers in complex NNs behave differently in eval (inference) and train modes.\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) # zero init losses vector\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split) # create inputs (X) and targets (Y) (Y is offset from X by 1 token)\n",
    "            logits, loss = model(X, Y) # iteratively get losses for each batch (for train, then for val)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() # average all (eval_iters=200) losses for train, then for val\n",
    "    model.train() # NB: model is set back to train phase\n",
    "    return out\n",
    "\n",
    "# 7. Super simple bigram language model (can forward; output logits and loss, and generate tokens)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape # because PyTorch F.cross_entropy() docs wants a (B,C,T) matrix instead OR a (B*T, C) 2D array (a little confusing)\n",
    "            logits = logits.view(B*T, C) # stretching out the array into 1D vector, and preserving Channel (C) as 2nd dim\n",
    "            targets = targets.view(B*T) # same as for logits. match dim. (note, targets.view(-1) also works, PyTorch guesses to match to logits.\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "# 7.1 Initialising model and moving it to device (CUDA GPU)\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device) # e.g. nn.Embedding() table has dot weights moved to GPU to perform calcs faster\n",
    "\n",
    "# 7.2 print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# 8. Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 9. Create a training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss() # less noisy measure of loss (average over eval_iters # of batches)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # randomly sample a batch (32 independent 8-token (integer) sequences) of data\n",
    "    xb, yb = get_batch('train') # xb (input tensor) and yb (targets) are each [B=32 x T=8] tensors\n",
    "\n",
    "    # evaluate the loss - improving the parameters (training the model)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 10. Generate from the model, ensuring initial context created to feed into generate() is created on the device\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # initial context: newline character (0)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae38bb-d065-48a9-9476-ec27d1eb8458",
   "metadata": {},
   "source": [
    "# (Self-)Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc11a5f-0cf3-445e-83e1-7d79b50db706",
   "metadata": {},
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf05f117-7192-4b16-b356-080d9921aba6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1808, -0.0700],\n",
      "         [-0.3596, -0.9152],\n",
      "         [ 0.6258,  0.0255],\n",
      "         [ 0.9545,  0.0643],\n",
      "         [ 0.3612,  1.1679],\n",
      "         [-1.3499, -0.5102],\n",
      "         [ 0.2360, -0.2398],\n",
      "         [-0.9211,  1.5433]],\n",
      "\n",
      "        [[ 1.3488, -0.1396],\n",
      "         [ 0.2858,  0.9651],\n",
      "         [-2.0371,  0.4931],\n",
      "         [ 1.4870,  0.5910],\n",
      "         [ 0.1260, -1.5627],\n",
      "         [-1.1601, -0.3348],\n",
      "         [ 0.4478, -0.8016],\n",
      "         [ 1.5236,  2.5086]],\n",
      "\n",
      "        [[-0.6631, -0.2513],\n",
      "         [ 1.0101,  0.1215],\n",
      "         [ 0.1584,  1.1340],\n",
      "         [-1.1539, -0.2984],\n",
      "         [-0.5075, -0.9239],\n",
      "         [ 0.5467, -1.4948],\n",
      "         [-1.2057,  0.5718],\n",
      "         [-0.5974, -0.6937]],\n",
      "\n",
      "        [[ 1.6455, -0.8030],\n",
      "         [ 1.3514, -0.2759],\n",
      "         [-1.5108,  2.1048],\n",
      "         [ 2.7630, -1.7465],\n",
      "         [ 1.4516, -1.5103],\n",
      "         [ 0.8212, -0.2115],\n",
      "         [ 0.7789,  1.5333],\n",
      "         [ 1.6097, -0.4032]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example (4 indep seqs, 8 chars each, speaking a language that only contains 2 tokens (e.g. 2 words/ 2 letters)):\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "print(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9683cb60-2a2e-41f1-99b7-47f0100cbcf5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Goal of aggregation\n",
    "\n",
    "We are trying to summarise **each token** (in a given input tensor `x`) **in the context of its history**\n",
    "\n",
    "* Version 1: [**INEFFICIENT**] Mean aggregation (w/ for loops): \n",
    "    * Token `x[t]` is summarised as the average of itself `x[t]` and the tokens that came before it `x[0]`, `x[1]`, ... `x[t-1]`\n",
    "    * BOW: \"bag-of-words\" aka just averaging all the words!\n",
    "* Version 2: [**EFFICIENT**] Mean aggregation (using matrix multiplication to perform a weighted sum): \n",
    "    * Same as V1, but faster\n",
    "* Version 3: Using Softmax:\n",
    "    * \n",
    "* Version 4: Self-attention:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e769a-f62c-4153-96f3-afb440ba1f86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Version 1: (Inefficient) Mean aggregation of historical context from past tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7bd88d5-848f-475c-b2aa-9e3f6c9d7fab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterate over all batch dims independently. For each batch dim, iterate over time.\n",
      "For the current batch, xprev is everything upto AND INCLUDING the t'th token (t+1 cos python 0-indexing) - includes their channel info (logits)\n",
      "\n",
      "\n",
      "x[0]: Each row contains the (C=) 2 logits for the (T=)8 chars [for the (B=0)1st batch]\n",
      " tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "\n",
      "xbow[0]: Same as above, but now each row element is the AVERAGE of itself and all previous rows\n",
      " tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "note the cascading averages in xbow[0] for each time step, wrt x[0]\n"
     ]
    }
   ],
   "source": [
    "##### version 1: We want x[b,t] = mean_{i<=t} x[b,i] \n",
    "# - i.e. aggregate using the average of the vectors of all PAST tokens and the current token\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B): # 4 batches\n",
    "    for t in range(T): # 8 tokens (per batch)\n",
    "        xprev = x[b,:t+1] # for current batch b, xprev shape is (t,C) \n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "        \n",
    "print('Iterate over all batch dims independently. For each batch dim, iterate over time.')\n",
    "print(\"For the current batch, xprev is everything upto AND INCLUDING the t'th token (t+1 cos python 0-indexing) - includes their channel info (logits)\\n\\n\")\n",
    "# print(x)\n",
    "print('x[0]: Each row contains the (C=) 2 logits for the (T=)8 chars [for the (B=0)1st batch]\\n', x[0])\n",
    "print('\\nxbow[0]: Same as above, but now each row element is the AVERAGE of itself and all previous rows\\n', xbow[0])\n",
    "print('note the cascading averages in xbow[0] for each time step, wrt x[0]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4804f-8b73-4222-bfd6-d2a0921c8695",
   "metadata": {},
   "source": [
    "### Version 2: (Efficient) Mean aggregation (using matrix multiplication to perform a weighted sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709054b-cdb4-4987-a22a-f4fb2d2cde11",
   "metadata": {},
   "source": [
    "#### Preamble to version 2 (folded code example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f45a0479-10df-42d0-ac51-31f65573cc93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncomment here for BTS\n",
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# preamble to version 2: toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b # matrix multiplication notation\n",
    "# print('a=')\n",
    "# print(a)\n",
    "# print('--')\n",
    "# print('b=')\n",
    "# print(b)\n",
    "# print('--')\n",
    "# print('c=')\n",
    "# print(c)\n",
    "# print('^c is dot prod of a and b')\n",
    "# print('\\n\\n------ How about a variable sum ------\\n\\n')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "# print('a=')\n",
    "# print(a)\n",
    "# print('--')\n",
    "# print('b=')\n",
    "# print(b)\n",
    "# print('--')\n",
    "# print('c=')\n",
    "# print(c)\n",
    "# print('\\n\\n--------------------\\n\\n')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "print('uncomment here for BTS')\n",
    "# print(a)\n",
    "# print(torch.sum(a, 1, keepdim=True)) # keepdim to keep it vertical (i.e. 3x1, instead of defaulting to 1x3 so broadcasting works out)\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30b4aa71-e8a4-4f47-b1e3-995b6b435ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "xbow[0]:\n",
      " tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]]) \n",
      "\n",
      "xbow2[0]:\n",
      " tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]]) \n",
      "First batches (B=0) of xbow and xbow2. \n",
      "Note: identical! xbow2 so much more efficient!\n"
     ]
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "weights = torch.tril(torch.ones(T, T)) # triangular form confines aggregation to HISTORICAL tokens only\n",
    "weights = weights / weights.sum(1, keepdim=True) # weights for each row's weighted sum (note: rows of wei sum to 1)\n",
    "print(weights)\n",
    "xbow2 = weights @ x # (B', T, T) @ (B, T, C) ----> (B, T, C)\n",
    "    # PyTorch creates a B' dimension for the TxT weights matrix, then does a \"batched matrix multiply\"\n",
    "    # This means for each of the B batches, PyTorch does a (TxT)(TxC) matrix multiplication; \n",
    "    # leaving a [TxC] FOR EACH batch, hence the output is (B,T,C)\n",
    "torch.allclose(xbow, xbow2) # Means both tensors are identical\n",
    "print('xbow[0]:\\n', xbow[0], '\\n\\nxbow2[0]:\\n', xbow2[0], '\\nFirst batches (B=0) of xbow and xbow2. \\nNote: identical! xbow2 so much more efficient!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a64e8-0ec1-4f0f-a7d3-c54d0d665f36",
   "metadata": {},
   "source": [
    "#### Notes on using batched matrix multiply to do a weighted aggregation (a bunch of weighted sums!)\n",
    "\n",
    "* $\\texttt{weights}_{\\text{[T}\\times\\text{T]}}$ and $\\texttt{x}_{\\text{[B}\\times\\text{T}\\times\\text{C]}}$ matrices don't have same dim, so PyTorch implicitly creates a Batch dim for `weights`, to perform a \"batched matrix multiplication\".\n",
    "* Now we're doing a weighted sum for each batch\n",
    "    * I.e. for each of the B batches, we are multiplying the $\\text{[T}\\times\\text{T]}$ `weights` matrix with the $\\text{[T}\\times\\text{C]}$ `x` inputs matrix\")\n",
    "    * This performs the mean aggregation (historical sum)\n",
    "* Try visualise each batch as the \n",
    "    * **Horizontal** planar (2D) slices of the $\\text{[T}\\times\\text{C]}$ [Time x Channel] object, where \n",
    "    * **Vertically** stacking the independent $\\text{[T}\\times\\text{C]}$ slices (aka batches) gives you the full $\\text{[B}\\times\\text{T}\\times\\text{C]}$ construct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1e64a-9377-42cc-9ad9-6faee0750ec1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Verison 3: Use softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d787e36-4e5a-4e7f-b2de-c70e606de2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tril:\n",
      " tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "weights (init):\n",
      " tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "weights (masked fill):\n",
      " tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "weights (applying softmax (a normalisation operation): [exp(each elem), divide by row sum] - uncomment above):\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "\n",
    "# reset x (due to use later in notebook)\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Softmax:\n",
    "tril = torch.tril(torch.ones(T, T)) # lower triangular ones matrix\n",
    "print('tril:\\n', tril)\n",
    "weights = torch.zeros((T,T)) # initialise weights matrix as all zeros (Later these \"affinities\" won't be init'd as zeros. They'll be data dependent)\n",
    "print('\\nweights (init):\\n', weights)\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # apply mask to weights matrix (where tril = 0) i.e. set upper-non-diagonal elements of weights matrix to \"-inf\"\n",
    "print('\\nweights (masked fill):\\n', weights)\n",
    "\n",
    "# Uncomment to see Step 1 of the 2 steps in the Softmax function\n",
    "# weights_exponentiated = torch.exp(weights)\n",
    "# print('\\nweights (each element exponentiated):\\n', weights_exponentiated)\n",
    "# print('Note: this looks like tril, but it\\'s because we will initialise the weights matrix differently later to show affinities') \n",
    "\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "print('\\nweights (applying softmax (a normalisation operation): [exp(each elem), divide by row sum] - uncomment above):\\n', weights) # normalise w/ softmax\n",
    "\n",
    "# Once again, batched matrix multiply with our toy example logits.\n",
    "xbow3 = weights @ x\n",
    "torch.allclose(xbow, xbow3) # again, identical!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ae784-bd38-49a4-937f-ed69cf45592a",
   "metadata": {},
   "source": [
    "#### Notes: Why use the softmax approach in self-attention?\n",
    "\n",
    "* `tril = torch.tril(torch.ones(T,T))`\n",
    "\t* Make a lower triangular matrix. We'll use its **upper-non-diagonal elements** as a mask to apply a `-inf` value to the `weights` matrix.\n",
    "    * This will ensure future tokens can't communicate to the present\n",
    "* `weights = torch.zeros((T,T))`\n",
    "    * Begin initialised with 0s, but this will change later, **different tokens will have different affinities for each other**\n",
    "    * This is like interaction string/affinity. Different tokens will find other tokens more or less interesting\n",
    "* `weights = weights.masked_fill(tril == 0, float('-inf'))`\n",
    "\t* Set **upper-non-diagonal elements** of `weights` matrix to `-inf` by applying a mask to `weights` matrix (where `tril = 0`)\n",
    "\t* Masking to`-inf`means**tokens from the future cannot communicate with the current token**\n",
    "\t\t* This `-inf` setting ensures no communication with future tokens, because `weights` will become **lower triangular**, after we apply `softmax()`\n",
    "* `weights = F.softmax(weights, dim=-1)`\n",
    "\t* Normalise `weights` matrix (using `softmax()`)\n",
    "\t\t* `weights` first becomes **Lower Triangular** as we exponentiate each element.\n",
    "\t\t* Then `weights` gets normalised so each row sums to 1\n",
    "* `xbow3 = weights @ x`\n",
    "\t* Then take **weighted sum** (batched matrix multiplication) to get the weighted aggregations of PAST elements\n",
    "\t\t* These remaining lower triangular elements of `weights` tells us how much of each PAST token \"fuses\" into the current token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56765b16-d48b-4f40-8264-0b71d5397223",
   "metadata": {},
   "source": [
    "## Recess: Let's revisit and improve the Bigram model definition\n",
    "\n",
    "Improvements:\n",
    "\n",
    "* No longer passing in `vocab_size` as an arg, it is already a global variable\n",
    "* Don't create full square $\\text{[C}\\times\\text{C]}$ **embedding table** (lookup table) of logits at once. \n",
    "\t* Therefore, we can no longer obtain logits in `forward()` directly (by looking it up in a single token embedding (lookup) table), \n",
    "\t* Instead, use an intermediate phase to create a **level(s) of indirection** (like a hidden layer in MLP)\n",
    "    * Init new global var `n_embd = 32`: Number of embedding dimensions. This is arbitrary and problem-specific (a little like how many hidden layers in MLP)\n",
    "    * **Initialise 2 new lookup tables**: \n",
    "\t    * `token_embedding_table`: $\\text{[\\texttt{vocab\\_size}}\\times\\text{C (= \\texttt{n\\_embd})]} \\rightarrow [65 \\times 32]$\n",
    "\t    * and `position_embedding_table`: $\\text{[T}\\times\\text{C (= \\texttt{n\\_embd})]} \\rightarrow [8 \\times 32]$\n",
    "* In `forward()` pass:\n",
    "\t* Get the token embeddings, `tok_emb`, of dims $\\text{[B}\\times\\text{T}\\times\\text{C (= }\\texttt{n\\_embd})]$\n",
    "\t\t* Note: Previously, `token_embedding_table` attribute embedded only the identities of the tokens inside `idx`. Now, `position_embedding_table` attribute embeds the positions of the tokens as well\n",
    "\t* Get the positional embeddings, `pos_emb`, of dims $[\\text{T}\\times \\text{C (= \\texttt{n\\_embd})}]$\n",
    "* Finally, we employ a **LINEAR LAYER** (`nn.Linear(n_embd, vocab_size)`) to go from `tok_emb` and `pos_emb` (each with second dim of $\\texttt{n\\_embd}=32$) up to `logits` with dims $\\text{[B}\\times\\text{T}\\times\\text{(C = }\\texttt{vocab\\_size}=65)]$.\n",
    "\t* We just completed the indirection: `idx` (token (65 dims) and position (8 dims)) $\\rightarrow$ `n_embd` (32 dims) $\\rightarrow$ up to `vocab size ` (65 dims)\n",
    "\t* Note the $\\text{C}$'s are different. $\\text{C}$ is actually `n_embd` now, by re-definition.\n",
    "\t* So now, `logits` has dims $\\text{[B}\\times\\text{T}\\times\\texttt{vocab\\_size}]$ (note: no longer called $C$)\n",
    "* <mark>More info is on GoodNotes</mark>\n",
    "\n",
    "### Further inspection of the `tok_emb` and `pos_emb` objects\n",
    "\n",
    "#### Table showing inputs and outputs for creation of these objects:\n",
    "\n",
    "| Object in `.forward()` | Input | Output |\n",
    "| ---- | ---- | ---- |\n",
    "| `tok_emb` | `idx` <br>$\\text{[B}\\times\\text{T]} \\rightarrow [4\\times8]$ | Look-up from a $65\\times32$ `token_embedding_table`<br>$\\text{[B}\\times\\text{T}\\times\\text{C=\\texttt{n\\_embd}]} \\rightarrow [4\\times8\\times32]$ |\n",
    "| `pos_emb` | `torch.arange(T, ...)` <br>$\\text{[T]} \\rightarrow [8]$ (a vector) | Look-up from a $8\\times32$ `position_embedding_table`<br>$\\text{[T}\\times\\text{C=\\texttt{n\\_embd}]} \\rightarrow [8\\times32]$ |\n",
    "\n",
    "### To create `tok_emb` and `pos_emb` why do we lookup from 2 tables with:\n",
    "- **65 rows?**: It's the `token_embedding_table`; 65 possible token values\n",
    "- **8 rows?**: It's the `position_embedding_table`; 8 possible position values\n",
    "- Each possible token / position value has a corresponding **row** in the respective (token / position) Embedding table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b3cbbae-4afb-4741-8c45-bfe4c8e3c58a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Updating the bigram language model (can forward; output logits and loss, and generate tokens)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # tokens no longer directly read off the logits for the next token from a lookup table (recall it was vocab_size x vocab_size)\n",
    "        # now we create a level(s) of indirection (similar to a hidden layer in MLP)\n",
    "        # therefore, we now have 2 lookup tables, one for the token's identity, another for its position\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # a linear layer to go from tok_emb -> (created via forward() call) -> to logits\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "                # since now using pos_emb, can never have > block_size being passed in \n",
    "                # otherwise pos_emb table will run out of scope, since it only has embeddings UP TO block_size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec8828ca-4e4b-4b09-9df2-b9e1a4b74ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 8\n",
      "tensor([[52, 43, 47, 45, 46, 40, 53, 59],\n",
      "        [39, 58,  1, 58, 46, 43,  1, 58],\n",
      "        [61,  1, 47, 52,  1, 51, 63,  1],\n",
      "        [ 1, 43, 51, 40, 56, 39, 41, 43]]) \n",
      " Embedding(8, 32)\n",
      "torch.Size([8])\n",
      "torch.Size([8, 32])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train') # xb (input tensor) and yb (targets) are each [B=4 x T=8] tensors\n",
    "B,T = xb.shape\n",
    "print(B,T)\n",
    "\n",
    "xb.position_embedding_table = nn.Embedding(T,32)\n",
    "print(xb, '\\n', xb.position_embedding_table)\n",
    "pos_emb = xb.position_embedding_table(torch.arange(T, device=device))\n",
    "print(torch.arange(T, device=device).shape)\n",
    "print(pos_emb.shape)\n",
    "\n",
    "# (torch.arange(T, device=device))\n",
    "# print(xb_pos_emb.weight)\n",
    "\n",
    "# Print detailed information about the Embedding object\n",
    "# for name, param in xb_pos_emb_table.named_parameters():\n",
    "#     print('name: ', name)\n",
    "#     print('type: ', type(param))\n",
    "#     print('param.shape: ', param.shape)\n",
    "#     print('param.requires_grad: ', param.requires_grad)\n",
    "#     print('=====')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0689dc79-397a-48aa-b326-aed20276ced2",
   "metadata": {},
   "source": [
    "### Version 4: *(Single-head)* Self-attention (THE CRUX OF GPTs)\n",
    "\n",
    "#### Recap:\n",
    "\n",
    "* So far, we're doing a <mark>simple mean of the historical tokens and the current token</mark> to describe the current token\n",
    "* We create a lower triangular mask of `weights`, then normalise to only get the historical context (i.e. tokens from the future cannot communicate with the current token)\n",
    "* **Disadvantage**: **Previous** and **current** info is being **mixed together in an average**\n",
    "\n",
    "#### Insight\n",
    "\n",
    "Now note, because we initialised as `weights = torch.zeros((T,T))`, <mark>every token (aka node!) has the same affinity for every other token</mark>. \n",
    "* In other words, since we <mark>**initialised**</mark> token affinities uniformly,\n",
    "* (Hence,) the final `weights` matrix has <mark>**uniform structure**</mark> (all elements on a row are equal)\n",
    "\n",
    "#### <mark>**BUT WE DON'T NECESSARILY WANT UNIFORM TOKEN AFFINITY!**</mark>\n",
    "\n",
    "Different tokens find different *other* tokens more or less interesting (e.g. a vowel may be preferentially interested in the consonants in its past, more so than the vowels)\n",
    "\n",
    "#### The answer:\n",
    "Do self-attention (this solves the above issue in a data-dependent way (i.e. learns structures in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f9214ed-20bc-4586-b3e1-2f6f4213b4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output object (A matrix) shapes of the Linear() modules above:  \n",
      " torch.Size([16, 32]) \n",
      " torch.Size([16, 32]) \n",
      " torch.Size([16, 32])\n",
      "\n",
      "This implies the A matrix has shape [ 16 x 32 ], where A_rows = 16 are output_dimensions, and A_cols = 32 are input_dimensions\n",
      "Thus, when we \"forward/propogate\" the Linear() module over x, dimensions [ 4 x 8 x 32 ], this is effectively doing y = x @ A^T (+ b)\n",
      "Hence, note the inner dimensions cancelling: y = xA^T = [ 4 x 8 x 32 ] @ [ 32 x 16 ] ==> [ 4 x 8 x 16 ]; aka [B,T,A_rows]\n",
      "\n",
      " torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# version 4: self-attention! - toy example of 4x8 tokens tensor, and information @ each token is 32 channel\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16 # these are 16 information channels, different to just \"vocab_size\".\n",
    "\n",
    "# initialising Linear modules - these apply a matrix multiply (y = x @ A^T + b)\n",
    "# some notes: \n",
    "#     - Linear module's args (from docs): Linear(in_features [C=32], out_features [head_size=16])\n",
    "#     - those args mean when we provide an input (x, below) with input features (32 channels),\n",
    "#     - this module will apply a lin. transfm. (y = x @ A^T + b) and produce output features (16). THINK LIKE AN MLP!!!\n",
    "#     - bias=False means weights are fixed\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "print('The output object (A matrix) shapes of the Linear() modules above: ', '\\n', key.weight.shape, '\\n', query.weight.shape, '\\n', value.weight.shape)\n",
    "\n",
    "A_r, A_c = key.weight.shape\n",
    "print('\\nThis implies the A matrix has shape [',A_r,'x',A_c,'], where A_rows =',A_r,'are output_dimensions, and A_cols =',A_c,'are input_dimensions')\n",
    "print('Thus, when we \"forward/propogate\" the Linear() module over x, dimensions [',B,'x',T,'x',C,'], this is effectively doing y = x @ A^T (+ b)')\n",
    "print('Hence, note the inner dimensions cancelling: y = xA^T = [',B,'x',T,'x',C,'] @ [',A_c,'x',A_r,'] ==> [',B,'x',T,'x',A_r,']; aka [B,T,A_rows]')\n",
    "\n",
    "# forwarding/propagating the Linear modules defined above on x\n",
    "# NB: Linear() docs: the input to Linear() (in the case below, x) is of shape [batch_size (B,T) x in_features (C)]\n",
    "# Therefore, Linear() applies a linear transformation and produces output of shape [batch_size (B,T) x out_features(16)]\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "# NB: No communication between tokens has happened as yet. \n",
    "# All tokens, in all BxT positions, have each independently produced a key and a query\n",
    "\n",
    "# now for communication between all these tokens. all the QUERIES will dot product will all the KEYS\n",
    "# this is a smarter way to initialise weights (instead of zeroes!)\n",
    "weights =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T) \n",
    "# (i.e. for each of the B batches, we'll end up with a TxT matrix of INITIAL token affinities)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # mask so \"future\" tokens cannot communicate with the present\n",
    "#weights = torch.zeros((T,T)) # weights are no longer initialised as zeros\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1) # i.e. -- exponentiate and normalise (0 to 1, so each row of weights sums to 1)\n",
    "\n",
    "v = value(x) # propogating the \"value\" linear over x\n",
    "out = weights @ v # aggregate over this value vector (instead of this raw x)\n",
    "#out = weights @ x # we're not aggregating raw tokens themselves (x is PRIVATE to the token); but rather their values (v is like PUBLIC info the token reveals)!\n",
    "\n",
    "print('\\n', out.shape) # thus, output of single head is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec905d07-9404-4fac-ac0e-1ba87a3e33b6",
   "metadata": {},
   "source": [
    "#### What's happening under the hood in self-attention?\n",
    "\n",
    "We're simply initialising `weights` a little more cleverly.\n",
    "\n",
    "* Each node/token at each position <mark>**in parallel, and independently emits 2 vectors**</mark>: `query` and `key`\n",
    "    * `query`: \"What am I looking for?\"\n",
    "    * `key`: \"What do I contain?\"\n",
    "    * Note: no communication has happened between tokens yet\n",
    "* In self-attention, to get **affinities between all tokens** in a sequence is to do a **dot product between queries and keys**.\n",
    "    * More specifically, the `query` of token `x[t]`, dot producted with all the `key`s of all the other tokens `x[0]`, `x[1]`, ... `x[t-1]`\n",
    "    * `weights` is initialised as that dot product!\n",
    "    * **Implication** the more highly aligned a given token's (e.g. `x[t]`'s) `query` is with another token's `key`, \n",
    "        * the higher their dot product (affinity), \n",
    "        * and thus the more `x[t]` will learn about that specific token (over other tokens in the sequence)\n",
    "    * For each of the $\\text{B}$ batches, we'll end up with a $\\text{[T}\\times\\text{T]}$ matrix of initial token affinities)\n",
    "* In other words, **self-attention is where communication occurs between tokens**\n",
    "\n",
    "---\n",
    "\n",
    "#### How is `value(x)` any different to raw `x`? What exactly is the difference?\n",
    "* `x` is information that is private to the token in position [B,T] \"I am 5th token with some identity, and my information is in vector `x`\"\n",
    "* \"For a single head, here's what I'm interested in (query:`q`), here's what I have (key:`k`), and if you find me interesting (your `query` dot producted with my `key` produces a high value), here's what I will share with you (value `v`). \n",
    "    * `v` is the thing which gets aggregated for a single head between the different tokens/nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b5f8f48-5bfe-4f07-b9c3-d6311d1a67db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0] # weights for batch 1, affinities between the tokens in this batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c1e56-31d9-4def-921f-b41166ffe12e",
   "metadata": {},
   "source": [
    "#### Outcome:`weights` is now data-dependent\n",
    "\n",
    "* Now we see, while each row still sums to 1, the `weights` matrix is now not constant. \n",
    "* Each batch ~element~ contains different tokens at different positions. So now **`weights` is data-dependent**\n",
    "    * Example: 8th token now knows its **content** and **position**.\n",
    "        * It can broadcast a `key`: \"I'm a vowel on the 8th position\"\n",
    "        * and a `query`: \"I'm looking for a consonant in anywhere up to the 4th position\n",
    "    * All tokens emit `key`s. \n",
    "        * One of the (`head_size=16`) channels might be \"I am a consonant in the somewhere up to the 4th position\"\n",
    "        * Upon `q @ k` dot prod, a token highly aligned tokens have a **high number** in that specific channel\n",
    "            * <mark>e.g. 0.2297 in row 8, col 4: the `key` of token 4 is highly aligned with the `query` of token 8 - so **token 8 has a high affinity for token 4**</mark>, as evidenced by the large dot product\n",
    "    * <mark>Thus, through softmax, the 8th token aggregates a LOT OF INFO about the 4th token into its position (8)</mark>; i.e. token 8 learns a lot about token 4 (relative to the other tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1639265-55f3-43bd-9062-cf77bbb734a6",
   "metadata": {},
   "source": [
    "### Notes on Attention:\n",
    "\n",
    "* Attention is a communication mechanism. \n",
    "    * Can be seen as nodes in a directed graph, where each node has some vector of information\n",
    "    * Nodes are looking at each other (edges pointing at each other) \n",
    "    * Each node aggregating information with a weighted sum from all nodes that point to it, with data-dependent weights.\n",
    "* There is no notion of space. Attention (by default) simply acts over a set of vectors. This is why we need to positionally encode tokens. The `pos_emb` object is what gives self-attention the notion of space\n",
    "    * *I.e. Nodes don't know \"where they are positioned in space\". Attention is just a set of vectors randomly in space which communicate*\n",
    "* Each of the $\\text{B}$ batches are processed completely independently and never \"talk\" to each other\n",
    "    * A **batched matrix multiply** is what applies a matrix multiplication **in parallel across the batch dimension**\n",
    "        * This is why (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "        * It's essentially doing $\\text{B}$ 2-D matrix multiplications in parallel. This is a CPU efficiency measure.\n",
    "* In an **\"encoder\" block of (self-)attention** just delete the single line that does masking with `tril`, allowing all tokens to communicate. \n",
    "    * That's the general case: e.g. for sentiment analysis w/ transformer, all tokens can freely speak w/ each other to predict the sentiment of the whole sentence\n",
    "    * This block here is called a **\"decoder\" (self-)attention block** because it has triangular masking, and is usually used in autoregressive settings, like language modelling.\n",
    "    * **Attention supports arbitrary connectivity between nodes!** Attention doesn't care if you use encoder or decoder blocks.\n",
    "* \"Self-attention\" just means that the **keys** `k`'s and **values** `v`'s are produced from the same source as **queries** `q`'s. \n",
    "    * This source is the initial input vector `x`, which contains $\\text{T}$ tokens, each emitting a `k`, `q`, and `v`.\n",
    "    * In \"cross-attention\", the queries still get produced from `x`, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "        * I.e. a separate set of nodes from which will pull information into our nodes\n",
    "* \"Scaled\" attention additional divides `weights` by $\\frac{1}{\\sqrt{\\texttt{head\\_size}}}$. An important normalisation!\n",
    "    * Softmax function can be sensitive to very large input values. These kill the gradient, and slow down learning, or cause it to stop altogether\n",
    "    * This makes it so when input `q`, `k` are unit variance (Gaussian: $\\mu=0$, and $\\sigma=1$), `weights` will be unit variance ($\\sigma=1$) too \n",
    "    * and so when `weights` feeds into Softmax step (see above), it will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a1be272-df4c-4c2d-87b9-b69e2480cdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var(k) =  tensor(1.0449) \n",
      "var(q) =  tensor(1.0700)\n",
      "\n",
      "Without scaled attention, variance of `weights` is on the order of `head_size`: \n",
      "var(weights) =  tensor(17.4690)\n",
      "\n",
      "With scaled attention, variance of weights is on the order of 1: \n",
      "var(weights) =  tensor(0.9957)\n"
     ]
    }
   ],
   "source": [
    "head_size = 16\n",
    "\n",
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "weights = q @ k.transpose(-2, -1)\n",
    "print('var(k) = ', k.var(), '\\nvar(q) = ', q.var())\n",
    "\n",
    "print('\\nWithout scaled attention, variance of `weights` is on the order of `head_size`:', '\\nvar(weights) = ', weights.var())\n",
    "\n",
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "weights = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "print('\\nWith scaled attention, variance of weights is on the order of 1:', '\\nvar(weights) = ', weights.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823826f-8ef7-45e0-9325-4053673c559d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### \"Scaled\" attention (cont.)\n",
    "\n",
    "If `weights` were not fairly diffuse (i.e. if `weights` took on very positive and/or negative numbers before `weights = F.softmax(weights, dim=-1)` step), `softmax` would converge towards one-hot vectors (gets too peaky)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2736ef56-de2b-4a8d-9ff8-b125d1eb85ca",
   "metadata": {},
   "source": [
    "#### Scaled Attention: Diffuse `weights` initialisation: \n",
    "\n",
    "Here, \"weights\" is fairly diffuse to begin with (all values are near 0, small +ve or -ve), so `softmax` produces diffuse outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebdac27c-8b1b-48dd-a2c1-e25717070f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8827373-17a0-48bb-a900-eb6a2db4ff84",
   "metadata": {},
   "source": [
    "#### Scaled Attention: Sharper `weights` initialisation (i.e. larger numbers)\n",
    "\n",
    "Here, the `softmax` outputs sharpen too, towards the maximum (extreme) number(s) in the initialised `weights` vector. \n",
    "\n",
    "* I.e., `softmax` is super peaky, and **each token will aggregate information from a single other node!** (see below)\n",
    "    * Note how after `softmax`ing to get the new `weights`, all values are squished to near-zero; except one element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81207f4f-2661-473a-ae99-482a0a89b9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec816f-3d0d-4aca-8a9c-1ba5e8532315",
   "metadata": {},
   "source": [
    "### Scaled Attention - TL;DR:\n",
    "\n",
    "* Scaling `weights` by $\\frac{1}{\\sqrt{\\texttt{head\\_size}}}$ simply to control the variance at initialisation \n",
    "    * This allows $\\sigma\\approx1$; \n",
    "    * And prevents `softmax()` from producing too peaky, one-hot-like vectors\n",
    "        * I.e. It prevents a given token/node from aggregating information only from single other tokens/nodes! Rather allowing info from multiple nodes to flow to the querying node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265fb5a-ed4c-4bba-8fb1-a5f68dc640e9",
   "metadata": {},
   "source": [
    "## Let's Create Single- and Multi-Head Self-Attention Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3749b0cd-c3c2-4109-9f11-740542307c0b",
   "metadata": {},
   "source": [
    "### 1. Class which implements: A Single-Head of Self-Attention\n",
    "\n",
    "* `__init__()` \n",
    "    * Given input `head_size`\n",
    "    * Constructor creates:\n",
    "        * `Linear` layers (aka. projections, or lin. transform $\\textbf{A}$ matrices) for `key`, `query` and `value` (to be applied in parallel & indep'ly to all nodes)\n",
    "        * `tril` variable creating the lower triangular matrix (not a param, but a buffer by PyTorch naming conventions, assigned to module using `register_buffer`)\n",
    "* `forward()` function:\n",
    "    * Given input `x`\n",
    "    * Function creates\n",
    "        * Calculates `k` and `q` (by forwarding the `key` and `query` Linear modules over `x`)\n",
    "        * Calculates attention scores (affinities) in `weights`, \n",
    "            * normalised using scaled attention, to control variance\n",
    "            * make it into a **decoder block** future can't commnicate with past\n",
    "            * softmax normalisation\n",
    "        * Produces `v`, and returns the aggregated value dot product of `weights` and `v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d0bf48c-e9aa-427c-994f-93d0f607e7c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C) key vectors, indep for each token in each batch\n",
    "        q = self.query(x) # (B,T,C) query vectors, indep for each token in each batch\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        weights = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T) -- note, it's scaled attention\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) -- decoder block: future can't communicate with past\n",
    "        weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b0111-2045-4824-a47c-51597ace3065",
   "metadata": {},
   "source": [
    "#### 1.1. Update Bigram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccf20b90-6808-4e78-8f33-3e5f078e1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Updating the bigram language model (can forward; output logits and loss, and generate tokens)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # e.g. 65 x 32 lookup table\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # e.g. T x 32 lookup table\n",
    "        self.sa_head = Head(n_embd) # Ceate self-attention head in constructor\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # a linear layer to go from tok_emb to logits\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x) # apply one head of self-attention. (B,T,C)\n",
    "        logits = self.lm_head(x) # decoder language modelling head (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "                # since we're now using a position_embedding_table, can never have MORE than `block_size` being passed in.\n",
    "                # if `idx`'s T-dimension is LONGER than block_size, then the self.position_embedding_table will run out of scope,\n",
    "                # since it only has embeddings UP TO block_size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2563acb-d871-4794-8cef-91e42a32b299",
   "metadata": {},
   "source": [
    "#### 1.2. [FOLD] Full Code so far in script form for iteration/analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "683fe27c-e761-498e-96e3-320e0ae07fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007553 M parameters\n",
      "step 0: train loss 4.2000, val loss 4.2047\n",
      "step 500: train loss 2.6911, val loss 2.7087\n",
      "step 1000: train loss 2.5196, val loss 2.5303\n",
      "step 1500: train loss 2.4775, val loss 2.4829\n",
      "step 2000: train loss 2.4408, val loss 2.4523\n",
      "step 2500: train loss 2.4272, val loss 2.4435\n",
      "step 3000: train loss 2.4130, val loss 2.4327\n",
      "step 3500: train loss 2.3956, val loss 2.4212\n",
      "step 4000: train loss 2.4041, val loss 2.3992\n",
      "step 4500: train loss 2.3980, val loss 2.4084\n",
      "step 4999: train loss 2.3951, val loss 2.4126\n",
      "\n",
      "Ano' ou sene od wistwin, wildalle adery wheerel cro ove.\n",
      "\n",
      "K:\n",
      "OMInciove dan uts wat fo stu bur,\n",
      "Wer\n",
      "my seng\n",
      "Becthikure\n",
      "Pe.\n",
      "INCECLIUS: MId miombellavo focen soun ers.\n",
      "\n",
      "A:\n",
      "Way berup he past arr:\n",
      "A sg wou poat;\n",
      "Wino-f-\n",
      "I: herove, wipeofre\n",
      "Yor.\n",
      "Bur ugigiatns Ligo wisen't here, thee my teath, weird, ghe sss the D:\n",
      "NCGas In.\n",
      "\n",
      "WAnd, hei iett or ganger ng\n",
      "Pow, yout Can werd? I, we.\n",
      "\n",
      "Thth asthe mtolecraby st\n",
      "Cow thy had ally whin ango:\n",
      "A, ered whutl by at aprest douried ste sthe!:\n",
      "ADD' I fo p Eulot iso inod quuratoerof se.\n",
      "\n",
      "INUmy Cme.\n",
      "\n",
      "D Imee sdof as, at wom; tthitout nd me shes it.\n",
      "\n",
      "Papoamareng llos berers dorue may gaech phrish fo su\n",
      "ld.\n",
      "\n",
      "Ous.\n",
      "\n",
      "INMOFiach Ail hy oufureng cog; he yughpat t, rit iet fald So ed semanbu hea Py I wo st cett?\n",
      "\n",
      "G aler ll imot.\n",
      "\n",
      "Whll teine rnd lanto uf he shand, owaved heathe;\n",
      "A!\n",
      "D Yofoe so ew ad wet ars\n",
      "H:\n",
      "Asthre, ucean ar.\n",
      "\n",
      "Cod MThis\n",
      "LI agherde.\n",
      "\n",
      "EG: lesiseavimy whr olrd\n",
      "K\n",
      "IS:\n",
      "S: pire fe ousee:\n",
      "BERPY: be ses dthel youse MI burt sed to sofo heapey aris alllere wer nde ive irele dere thit yom; s boeist thit; ils ay vel,\n",
      "Cof amile facang so ker whe cises, orove the isor, chathey hyuseve ys; whane shy m hirey ore no?\n",
      "\n",
      "SThe imy,\n",
      "winy omy aisit;\n",
      "'shalie.\n",
      "COMy ten al'in,\n",
      "O: be hat wit sstoht ay yo t,:\n",
      "-Whtt f bor Court gill womeava ice ha'lld:\n",
      "I simu d oy ben hour tr ore css f-het ipobou so Slalpest, st wise woer ru isos mpee te;\n",
      "and\n",
      "Thy Jor alldadsre; tur ow-whancof oks,\n",
      "S lad nd tho If ml ndo noeng oolle ou htellarrd heeckeh or mus\n",
      "Weldisot.\n",
      "MO:\n",
      "A hom?\n",
      "\n",
      "Weth har sen fis ma odo sisesos, det mve patl mume boulighavenyo ansot by pyevere'seal'do mbuchidobered toker st torst wat toull wead higon priil hpeinl I movitmey ke, ci, ben. Buks lien, us Bent let rbe f shek, ben yere il testicancome CG, oald tharlens ancor alits, falve, tan youl waret pid ore ante'di she lerd he hul, ros\n",
      "Wimovere for sece htate hy ad where, steds hour tongend\n",
      "S es ovatthy isus?\n",
      "Wh the\n",
      "Biet thalile ngrie?\n",
      "As! st, cholll alr th yon allge yifrelver nod I loare: theve, hor egee ise hepe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 0. hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # run on GPU if available\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "# 1. Reproducibility \n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# 2. Read the data\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 3. Get the tokeniser (encoder and decoder functions)\n",
    "# Here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# 4. Create train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# 5. Use the data loader to get a batch of random inputs and targets (integer seq's from full text dataset)\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # ensure loaded data is moved to device (CUDA GPU)\n",
    "    return x, y\n",
    "\n",
    "# 6. Averages the loss value over multiple batches\n",
    "# @torch.no_grad() is a context manager. \n",
    "#     It tells PyTorch everything in estimate_loss() function doesn't undergo back propogation (i.e. tell PyTorch we don't intend to call .backward() method)\n",
    "#     PyTorch thus doesn't store intermediate values, so can be much more mem efficiency\n",
    "@torch.no_grad() # context manager\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # NB: model is set to eval phase because complex neural networks behave differently in eval (inference) and train modes.\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) # zero init losses vector\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y) # iteratively get losses for each batch (for train, then for val)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() # average all (200) losses for train, then for val\n",
    "    model.train() # NB: model is set back to train phase\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        weights = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = weights @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "# 7. Updating the bigram language model (can forward; output logits and loss, and generate tokens)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd) # Ceate self-attention head in constructor\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # a linear layer to go from tok_emb to logits\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x) # appy one head of self-attention. (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "                # since we're now using a position_embedding_table, can never have MORE than `block_size` being passed in.\n",
    "                # if `idx`'s T-dimension is LONGER than block_size, then the self.position_embedding_table will run out of scope,\n",
    "                # since it only has embeddings UP TO block_size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "# 7.1 Initialising model and moving it to device (CUDA GPU)\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device) # e.g. nn.Embedding() table has dot weights moved to GPU to perform calcs faster\n",
    "\n",
    "# 7.2 print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# 8. Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 9. Create a training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss() # less noisy measure of loss (average over eval_iters # of batches)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 10. Generate from the model, ensuring initial context created to feed into generate() is created on the device\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) \n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a0282-8b17-4b10-8311-a43ebdf8d1c6",
   "metadata": {},
   "source": [
    "### 2. Class which implements: Multiple Heads of Self-Attention\n",
    "\n",
    "In other words, apply multiple \"Single Heads of Self-Attention\" in parallel, and concatenating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f65c39e0-1d09-41eb-adbb-8f8e46ea2313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        \"\"\" ff\n",
    "\n",
    "        Arguments:\n",
    "        - num_heads: Number of Self-Attention Heads to implement in parallel\n",
    "        - head_size: Size of each SA Head\n",
    "\n",
    "        Returns:\n",
    "        - \n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" concat outputs of self-attention heads list (run in parallel) over the channel dim\n",
    "\n",
    "        Arguments:\n",
    "        - x\n",
    "\n",
    "        Returns\n",
    "\n",
    "        \"\"\"\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cab80676-a3ec-4377-b616-c8d149000e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f28ab9c1-a06d-47b5-92f1-6974fae3380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7637c64-7775-4110-b8d1-7e45e6d96334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e2e62-b291-4c15-a873-7a83fa14db8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f516de-2252-449d-bb1f-abb8da8d5688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2489657b-499d-4c68-bb12-0238a4e43cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b51e0a7-7091-431c-afd8-48dfdebbdb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "932df938-90f0-4d08-8b1d-42fa913e00de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.5763e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "681b2d76-1bf4-4ae1-b9c5-0fc69331dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les rseaux de neurones sont gniaux! <START> neural networks are awesome!<END>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad54d8-5cbd-45f6-a9fe-14767eaa3384",
   "metadata": {},
   "source": [
    "#### 1.1. Update Bigram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68dcb458-507b-4f25-9314-e02de1ddaa90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Updating the bigram language model (can forward; output logits and loss, and generate tokens)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # a linear layer to go from tok_emb to logits\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e01ecab-564f-4362-b451-3f220a5544fb",
   "metadata": {},
   "source": [
    "# Full finished code (folded below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6da436f2-7926-4503-8ed9-1cd800aaafc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 99: train loss 2.6640, val loss 2.6725\n",
      "\n",
      "ARLvoh Ko ICIis avelll ld w, taon wilay s houne tho;\n",
      "\n",
      "De seonenthehur gy s he brhe l nd thcgile ure gtou thyclefd hal, mab s tho QNn.\n",
      " Cqed ssken, Iorwhz whe powe ocos bis mroed e isehed ve reticev icernshof, ges ind ineEdegh I s woprcfil outhy hI hore f o fecen'en.\n",
      ":\n",
      "ThecofoAt, pu;rad it, for ame d p homiethiindVot ly d my a:y\n",
      "Soe am\n",
      "NYonere bo chNIy h.\n",
      "I ALTurd hlad ad bour?I he brir,ousorofante ind ke tuls wagarf omlint hacofe pcho uteutiith; aty s, ee 3xsormhes thabeef thwe ou n tither 'rxret B tfole! erertto on nc$nfoasde w, t, mee y hasestdesten ciefiwAD b:v;\n",
      "3o hecisaraNnor,. y\n",
      "O owshe\n",
      "CauVhthe konend bitheatvo y T bofoot ay he ang b;\n",
      "ino pewl othoveeNyNoin: Ie finpro fins beoisal shod f tad.\n",
      "Xerere, fore,t is ofilad, tathevoemod int kit olle ieal orce wd thir bI by tofere.\n",
      "Mch X, irt el t Lodondurim:\n",
      "I wt bl.\n",
      "Thasde; kCwhoo anthealEklfon'mesuste l t wethcalatier ton os hestr rnce d avuntWbe-the s buoacthe whohenNre alourpauneeas ne h bin. INzotHis no brtptw e taaselitice akl.I stoth wistht l ynser neve pranrpngp cey bourrecu bLC.\n",
      "hen.\n",
      "Th twUher w man; fin!\n",
      "AittA!woiNand f?y\n",
      "'w hrechir Cv therarere wa tillincezochand.\n",
      "As\n",
      "Thoco?Ae hitafo t, w,\n",
      "Bd pnarot ony inVrs ze wour bh , wSgrenorsuswoced:\n",
      "\n",
      "The ounowinos\n",
      "SC IN:\n",
      "\n",
      "SyJout.\n",
      "US:\n",
      "\n",
      "DUorabhiladn tohser. kSGnaan t\n",
      "\n",
      "xbhe rathednorit pe Rh fwiRedithithes coy Kuur  whoroYechoriasththene t .\n",
      "\n",
      "Akaw helind by t;hisofanuss t voseknony\n",
      "\n",
      "I oursnd beeUFeinforin d my, ted t st t f he fJee'tifsic.\n",
      "Tlilll bOeNUBsuNks heriny Ao t w be bais woAnd anePke, yCese,\n",
      "OEToshandul et A\n",
      "I n.\n",
      "My o afoupevans, f ato GARee; m:\n",
      "RilthxM wy cerelloE, cowhe hein .\n",
      "GIow:\n",
      "S ks ks avini'lis mJ:\n",
      "; mUIedd'dfrano hithe, theNfg,', ceny heunon th t.\n",
      "\n",
      "C, uslthe ishe peloYOFounditAJt be.\n",
      "Sy vy ind he ol Gemee Iaqove !ano bdnonhe sdenefetu fenor fo h\n",
      "kr raliche revet bterameseWf mhe.ous wonoro bit, rlyaln lmemuromepe hintig i p d seec, cemeth mcthoangochoso nyto s wiguty Kt'nstold a ondetan omereuke.\n",
      "Iozltiq:\n",
      "Thairt ou bho be c?\n",
      "\n",
      "Pod to ifrr t.\n",
      "ARHo E:\n",
      "Bug\n"
     ]
    }
   ],
   "source": [
    "## import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 0. hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 100 # make 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # run on GPU if available\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "# 1. Reproducibility \n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# 2. Read the data\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 3. Get the tokeniser (encoder and decoder functions)\n",
    "# Here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# 4. Create train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# 5. Use the data loader to get a batch of random inputs and targets (integer seq's from full text dataset)\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # ensure loaded data is moved to device (CUDA GPU)\n",
    "    return x, y\n",
    "\n",
    "# 6. Averages the loss value over multiple batches\n",
    "# @torch.no_grad() is a context manager. \n",
    "#     It tells PyTorch everything in estimate_loss() function doesn't undergo back propogation (i.e. tell PyTorch we don't intend to call .backward() method)\n",
    "#     PyTorch thus doesn't store intermediate values, so can be much more mem efficiency\n",
    "@torch.no_grad() # context manager, \n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # NB: model is set to eval phase because complex neural networks behave differently in eval (inference) and train modes.\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) # zero init losses vector\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y) # iteratively get losses for each batch (for train, then for val)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() # average all (200) losses for train, then for val\n",
    "    model.train() # NB: model is set back to train phase\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# 7. Super simple bigram language model (can forward; output logits and loss, and generate tokens)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "# 7.1 Initialising model and moving it to device (CUDA GPU)\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device) # e.g. nn.Embedding() table has dot weights moved to GPU to perform calcs faster\n",
    "\n",
    "# 7.2 print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# 8. Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 9. Create a training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss() # less noisy measure of loss (average over eval_iters # of batches)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 10. Generate from the model, ensuring initial context created to feed into generate() is created on the device\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) \n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
