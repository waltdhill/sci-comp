{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bea7ea7-e63e-4639-af82-6a20297cc0be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download tiny shakespeare training dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64cfbf0f-57f6-4c6c-b635-8d868dd04f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in and inspect the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93dbe375-cc9f-4b3b-b254-8e89a7e801b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7615eedc-f572-4039-970f-5ed557011c16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examine the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a79011-1f2d-4e58-964e-f9f441bf178b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb34ed8a-2b2b-44d9-b641-828663cc1e30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers \n",
    "# aka \"tokenise\" the input text - convert chars (in strings) into integer elements\n",
    "s_to_i = { ch:i for i,ch in enumerate(chars) } # create lookup table from character to integer\n",
    "i_to_s = { i:ch for i,ch in enumerate(chars) } # create lookup table from integer to character\n",
    "\n",
    "# print(s_to_i.keys(), '\\n')\n",
    "# print(s_to_i.values())\n",
    "# print('\\n')\n",
    "# print(i_to_s.keys(), '\\n')\n",
    "# print(i_to_s.values())\n",
    "\n",
    "# An encoder and decoder together are referred to as a \"tokeniser\"\n",
    "encode = lambda s: [s_to_i[c] for c in s] # encoder: take a string, output a list of integers representing that string\n",
    "decode = lambda l: ''.join([i_to_s[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81f505-1b3c-4053-bc2b-34db0223cd1e",
   "metadata": {},
   "source": [
    "## Tokenisation schema\n",
    "\n",
    "There are many other tokenisation schema (usually with much larger vocabularies, the unit is a sub-word, not just individual letters)\n",
    "* Google uses SentencePiece - a sub-word schema\n",
    "* OpenAI uses tiktoken - a fast Byte Pair Encoding (BPE) tokeniser (used by GPT-2) \n",
    "    * [e.g. using tiktoken gives you 3-integer sequence, given it has a much larger vocabulary of size 50,257 tokens]\n",
    "\n",
    "**It's a trade off**: \n",
    "* *larger vocabularies* (i.e. larger code books of tokens) get you *shorter integer sequences*\n",
    "* vice versa: *smaller vocabularies* (i.e. smaller code books of tokens) get you *longer integer sequences*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "782208c4-dd5f-40fb-9ee6-ce9ee7ed9925",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# encode (tokenise) the entire text dataset, and store it as a torch.Tensor\n",
    "import torch # using PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 chars from earlier will, to the GPT, look like this\n",
    "# If torch not installed error, run the following terminal command in active jupyter env: \n",
    "# conda install pytorch::pytorch torchvision torchaudio -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65adedc4-d8c0-4d89-bd92-ee03d9028de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the data into train and validation sets\n",
    "n = int(0.9*len(data)) # the first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:] # to assess how much the model has overfit the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed0ee9-6a2e-42f1-93d9-1aeeadb13323",
   "metadata": {},
   "source": [
    "## Training the transformer architecture on the text dataset\n",
    "\n",
    "* Let's input the integer sequence into the transformer so it can \"learn\" the training data.\n",
    "* This needs to be done in blocks/chunks (sampled randomly from the training set), not the entire integer sequence at once as this is infeasible\n",
    "* Each block/chunk has a maximum length of `block_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c218572-5e4b-4ec2-99d8-c7a5d1a081b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 # max length of a chunk (aka. context_length)\n",
    "train_data[:block_size+1] # let's sample 9 chars from the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089eeab-32e9-4b68-bb93-c7adfd75ee05",
   "metadata": {},
   "source": [
    "### Sample a block from the dataset\n",
    "* The 9 characters (tokens) sampled above actually **simultaneously contain 8 examples** of \"how english works\".\n",
    "* This simultaneously trains the transformer to make a prediction at each of the 8 positions in the block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0ba50bc-b4ab-40b4-a5b0-c17628805a82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the 8 examples hidden in the 9 character chunk sampled from the training set\n",
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] # inputs to the transformer (the first `block_size` chars)\n",
    "y = train_data[1:block_size+1] # the targets to each position in the input transformer (i.e. offset by 1)\n",
    "\n",
    "print('These are the 8 examples hidden in the 9 character chunk sampled from the training set')\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] # context: all chars in x, upto and including t\n",
    "    target = y[t] # target the t'th char in y (aka the t+1'th char in x)\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8acf80c-fcea-427d-a5ea-6a0e396ff131",
   "metadata": {},
   "source": [
    "### Why train on all 8 examples (context size of 1 up to context size of `block_size`?)\n",
    "* So the transformer is used to seeing contexts of all lengths (all the way from 1 to `block_size`), \n",
    "* This way we can start the sampling generation from as little as 1 character (token) of context, up to `block_size` characters, and transformer knows how to predict the next character (token)\n",
    "* More than `block_size` characters (tokens), and we need to truncate, bc the transformer is never expected to see more than `block_size` inputs. \n",
    "    * This is why GPT-3/4 are so good, with `block_size` of 8k-32k tokens, allowing for very large inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a668a05-ad3c-4206-8816-e7bbeb0fecb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have randomly sampled four 9-char (9-token) chunks from the full text to form two new 4x8 tensors (input and target)\n",
      "We now have a batch containing a total of 32 independent examples to train the transformer\n",
      "\n",
      "inputs (for transformer to process):\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "\n",
      "targets (for transformer to look up for\n",
      " each corresp. input sequence):\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n",
      "----\n",
      "\n",
      "Here are the 32 independent examples spelled out, for the transformer to be trained on\n",
      "Another way to read the following is \"in the context of [input integer sequence], [target] comes next\"\n",
      "\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # independent sequences processed in parallel by the transformer on each forward/backward pass (independently, due to GPUs being highly efficient!)\n",
    "block_size = 8 # maximum context length for predicitons\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generate 4 random numbers; positions in the total Shakespeare text to sample 4 blocks from\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # inputs to the transformer (first `block_size` chars)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # targets for each position in inputs (i.e. offset by 1)\n",
    "    return x, y\n",
    "\n",
    "print('We have randomly sampled four 9-char (9-token) chunks from the full text to form two new 4x8 tensors (input and target)')\n",
    "print('We now have a batch containing a total of 32 independent examples to train the transformer')\n",
    "print()\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs (for transformer to process):')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print()\n",
    "print('targets (for transformer to look up for\\n each corresp. input sequence):')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('\\n----\\n')\n",
    "\n",
    "print('Here are the 32 independent examples spelled out, for the transformer to be trained on')\n",
    "print('Another way to read the following is \"in the context of [input integer sequence], [target] comes next\"')\n",
    "print()\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46e3969f-9b87-44a1-a115-158d236e181a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de512e-5208-4eab-a7d5-fd0a252d9fb1",
   "metadata": {},
   "source": [
    "## Bigram language model \n",
    "A simple first baseline neural network for language modelling\n",
    "# TBC: See makemore series for indepth explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "891dc0cf-efc1-489f-a1ce-9a9fe2e6ccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape # because PyTorch F.cross_entropy() docs wants a (B,C,T) matrix instead OR a (B*T, C) 2D array (a little confusing)\n",
    "            logits = logits.view(B*T, C) # stretching out the array into 1D vector (B*T = 32-elem long), and preserving Channel (C) as 2nd dim (reshape logits for PyTorch compatibility)\n",
    "            targets = targets.view(B*T) # same as for logits. match dim., stretching targets to a 32-elem 1D array (note, targets.view(-1) also works, PyTorch guesses to match to logits.)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss) # We have 65 possible vocabulary elements, so (guess) we expect -ln(1/65) = ~4.17, but loss ie actually ~4.87. Means initial predictions are not diffuse (flat). They have some entropy (bumpy) so we're guessing wrong for the next token!\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # 1x1 tensor (B=1, T=1), with 0 (newline character) to start the generation.\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e51e6-0946-4941-b7cf-c8ce44631f83",
   "metadata": {},
   "source": [
    "### Note\n",
    "The output generated is random because the model is untrained so generates garbage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1df654f-ab62-4a13-a9d9-38c2e8a794b1",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8284e300-4f02-4cfc-8d49-d4346962d8e6",
   "metadata": {},
   "source": [
    "### Create the optimiser\n",
    "\n",
    "In makemore series, only stochastic gradient descent (SGD) was used for its simplicity.\n",
    "\n",
    "Here, the `AdamW` optimiser is used. It is a much more advanced optimiser\n",
    "* For usual large networks, a learning rate of `3e-4` or similar order is suggested\n",
    "* Since our network is so small, the learning rate of `1e-3` works! Larger would probably work\n",
    "\n",
    "#### How it works\n",
    "\n",
    "An optimiser object:\n",
    "\n",
    "0. Resets all gradients to 0\n",
    "1. Gets the gradients for all parameters\n",
    "2. and updates the parameters using those gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1084b921-5604-458a-970e-90098a4bd3d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4225 parameters\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "print(sum(p.numel() for p in m.parameters()), 'parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76fbbd81-b574-475a-be4c-78134b18e328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.382369041442871\n",
      "4225 parameters\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # larger!\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data, 2 [B x T] tensors\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb) \n",
    "    optimizer.zero_grad(set_to_none=True) # zero out all gradients from previous step\n",
    "    loss.backward() # get gradients for all parameters\n",
    "    optimizer.step() # user gradients to update parameters\n",
    "\n",
    "print(loss.item())\n",
    "print(sum(p.numel() for p in m.parameters()), 'parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11629faf-c183-478a-ab2c-5fc91029fa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "BEY:! Indy; by s afreanoo adicererupa anse tecorro llaus a!\n",
      "OLeneerithesinthengove fal amas trr\n",
      "TI ar I t, mes, n IUSt my w, fredeeyove\n",
      "THek' merer, dd\n",
      "We ntem lud engitheso; cer ize helorowaginte the?\n",
      "Thak orblyoruldvicee chot, p,\n",
      "Bealivolde Th li\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99d0e4-bd51-435d-b059-d24971bed1ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Massive improvement!\n",
    "\n",
    "Note though, the tokens are not yet talking to each other (i.e. the model is not seeing the full context, only the most recent step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79703d36-66d9-4858-9c35-9b4e9bb0fa30",
   "metadata": {},
   "source": [
    "### [FOLD] Full Code so far in script form for iteration/analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdfd705f-d2f9-4f68-8587-f6463162f5d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004225 M parameters\n",
      "step 0: train loss 4.7305, val loss 4.7241\n",
      "step 300: train loss 2.8110, val loss 2.8249\n",
      "step 600: train loss 2.5434, val loss 2.5682\n",
      "step 900: train loss 2.4932, val loss 2.5088\n",
      "step 1200: train loss 2.4863, val loss 2.5035\n",
      "step 1500: train loss 2.4665, val loss 2.4921\n",
      "step 1800: train loss 2.4683, val loss 2.4936\n",
      "step 2100: train loss 2.4696, val loss 2.4846\n",
      "step 2400: train loss 2.4638, val loss 2.4879\n",
      "step 2700: train loss 2.4738, val loss 2.4911\n",
      "step 2999: train loss 2.4611, val loss 2.4903\n",
      "\n",
      "LIZAntaitoupis!\n",
      "BENIngt,\n",
      "N tiel, serhe hill: h wous sal ayolf sthereeyowoulour: horgonof m\n",
      "\n",
      "\n",
      "sunicour,\n",
      "\n",
      "\n",
      "ANLOurak anominfaind oul bond f DIC:\n",
      "O g.\n",
      "\n",
      "Gr IOLouspold se.\n",
      "Dotamy t Y mioke om, d a he ates,\n",
      "ARDus ang s tist;\n",
      "'s\n",
      "ORINUSTod ad yety CLAns,\n",
      "Takerecithak ws: got hesal tobjobis,'s dsent,\n",
      "BRCOfararnt wilanou?\n",
      "F ve:\n",
      "ORDO\n",
      "FOpe tend d hil RK:\n",
      "Four wath, f.\n",
      "I\n",
      "BUMEToirofre ahow ive I's,\n",
      "NI to'l\n",
      "A:\n",
      "NTINERK:\n",
      "SThingou, thingilet menyo trt hisis; shisig t ern vey theY wanan, stoue l heny me Wheris wo m am g.\n",
      "He,\n",
      "MAnese otr d youse all s, tills idil h th l dowisourin w, iatuss, cchit wo horilod fe d\n",
      "p y'her, lowrn smeoorel do\n",
      "IOfathadm pulen\n",
      "Sc wo the;\n",
      "Pancalolinghowhatharean:\n",
      "QUConewhass bowoond;\n",
      "Fomere d shacenotep.\n",
      "CI y mbotot swefesealso br. ave aviasurf my, mayo t ivee ioulrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;\n",
      "\n",
      "Whan.\n",
      "Buspowhald ve!\n",
      "RIULOFFofusheserer hee anf,\n",
      "TOFonk? me ain cketoty dedo bo'llll at ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "BEY:\n",
      "TIndy; bungaroreanoo adicererupo anse tecorro llaus a!\n",
      "OLeneerithesinthenou theal amas trr\n",
      "TI ar I t, mes, o IUSt my w, fredeeyove\n",
      "THek' merer, dd\n",
      "We ntem lud ey'stheso; cer ize helour\n",
      "Juthte thext;\n",
      "\n",
      "I orblyoruldvicee chot, p,\n",
      "Be e Yolde Th likl's amen, tofroun s BYo tred ceathe, il ivilde w\n",
      "O ff y\n",
      "Five:\n",
      "Mied aithe.\n",
      "Sevis; fofounce herevern outh f athawendesees yof th withind be wameats tsteer y blitow,\n",
      "Yey' o dis.\n",
      "Whemy fich rte u hart ararwsa\n",
      "Wou fed ooreathoune\n",
      "IESIOFin,\n",
      "MIOndeed sus;\n",
      "Wh.\n",
      "S:\n",
      "NMy Bupind g.\n",
      "iudshank\n",
      "An chin is a arokisupxaseru t w ity merode l LOLo bebte loolld worinero ya l aknge ond thal ttry b's mo ge ck.\n",
      "\n",
      "gh, inketilllin trewnut; allar,\n",
      "CAnt cithapis ZETEnowrdistherdrtes saure ' erpoperrposthelind y ss of hef thepesct\n",
      "Ywit harfoul'st, ar izenthe ct.\n",
      "Fo, sther:\n",
      "IUSay pr o,-she.\n",
      "Wowstothedind rteed; the t,\n",
      "STo wremin bjest e tistloue ded ndean-bros g wes mout fok yolaime do myoulato,\n",
      "Mok h ay t nch sle bionhoured whaneables mye.\n",
      "For f beng tho; aroTo ld? mintaknceawe;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 0. hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # run on GPU if available\n",
    "eval_iters = 200\n",
    "# ------------\n",
    "\n",
    "# 1. Reproducibility \n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# 2. Read the data\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 3. Get the tokeniser (encoder and decoder functions)\n",
    "# Here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# 4. Create train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# 5. Use the data loader to get a batch of random inputs and targets (integer seq's from full text dataset)\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # ensure loaded data is moved to device (CUDA GPU)\n",
    "    return x, y\n",
    "\n",
    "# 6. Averages the loss value over multiple batches\n",
    "# @torch.no_grad() is a context manager. \n",
    "#     It tells PyTorch everything in estimate_loss() function doesn't undergo back propogation (i.e. tell PyTorch we don't intend to call .backward() method)\n",
    "#     PyTorch thus doesn't store intermediate variables (gradients??), so can be much more mem efficiency\n",
    "@torch.no_grad() # context manager\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # NB: model is set to eval phase because some layers in complex NNs behave differently in eval (inference) and train modes.\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) # zero init losses vector\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split) # create inputs (X) and targets (Y) (Y is offset from X by 1 token)\n",
    "            logits, loss = model(X, Y) # iteratively get losses for each batch (for train, then for val)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() # average all (eval_iters=200) losses for train, then for val\n",
    "    model.train() # NB: model is set back to train phase\n",
    "    return out\n",
    "\n",
    "# 7. Super simple bigram language model (can forward; output logits and loss, and generate tokens)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape # because PyTorch F.cross_entropy() docs wants a (B,C,T) matrix instead OR a (B*T, C) 2D array (a little confusing)\n",
    "            logits = logits.view(B*T, C) # stretching out the array into 1D vector, and preserving Channel (C) as 2nd dim\n",
    "            targets = targets.view(B*T) # same as for logits. match dim. (note, targets.view(-1) also works, PyTorch guesses to match to logits.\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "# 7.1 Initialising model and moving it to device (CUDA GPU)\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device) # e.g. nn.Embedding() table has dot weights moved to GPU to perform calcs faster\n",
    "\n",
    "# 7.2 print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# 8. Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 9. Create a training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss() # less noisy measure of loss (average over eval_iters # of batches)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # randomly sample a batch (32 independent 8-token (integer) sequences) of data\n",
    "    xb, yb = get_batch('train') # xb (input tensor) and yb (targets) are each [B=32 x T=8] tensors\n",
    "\n",
    "    # evaluate the loss - improving the parameters (training the model)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 10. Generate from the model, ensuring initial context created to feed into generate() is created on the device\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # initial context: newline character (0)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc11a5f-0cf3-445e-83e1-7d79b50db706",
   "metadata": {},
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf05f117-7192-4b16-b356-080d9921aba6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example (4 indep seqs, 8 chars each, speaking a language that only contains 2 tokens (e.g. 2 words/ 2 letters)):\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9683cb60-2a2e-41f1-99b7-47f0100cbcf5",
   "metadata": {},
   "source": [
    "#### Goal of aggregation\n",
    "\n",
    "We are trying to summarise **each token** (in a given input tensor `x`) **in the context of its history**\n",
    "\n",
    "* Version 1: [**INEFFICIENT**] Mean aggregation (w/ for loops): \n",
    "    * Token `x[t]` is summarised as the average of itself `x[t]` and the tokens that came before it `x[0]`, `x[1]`, ... `x[t-1]`\n",
    "    * BOW: \"bag-of-words\" aka just averaging all the words!\n",
    "* Version 2: [**EFFICIENT**] Mean aggregation (using matrix multiplication to perform a weighted sum): \n",
    "    * Same as V1, but faster\n",
    "* Version 3: Using Softmax:\n",
    "    * \n",
    "* Version 4: Self-attention:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e769a-f62c-4153-96f3-afb440ba1f86",
   "metadata": {},
   "source": [
    "### Version 1: (Inefficient) Mean aggregation of historical context from past tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7bd88d5-848f-475c-b2aa-9e3f6c9d7fab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterate over all batch dims indepentently, and over time.\n",
      "xprev is everything upto AND INCLUDING the t'th token (t+1 cos python 0-indexing) - includes their channel info (logits)\n",
      "\n",
      "\n",
      "x[0]: Each row contains the (C=) 2 logits for the (T=)8 chars [for the (B=0)1st batch]\n",
      " tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "\n",
      "xbow[0]: Same as above, but now each row element is the AVERAGE of itself and all previous rows\n",
      " tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "note the cascading averages in xbow[0] for each time step, wrt x[0]\n"
     ]
    }
   ],
   "source": [
    "# version 1: We want x[b,t] = mean_{i<=t} x[b,i] \n",
    "# - i.e. aggregate using the average of the vectors of all PAST tokens and the current token\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B): # 4 batches\n",
    "    for t in range(T): # 8 tokens (per batch)\n",
    "        xprev = x[b,:t+1] # for current batch b, xprev shape is (t,C) \n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "        \n",
    "print('Iterate over all batch dims indepentently, and over time.')\n",
    "print(\"xprev is everything upto AND INCLUDING the t'th token (t+1 cos python 0-indexing) - includes their channel info (logits)\\n\\n\")\n",
    "# print(x)\n",
    "print('x[0]: Each row contains the (C=) 2 logits for the (T=)8 chars [for the (B=0)1st batch]\\n', x[0])\n",
    "print('\\nxbow[0]: Same as above, but now each row element is the AVERAGE of itself and all previous rows\\n', xbow[0])\n",
    "print('note the cascading averages in xbow[0] for each time step, wrt x[0]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4804f-8b73-4222-bfd6-d2a0921c8695",
   "metadata": {},
   "source": [
    "### Version 2: (Efficient) Mean aggregation (using matrix multiplication to perform a weighted sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709054b-cdb4-4987-a22a-f4fb2d2cde11",
   "metadata": {},
   "source": [
    "#### Preamble to version 2 (folded code example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f45a0479-10df-42d0-ac51-31f65573cc93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncomment here for BTS\n",
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# preamble to version 2: toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b # matrix multiplication notation\n",
    "# print('a=')\n",
    "# print(a)\n",
    "# print('--')\n",
    "# print('b=')\n",
    "# print(b)\n",
    "# print('--')\n",
    "# print('c=')\n",
    "# print(c)\n",
    "# print('^c is dot prod of a and b')\n",
    "# print('\\n\\n------ How about a variable sum ------\\n\\n')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "# print('a=')\n",
    "# print(a)\n",
    "# print('--')\n",
    "# print('b=')\n",
    "# print(b)\n",
    "# print('--')\n",
    "# print('c=')\n",
    "# print(c)\n",
    "# print('\\n\\n--------------------\\n\\n')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "print('uncomment here for BTS')\n",
    "# print(a)\n",
    "# print(torch.sum(a, 1, keepdim=True)) # keepdim to keep it vertical (i.e. 3x1, instead of defaulting to 1x3 so broadcasting works out)\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30b4aa71-e8a4-4f47-b1e3-995b6b435ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbow[0]:\n",
      " tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]]) \n",
      "\n",
      "xbow2[0]:\n",
      " tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]]) \n",
      "First batches (B=0) of xbow and xbow2. \n",
      "Note: identical! xbow2 so much more efficient!\n"
     ]
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "weights = torch.tril(torch.ones(T, T)) # triangular form confines aggregation to HISTORICAL tokens only\n",
    "weights = weights / weights.sum(1, keepdim=True) # weights for each row's weighted sum (note: rows of wei sum to 1)\n",
    "# print(weights)\n",
    "xbow2 = weights @ x # (B', T, T) @ (B, T, C) ----> (B, T, C)\n",
    "    # PyTorch creates a B' dimension for the TxT weights matrix, then does a \"batched matrix multiply\"\n",
    "    # This means for each of the B batches, PyTorch does a (TxT)(TxC) matrix multiplication; \n",
    "    # leaving a [TxC] FOR EACH batch, hence the output is (B,T,C)\n",
    "torch.allclose(xbow, xbow2) # Means both tensors are identical\n",
    "print('xbow[0]:\\n', xbow[0], '\\n\\nxbow2[0]:\\n', xbow2[0], '\\nFirst batches (B=0) of xbow and xbow2. \\nNote: identical! xbow2 so much more efficient!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a64e8-0ec1-4f0f-a7d3-c54d0d665f36",
   "metadata": {},
   "source": [
    "#### Notes on using batched matrix multiply to do a weighted aggregation (a bunch of weighted sums!)\n",
    "\n",
    "* $\\texttt{weights}_{\\text{[T}\\times\\text{T]}}$ and $\\texttt{x}_{\\text{[B}\\times\\text{T}\\times\\text{C]}}$ matrices don't have same dim, so PyTorch implicitly creates a Batch dim for `weights`, to perform a \"batched matrix multiplication\".\n",
    "* Now we're doing a weighted sum for each batch\n",
    "    * I.e. for each of the B batches, we are multiplying the $\\text{[T}\\times\\text{T]}$ `weights` matrix with the $\\text{[T}\\times\\text{C]}$ `x` inputs matrix\")\n",
    "    * This performs the mean aggregation (historical sum)\n",
    "* Try visualise each batch as the \n",
    "    * **Horizontal** planar (2D) slices of the $\\text{[T}\\times\\text{C]}$ [Time x Channel] object, where \n",
    "    * **Vertically** stacking the independent $\\text{[T}\\times\\text{C]}$ slices (aka batches) gives you the full $\\text{[B}\\times\\text{T}\\times\\text{C]}$ construct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1e64a-9377-42cc-9ad9-6faee0750ec1",
   "metadata": {},
   "source": [
    "### Verison 3: Use softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d787e36-4e5a-4e7f-b2de-c70e606de2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tril:\n",
      " tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]]) \n",
      "\n",
      "weights (init):\n",
      " tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "weights (masked fill):\n",
      " tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "weights (softmax - a normalisation fn: [e^x / sum of row] ):\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T)) # lower triangular ones matrix\n",
    "weights = torch.zeros((T,T)) # zero init weights\n",
    "print('tril:\\n', tril, '\\n\\nweights (init):\\n', weights)\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "print('\\nweights (masked fill):\\n', weights)\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "print('\\nweights (softmax - a normalisation fn: [e^x / sum of row] ):\\n', weights)\n",
    "xbow3 = weights @ x\n",
    "torch.allclose(xbow, xbow3) # again, identical!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ae784-bd38-49a4-937f-ed69cf45592a",
   "metadata": {},
   "source": [
    "#### Notes: Why use the softmax approach in self-attention?\n",
    "\n",
    "* `weights = torch.zeros((T,T))`\n",
    "    * Begin initialised with 0s, but this will change later, **different tokens will have different affinities for each other**\n",
    "    * This is like interaction string/affinity. Different tokens will find other tokens more or less interesting\n",
    "* `weights = weights.masked_fill(tril == 0, float('-inf'))`\n",
    "    * Setting to `-inf` means **tokens from the future cannot communicate with the current token**\n",
    "* `weights = F.softmax(weights, dim=-1)` and `xbow3 = weights @ x`\n",
    "    * Normalise and then take weighted sum (batched matrix multiplication) to get the weighted aggregations of PAST elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56765b16-d48b-4f40-8264-0b71d5397223",
   "metadata": {},
   "source": [
    "## Recess: Let's revisit and improve the Bigram model definition\n",
    "\n",
    "Improvements:\n",
    "\n",
    "* No longer passing in `vocab_size` as an arg, it is already a global variable\n",
    "* Don't create full square $\\text{[C}\\times\\text{C]}$ **embedding table** of logits at once. Instead, use intermediate phase\n",
    "    * Init new var `n_embd = 32`: Number of embedding suggestions\n",
    "* Thus, can't obtain logits in `forward()` directly, instead we get `tok_emb` token embeddings of dim $\\text{[B}\\times\\text{T}\\times\\text{C = }\\texttt{n\\_embd}]$\n",
    "    * We employ a **LINEAR LAYER** (`nn.Linear(n_embd, vocab_size)`) to go from `tok_emb` to `logits` (dim $\\text{[B}\\times\\text{T}\\times\\text{C = }\\texttt{vocab\\_size}]$)\n",
    "    * Note the $\\text{C}$'s are different. $\\text{C}$ is actually `n_embd` now, by re-definition.\n",
    "* Previously, `token_embedding_table` attribute embedded the identities of the tokens inside `idx`\n",
    "    * Now, `position_embedding_table` attribute embeds the positions of the tokens as well\n",
    "* <mark>More info is on GoodNotes</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b3cbbae-4afb-4741-8c45-bfe4c8e3c58a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Updating the bigram language model (can forward; output logits and loss, and generate tokens)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # a linear layer to go from tok_emb to logits\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "                # since now using pos_emb, can never have > block_size being passed in \n",
    "                # otherwise pos_emb table will run out of scope, since it only has embeddings UP TO block_size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0689dc79-397a-48aa-b326-aed20276ced2",
   "metadata": {},
   "source": [
    "### Version 4: *(Single-head)* Self-attention (THE CRUX OF GPTs)\n",
    "\n",
    "#### Recap:\n",
    "\n",
    "* So far, we're doing a <mark>simple mean of the historical tokens and the current token</mark> to describe the current token\n",
    "* We create a lower triangular mask of `weights`, then normalise to only get the historical context (i.e. tokens from the future cannot communicate with the current token)\n",
    "* **Disadvantage**: **Previous** and **current** info is being **mixed together in an average**\n",
    "\n",
    "#### Insight\n",
    "\n",
    "Now note, because we initialised as `weights = torch.zeros((T,T))`, <mark>every token (aka node!) has the same affinity for every other token</mark>. \n",
    "* In other words, since we <mark>**initialised**</mark> token affinities uniformly,\n",
    "* (Hence,) the final `weights` matrix has <mark>**uniform structure**</mark> (all elements on a row are equal)\n",
    "\n",
    "#### <mark>**BUT WE DON'T NECESSARILY WANT UNIFORM TOKEN AFFINITY!**</mark>\n",
    "\n",
    "Different tokens find different *other* tokens more or less interesting (e.g. a vowel may be preferentially interested in the consonants in its past, more so than the vowels)\n",
    "\n",
    "#### The answer:\n",
    "Do self-attention (this solves the above issue in a data-dependent way (i.e. learns structures in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f9214ed-20bc-4586-b3e1-2f6f4213b4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention! - toy example of 4x8 tokens tensor, and information @ each token is 32 channel\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16 # these are 16 information channels, different to just \"vocab_size\".\n",
    "key = nn.Linear(C, head_size, bias=False) # initialising Linear modules; bias=False applies matrix multiply with some fixed weights\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16), forwarding/propogating the \"key\" module on x\n",
    "q = query(x) # (B, T, 16), forwarding the \"query\" module on x\n",
    "\n",
    "# a smarter way to initialise weights (instead of zeroes!)\n",
    "weights =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T) \n",
    "# (i.e. for every row of B, we'll end up with a TxT matrix of initial token affinities)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#weights = torch.zeros((T,T)) # weights are no longer initialised as zeros\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1) # i.e. -- exponentiate and normalise (0 to 1)\n",
    "\n",
    "v = value(x) # propogating the \"value\" linear over x\n",
    "out = weights @ v # aggregate over this value vector (instead of this raw x)\n",
    "#out = weights @ x # we're not aggregating raw tokens themselves (x is PRIVATE to the token); but rather their values (v is like PUBLIC info the token reveals)!\n",
    "\n",
    "out.shape # thus, output of single head is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec905d07-9404-4fac-ac0e-1ba87a3e33b6",
   "metadata": {},
   "source": [
    "#### What's happening under the hood in self-attention?\n",
    "\n",
    "I think we're simply initialising `weights` a little more cleverly.\n",
    "\n",
    "* Each node/token at each position <mark>**in parallel, and independently emits 2 vectors**</mark>: `query` and `key`\n",
    "    * `query`: \"What am I looking for?\"\n",
    "    * `key`: \"What do I contain?\"\n",
    "    * Note: no communication has happened between tokens yet\n",
    "* In self-attention, to get **affinities between all tokens** in a sequence is to do a **dot product between queries and keys**.\n",
    "    * More specifically, the `query` of token `x[t]`, dot producted with all the `key`s of all the other tokens `x[0]`, `x[1]`, ... `x[t-1]`\n",
    "    * `weights` is initialised as that dot product!\n",
    "    * **Implication** the more highly aligned a given token's (e.g. `x[t]`'s) `query` is with another token's `key`, \n",
    "        * the higher their dot product (affinity), \n",
    "        * and thus the more `x[t]` will learn about that specific token (over other tokens in the sequence)\n",
    "    * For every row of $\\text{B}$, we'll end up with a $\\text{[T}\\times\\text{T]}$ matrix of initial token affinities)\n",
    "* In other words, **self-attention is where communication occurs between tokens**\n",
    "\n",
    "---\n",
    "\n",
    "#### How is `value(x)` any different to raw `x`? What exactly is the difference?\n",
    "* `x` is information that is private to the token in position [B,T] \"I am 5th token with some identity, and my information is in vector `x`\"\n",
    "* \"For a single head, here's what I'm interested in (query:`q`), here's what I have (key:`k`), and if you find me interesting (your `query` dot producted with my `key` produces a high value), here's what I will share with you (value `v`). \n",
    "    * `v` is the thing which gets aggregated for a single head between the different tokens/nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b5f8f48-5bfe-4f07-b9c3-d6311d1a67db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0] # weights for batch 1, affinities between the tokens in this batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c1e56-31d9-4def-921f-b41166ffe12e",
   "metadata": {},
   "source": [
    "#### Outcome:`weights` is now data-dependent\n",
    "\n",
    "* Now we see, while each row still sums to 1, the `weights` matrix is now not constant. \n",
    "* Each batch element contains different tokens at different positions. So now **`weights` is data-dependent**\n",
    "    * Example: 8th token now knows its **content** and **position**.\n",
    "        * It can broadcast a `key`: \"I'm a vowel on the 8th position\"\n",
    "        * and a `query`: \"I'm looking for a consonant in anywhere up to the 4th position\n",
    "    * All tokens emit `key`s. \n",
    "        * One of the (`head_size=16`) channels might be \"I am a consonant in the somewhere up to the 4th position\"\n",
    "        * Upon `q @ k` dot prod, a token highly aligned tokens have a **high number** in that specific channel\n",
    "            * <mark>e.g. 0.2297 in row 8, col 4: the `key` of token 4 is highly aligned with the `query` of token 8 - so **token 8 has a high affinity for token 4**</mark>, as evidenced by the large dot product\n",
    "    * <mark>Thus, through softmax, the 8th token aggregates a LOT OF INFO about the 4th token into its position (8)</mark>; i.e. token 8 learns a lot about token 4 (relative to the other tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1639265-55f3-43bd-9062-cf77bbb734a6",
   "metadata": {},
   "source": [
    "### Notes on Attention:\n",
    "\n",
    "* Attention is a communication mechanism. \n",
    "    * Can be seen as nodes in a directed graph, where each node has some vector of information\n",
    "    * Nodes are looking at each other (edges pointing at each other) \n",
    "    * Each node aggregating information with a weighted sum from all nodes that point to in, with data-dependent weights.\n",
    "* There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "    * *I.e. Nodes don't know \"where they are positioned in space\". Attention is just a set of vectors randomly in space which communicate*\n",
    "* Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "    * A **batched matrix multiply** is what applies a matrix multiplication **in parallel across the batch dimension**\n",
    "        * This is why (B, T, 16) @ (B, 16, T) ---> (B, T, T) \n",
    "* In an **\"encoder\" block of (self-)attention** just delete the single line that does masking with tril, allowing all tokens to communicate. \n",
    "    * That's the general case: e.g. for sentiment analysis w/ transformer, all tokens can freely speak w/ each other to predict the sentiment of the whole sentence\n",
    "    * This block here is called a **\"decoder\" (self-)attention block** because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "    * **Attention supports arbitrary connectivity between nodes!** Attention doesn't care if you use encoder or decoder blocks.\n",
    "* \"Self-attention\" just means that the **keys** `k`'s and **values** `v`'s are produced from the same source as **queries** `q`'s. \n",
    "    * This source is the initial input vector `x`, which contains $\\text{T}$ tokens, each emitting a `k`, `q`, and `v`.\n",
    "    * In \"cross-attention\", the queries still get produced from `x`, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "        * I.e. a separate set of nodes from which will pull information into our nodes\n",
    "* \"Scaled\" attention additional divides `weights` by $\\frac{1}{\\sqrt{\\texttt{head\\_size}}}$. An important normalisation!\n",
    "    * This makes it so when input `q`, `k` are unit variance (Gaussian: $\\mu=0$, and $\\sigma=1$), `weights` will be unit variance ($\\sigma=1$) too \n",
    "    * and so when `weights` feeds into Softmax step (see above), it will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a1be272-df4c-4c2d-87b9-b69e2480cdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var(k) =  tensor(1.0449) \n",
      "var(q) =  tensor(1.0700)\n",
      "\n",
      "Without scaled attention, variance of `weights` is on the order of `head_size`: \n",
      "var(weights) =  tensor(17.4690)\n",
      "\n",
      "With scaled attention, variance of weights is on the order of 1: \n",
      "var(weights) =  tensor(0.9957)\n"
     ]
    }
   ],
   "source": [
    "head_size = 16\n",
    "\n",
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "weights = q @ k.transpose(-2, -1)\n",
    "print('var(k) = ', k.var(), '\\nvar(q) = ', q.var())\n",
    "\n",
    "print('\\nWithout scaled attention, variance of `weights` is on the order of `head_size`:', '\\nvar(weights) = ', weights.var())\n",
    "\n",
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "weights = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "print('\\nWith scaled attention, variance of weights is on the order of 1:', '\\nvar(weights) = ', weights.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823826f-8ef7-45e0-9325-4053673c559d",
   "metadata": {},
   "source": [
    "###### \"Scaled\" attention (cont.)\n",
    "\n",
    "If `weights` were not fairly diffuse (i.e. if `weights` took on very positive and/or negative numbers before softmax step), softmax would converge towards one-hot vectors (gets too peaky)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2736ef56-de2b-4a8d-9ff8-b125d1eb85ca",
   "metadata": {},
   "source": [
    "#### Scaled Attention: Diffuse `weights` initialisation: \n",
    "\n",
    "Here, \"weights\" is fairly diffuse to begin with (near 0, small +ve or -ve), so softmax produces diffuse outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebdac27c-8b1b-48dd-a2c1-e25717070f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8827373-17a0-48bb-a900-eb6a2db4ff84",
   "metadata": {},
   "source": [
    "#### Scaled Attention: Sharper `weights` initialisation (i.e. larger numbers)\n",
    "\n",
    "Here, the softmax outputs sharpen too, towards the maximum (extreme) number(s) in the initialised `weights` vector. \n",
    "\n",
    "* I.e., softmax is super peaky, and **each token will aggregate information from a single other node!** (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81207f4f-2661-473a-ae99-482a0a89b9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec816f-3d0d-4aca-8a9c-1ba5e8532315",
   "metadata": {},
   "source": [
    "### Scaled Attention - TL;DR:\n",
    "\n",
    "* Scaling `weights` by $\\frac{1}{\\sqrt{\\texttt{head\\_size}}}$ simply to control the variance at initialisation \n",
    "    * This allows $\\sigma\\approx1$; \n",
    "    * And prevents `softmax()` from producing too peaky, one-hot-like vectors\n",
    "        * I.e. It prevents a given token/node from aggregating information only from single other tokens/nodes! Rather allowing info from multiple nodes to flow to the querying node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265fb5a-ed4c-4bba-8fb1-a5f68dc640e9",
   "metadata": {},
   "source": [
    "## Let's Create Single- and Multi-Head Self-Attention Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3749b0cd-c3c2-4109-9f11-740542307c0b",
   "metadata": {},
   "source": [
    "### 1. Class which implements: A Single-Head of Self-Attention\n",
    "\n",
    "* `__init__()` \n",
    "    * Given input `head_size`\n",
    "    * Constructor creates:\n",
    "        * `Linear` layers (aka. projections) for `key`, `query` and `value` (to be applied in parallel and independently on to all nodes)\n",
    "        * `tril` variable creating the lower triangular matrix (not a param, but a buffer by PyTorch naming conventions, assigned to module using `register_buffer`)\n",
    "* `forward()` function:\n",
    "    * Given input `x`\n",
    "    * Function creates\n",
    "        * Calculates `k` and `q` (by forwarding the `key` and `query` Linear modules over `x`)\n",
    "        * Calculates attention scores (affinities) in `weights`, \n",
    "            * normalised using scaled attention, to control variance\n",
    "            * make it into a **decoder block** future can't commnicate with past\n",
    "            * softmax normalisation\n",
    "        * Produces `v`, and returns the aggregated value dot product of `weights` and `v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d0bf48c-e9aa-427c-994f-93d0f607e7c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        weights = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2563acb-d871-4794-8cef-91e42a32b299",
   "metadata": {},
   "source": [
    "### [FOLD] Full Code so far in script form for iteration/analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "683fe27c-e761-498e-96e3-320e0ae07fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007553 M parameters\n",
      "step 0: train loss 4.2000, val loss 4.2047\n",
      "step 500: train loss 2.6911, val loss 2.7087\n",
      "step 1000: train loss 2.5196, val loss 2.5303\n",
      "step 1500: train loss 2.4775, val loss 2.4829\n",
      "step 2000: train loss 2.4408, val loss 2.4523\n",
      "step 2500: train loss 2.4272, val loss 2.4435\n",
      "step 3000: train loss 2.4130, val loss 2.4327\n",
      "step 3500: train loss 2.3956, val loss 2.4212\n",
      "step 4000: train loss 2.4041, val loss 2.3992\n",
      "step 4500: train loss 2.3980, val loss 2.4084\n",
      "step 4999: train loss 2.3951, val loss 2.4126\n",
      "\n",
      "Ano' ou sene od wistwin, wildalle adery wheerel cro ove.\n",
      "\n",
      "K:\n",
      "OMInciove dan uts wat fo stu bur,\n",
      "Wer\n",
      "my seng\n",
      "Becthikure\n",
      "Pe.\n",
      "INCECLIUS: MId miombellavo focen soun ers.\n",
      "\n",
      "A:\n",
      "Way berup he past arr:\n",
      "A sg wou poat;\n",
      "Wino-f-\n",
      "I: herove, wipeofre\n",
      "Yor.\n",
      "Bur ugigiatns Ligo wisen't here, thee my teath, weird, ghe sss the D:\n",
      "NCGas In.\n",
      "\n",
      "WAnd, hei iett or ganger ng\n",
      "Pow, yout Can werd? I, we.\n",
      "\n",
      "Thth asthe mtolecraby st\n",
      "Cow thy had ally whin ango:\n",
      "A, ered whutl by at aprest douried ste sthe!:\n",
      "ADD' I fo p Eulot iso inod quuratoerof se.\n",
      "\n",
      "INUmy Cme.\n",
      "\n",
      "D Imee sdof as, at wom; tthitout nd me shes it.\n",
      "\n",
      "Papoamareng llos berers dorue may gaech phrish fo su\n",
      "ld.\n",
      "\n",
      "Ous.\n",
      "\n",
      "INMOFiach Ail hy oufureng cog; he yughpat t, rit iet fald So ed semanbu hea Py I wo st cett?\n",
      "\n",
      "G aler ll imot.\n",
      "\n",
      "Whll teine rnd lanto uf he shand, owaved heathe;\n",
      "A!\n",
      "D Yofoe so ew ad wet ars\n",
      "H:\n",
      "Asthre, ucean ar.\n",
      "\n",
      "Cod MThis\n",
      "LI agherde.\n",
      "\n",
      "EG: lesiseavimy whr olrd\n",
      "K\n",
      "IS:\n",
      "S: pire fe ousee:\n",
      "BERPY: be ses dthel youse MI burt sed to sofo heapey aris alllere wer nde ive irele dere thit yom; s boeist thit; ils ay vel,\n",
      "Cof amile facang so ker whe cises, orove the isor, chathey hyuseve ys; whane shy m hirey ore no?\n",
      "\n",
      "SThe imy,\n",
      "winy omy aisit;\n",
      "'shalie.\n",
      "COMy ten al'in,\n",
      "O: be hat wit sstoht ay yo t,:\n",
      "-Whtt f bor Court gill womeava ice ha'lld:\n",
      "I simu d oy ben hour tr ore css f-het ipobou so Slalpest, st wise woer ru isos mpee te;\n",
      "and\n",
      "Thy Jor alldadsre; tur ow-whancof oks,\n",
      "S lad nd tho If ml ndo noeng oolle ou htellarrd heeckeh or mus\n",
      "Weldisot.\n",
      "MO:\n",
      "A hom?\n",
      "\n",
      "Weth har sen fis ma odo sisesos, det mve patl mume boulighavenyo ansot by pyevere'seal'do mbuchidobered toker st torst wat toull wead higon priil hpeinl I movitmey ke, ci, ben. Buks lien, us Bent let rbe f shek, ben yere il testicancome CG, oald tharlens ancor alits, falve, tan youl waret pid ore ante'di she lerd he hul, ros\n",
      "Wimovere for sece htate hy ad where, steds hour tongend\n",
      "S es ovatthy isus?\n",
      "Wh the\n",
      "Biet thalile ngrie?\n",
      "As! st, cholll alr th yon allge yifrelver nod I loare: theve, hor egee ise hepe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 0. hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # run on GPU if available\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "# 1. Reproducibility \n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# 2. Read the data\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 3. Get the tokeniser (encoder and decoder functions)\n",
    "# Here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# 4. Create train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# 5. Use the data loader to get a batch of random inputs and targets (integer seq's from full text dataset)\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # ensure loaded data is moved to device (CUDA GPU)\n",
    "    return x, y\n",
    "\n",
    "# 6. Averages the loss value over multiple batches\n",
    "# @torch.no_grad() is a context manager. \n",
    "#     It tells PyTorch everything in estimate_loss() function doesn't undergo back propogation (i.e. tell PyTorch we don't intend to call .backward() method)\n",
    "#     PyTorch thus doesn't store intermediate values, so can be much more mem efficiency\n",
    "@torch.no_grad() # context manager\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # NB: model is set to eval phase because complex neural networks behave differently in eval (inference) and train modes.\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) # zero init losses vector\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y) # iteratively get losses for each batch (for train, then for val)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() # average all (200) losses for train, then for val\n",
    "    model.train() # NB: model is set back to train phase\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        weights = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = weights @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "# 7. Updating the bigram language model (can forward; output logits and loss, and generate tokens)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd) # crCeate self-attention head in constructor\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # a linear layer to go from tok_emb to logits\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x) # appy one head of self-attention. (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "                # since now using pos_emb, can never have > block_size being passed in \n",
    "                # otherwise pos_emb table will run out of scope, since it only has embeddings UP TO block_size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "# 7.1 Initialising model and moving it to device (CUDA GPU)\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device) # e.g. nn.Embedding() table has dot weights moved to GPU to perform calcs faster\n",
    "\n",
    "# 7.2 print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# 8. Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 9. Create a training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss() # less noisy measure of loss (average over eval_iters # of batches)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 10. Generate from the model, ensuring initial context created to feed into generate() is created on the device\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) \n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a0282-8b17-4b10-8311-a43ebdf8d1c6",
   "metadata": {},
   "source": [
    "### 2. Class which implements: A Single-Head of Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f65c39e0-1d09-41eb-adbb-8f8e46ea2313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cab80676-a3ec-4377-b616-c8d149000e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f28ab9c1-a06d-47b5-92f1-6974fae3380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7637c64-7775-4110-b8d1-7e45e6d96334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e2e62-b291-4c15-a873-7a83fa14db8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f516de-2252-449d-bb1f-abb8da8d5688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2489657b-499d-4c68-bb12-0238a4e43cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b51e0a7-7091-431c-afd8-48dfdebbdb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "932df938-90f0-4d08-8b1d-42fa913e00de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.5763e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "681b2d76-1bf4-4ae1-b9c5-0fc69331dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les rseaux de neurones sont gniaux! <START> neural networks are awesome!<END>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad54d8-5cbd-45f6-a9fe-14767eaa3384",
   "metadata": {},
   "source": [
    "### Updating the Bigram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68dcb458-507b-4f25-9314-e02de1ddaa90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Updating the bigram language model (can forward; output logits and loss, and generate tokens)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # a linear layer to go from tok_emb to logits\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e01ecab-564f-4362-b451-3f220a5544fb",
   "metadata": {},
   "source": [
    "# Full finished code (folded below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6da436f2-7926-4503-8ed9-1cd800aaafc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6568, val loss 2.6670\n",
      "step 200: train loss 2.5091, val loss 2.5059\n",
      "step 300: train loss 2.4196, val loss 2.4336\n",
      "step 400: train loss 2.3498, val loss 2.3560\n",
      "step 500: train loss 2.2964, val loss 2.3130\n",
      "step 600: train loss 2.2406, val loss 2.2497\n",
      "step 700: train loss 2.2052, val loss 2.2185\n",
      "step 800: train loss 2.1633, val loss 2.1861\n",
      "step 900: train loss 2.1246, val loss 2.1511\n",
      "step 1000: train loss 2.1036, val loss 2.1309\n",
      "step 1100: train loss 2.0709, val loss 2.1196\n",
      "step 1200: train loss 2.0381, val loss 2.0794\n",
      "step 1300: train loss 2.0243, val loss 2.0633\n",
      "step 1400: train loss 1.9929, val loss 2.0362\n",
      "step 1500: train loss 1.9711, val loss 2.0313\n",
      "step 1600: train loss 1.9633, val loss 2.0484\n",
      "step 1700: train loss 1.9410, val loss 2.0126\n",
      "step 1800: train loss 1.9111, val loss 1.9973\n",
      "step 1900: train loss 1.9103, val loss 1.9877\n",
      "step 2000: train loss 1.8853, val loss 1.9945\n",
      "step 2100: train loss 1.8711, val loss 1.9765\n",
      "step 2200: train loss 1.8594, val loss 1.9612\n",
      "step 2300: train loss 1.8541, val loss 1.9503\n",
      "step 2400: train loss 1.8437, val loss 1.9446\n",
      "step 2500: train loss 1.8157, val loss 1.9425\n",
      "step 2600: train loss 1.8264, val loss 1.9394\n",
      "step 2700: train loss 1.8099, val loss 1.9330\n",
      "step 2800: train loss 1.8071, val loss 1.9250\n",
      "step 2900: train loss 1.8077, val loss 1.9323\n",
      "step 3000: train loss 1.7989, val loss 1.9207\n",
      "step 3100: train loss 1.7713, val loss 1.9230\n",
      "step 3200: train loss 1.7555, val loss 1.9141\n",
      "step 3300: train loss 1.7606, val loss 1.9092\n",
      "step 3400: train loss 1.7565, val loss 1.8923\n",
      "step 3500: train loss 1.7361, val loss 1.8905\n",
      "step 3600: train loss 1.7256, val loss 1.8840\n",
      "step 3700: train loss 1.7323, val loss 1.8863\n",
      "step 3800: train loss 1.7229, val loss 1.8916\n",
      "step 3900: train loss 1.7187, val loss 1.8654\n",
      "step 4000: train loss 1.7119, val loss 1.8552\n",
      "step 4100: train loss 1.7118, val loss 1.8778\n",
      "step 4200: train loss 1.7038, val loss 1.8559\n",
      "step 4300: train loss 1.6976, val loss 1.8423\n",
      "step 4400: train loss 1.7050, val loss 1.8618\n",
      "step 4500: train loss 1.6898, val loss 1.8519\n",
      "step 4600: train loss 1.6872, val loss 1.8321\n",
      "step 4700: train loss 1.6819, val loss 1.8385\n",
      "step 4800: train loss 1.6678, val loss 1.8449\n",
      "step 4900: train loss 1.6703, val loss 1.8382\n",
      "step 4999: train loss 1.6633, val loss 1.8223\n",
      "\n",
      "Flie?\n",
      "\n",
      "WARICENTIO:\n",
      "Shrungst bewiter are a toom here:\n",
      "In if the wouIes?\n",
      "Out; and sate, and for one that I are and those it;\n",
      "Git.\n",
      "\n",
      "WARIO:\n",
      "Advory's toble sear; the, will God\n",
      "of breath what Mear;\n",
      "Was can is your name Burry eyree facge\n",
      "For will dath thee herp too thy laments\n",
      "That would may clood, one these do spost I vour have weret, where sup.\n",
      "How sens Gortunt, which what fit,\n",
      "Out thre, is wwife that broth. Who', betch'd your.\n",
      "\n",
      "TRABELLANE:\n",
      "Shall\n",
      "My sworn must he anour,\n",
      "Buntius; in So metter those make me,\n",
      "And fliems, my chince.\n",
      "\n",
      "POMY:\n",
      "Yet my nourtwarly to be thrany's discont,\n",
      "If day a gends pmenaton him, say.\n",
      "\n",
      "DUKE OF YORK:\n",
      "The twerefory well streage was babolantand now thing:\n",
      "O, this to set myself, cour bid to shall her speesen he crown.\n",
      "Vnow as thou thront, plarter no adds in thyself croath.\n",
      "My lord, but terruly friend\n",
      "Ristomfurts to-mries\n",
      "Againt serviet, contandy that kisspy grave, we mine!\n",
      "Or he him my spast,\n",
      "I so unsinced, wen is bese zable gity;\n",
      "Not do to seing, then thee from whosame noby.\n",
      "Go and neer thou would may night.\n",
      "\n",
      "RUCHERSIO:\n",
      "That, by tongue will be in him our sitittion;\n",
      "She now to be, all goots stespers\n",
      "in An rentry. Towe pas-dayfull keep,\n",
      "That thy will your sould in him,\n",
      "And ladditlebaning that gentrand, which myself, betish end alwied's boy exides'd.\n",
      "\n",
      "ISABTH:\n",
      "My master the slands you great? I shalk;\n",
      "Stat the kngscanty on straight boys hitger;\n",
      "Becompely his doath; of us Voly.\n",
      "\n",
      "Sensure:\n",
      "And there your worts, all, save infict is a those astold\n",
      "of by my felling wit-be in his in\n",
      "Hard's deaths chanting me them is seedder'd was busweet.\n",
      "\n",
      "CORINIUS:\n",
      "No, for I have your merch'mord.\n",
      "Is you graXut affinzy houth this sele yourders?\n",
      "\n",
      "POLFORD NORWARD YOMIUS:\n",
      "You come you.\n",
      "\n",
      "POYCUS:\n",
      "Thy gleist the dongsorn:\n",
      "Nay fantle Becoleforfact tell servy inters! in I know tould lappread\n",
      "Goat you sucalf me wars non your\n",
      "art:\n",
      "As one thumself, who live requann.\n",
      "Anjury thee that with, and we not?\n",
      "Good the preasurs, comison toOH!\n",
      "\n",
      "Second Affordmn:-now.\n",
      "\n",
      "Must wear I pearince?\n",
      "And think\n"
     ]
    }
   ],
   "source": [
    "## import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 0. hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # run on GPU if available\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "# 1. Reproducibility \n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# 2. Read the data\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 3. Get the tokeniser (encoder and decoder functions)\n",
    "# Here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# 4. Create train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# 5. Use the data loader to get a batch of random inputs and targets (integer seq's from full text dataset)\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # ensure loaded data is moved to device (CUDA GPU)\n",
    "    return x, y\n",
    "\n",
    "# 6. Averages the loss value over multiple batches\n",
    "# @torch.no_grad() is a context manager. \n",
    "#     It tells PyTorch everything in estimate_loss() function doesn't undergo back propogation (i.e. tell PyTorch we don't intend to call .backward() method)\n",
    "#     PyTorch thus doesn't store intermediate values, so can be much more mem efficiency\n",
    "@torch.no_grad() # context manager, \n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # NB: model is set to eval phase because complex neural networks behave differently in eval (inference) and train modes.\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) # zero init losses vector\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y) # iteratively get losses for each batch (for train, then for val)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() # average all (200) losses for train, then for val\n",
    "    model.train() # NB: model is set back to train phase\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# 7. Super simple bigram language model (can forward; output logits and loss, and generate tokens)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "# 7.1 Initialising model and moving it to device (CUDA GPU)\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device) # e.g. nn.Embedding() table has dot weights moved to GPU to perform calcs faster\n",
    "\n",
    "# 7.2 print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# 8. Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 9. Create a training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss() # less noisy measure of loss (average over eval_iters # of batches)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 10. Generate from the model, ensuring initial context created to feed into generate() is created on the device\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) \n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
